// -----// IR Dump After {anonymous}::FuncOpLoweringPass () //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @main() {
    %0 = "ta.index_label"() : () -> !ta.indexlabel
    %1 = "ta.index_label"() : () -> !ta.indexlabel
    %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    %5 = "ta.add"(%2, %3, %0, %1, %0, %1, %0, %1) <{MaskType = "none", formats = ["Dense", "Dense", "Dense"], indexing_maps = [#map, #map, #map], semiring = "noop_plusxy"}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel) -> tensor<1024x1024xf64>
    "ta.set_op"(%5, %4) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> ()
    "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
    return
  }
}


// -----// IR Dump After {anonymous}::TensorAlgebraCheckImplicitTensorDeclPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "ta.add"(%2, %3, %0, %1, %0, %1, %0, %1) <{MaskType = "none", formats = ["Dense", "Dense", "Dense"], indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], semiring = "noop_plusxy"}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel) -> tensor<1024x1024xf64>
  "ta.set_op"(%5, %4) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> ()
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerTensorAlgebraToIndexTreePass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DenseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::TensorFillLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DenseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseTempOutputTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseOutputTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DimOpLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::TensorFillLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::PCToLoopsLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerTensorAlgebraToSCFPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "parallel"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerIndexTreeToSCFPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = tensor.dim %2, %c0_4 : tensor<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = tensor.dim %2, %c1_7 : tensor<1024x1024xf64>
    scf.parallel (%arg1) = (%c0_5) to (%dim_8) step (%c1_6) {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %5 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %6 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %7 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %8 = arith.addf %5, %6 : f64
      memref.store %8, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      scf.reduce 
    }
    scf.reduce 
  }
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.parallel (%arg1) = (%c0_5) to (%dim_8) step (%c1_6) {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %3 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %5 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %6 = arith.addf %3, %4 : f64
      memref.store %6, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      scf.reduce 
    }
    scf.reduce 
  }
  "ta.print"(%2) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::STCRemoveDeadOpsPass () //----- //
func.func @main() {
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.parallel (%arg1) = (%c0_5) to (%dim_8) step (%c1_6) {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = arith.addf %1, %2 : f64
      memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      scf.reduce 
    }
    scf.reduce 
  }
  "ta.print"(%0) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LateLoweringPass () //----- //
func.func @main() {
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.parallel (%arg1) = (%c0_5) to (%dim_8) step (%c1_6) {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = arith.addf %1, %2 : f64
      memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      scf.reduce 
    }
    scf.reduce 
  }
  %cast = memref.cast %alloc_1 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main() {
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
    %cst = arith.constant 2.200000e+00 : f64
    linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
    %cst_2 = arith.constant 3.400000e+00 : f64
    linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
    %cst_3 = arith.constant 0.000000e+00 : f64
    linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c0_4 = arith.constant 0 : index
    %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
    scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
      scf.parallel (%arg1) = (%c0_5) to (%dim_8) step (%c1_6) {
        %cst_9 = arith.constant 0.000000e+00 : f64
        %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
        %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
        %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
        %4 = arith.addf %1, %2 : f64
        memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
        scf.reduce 
      }
      scf.reduce 
    }
    %cast = memref.cast %alloc_1 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.parallel (%arg0) = (%c0) to (%c1024) step (%c1) {
      scf.parallel (%arg1) = (%c0) to (%c1024) step (%c1) {
        %0 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
        %1 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
        %2 = arith.addf %0, %1 : f64
        memref.store %2, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
        scf.reduce 
      }
      scf.reduce 
    }
    %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After CometParallelLoopsToGpu (comet-parallel-loops-to-gpu) //----- //
func.func private @comet_sort_index(memref<*xindex>, index, index)

// -----// IR Dump After CometParallelLoopsToGpu (comet-parallel-loops-to-gpu) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After CometParallelLoopsToGpu (comet-parallel-loops-to-gpu) //----- //
func.func @main() {
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 2.200000e+00 : f64
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %c1_2 = arith.constant 1 : index
  %dim = memref.dim %alloc, %c1_2 : memref<1024x1024xf64>
  %c1_3 = arith.constant 1 : index
  %dim_4 = memref.dim %alloc, %c1_3 : memref<1024x1024xf64>
  %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %c1_6 = arith.constant 1 : index
  %dim_7 = memref.dim %alloc_5, %c1_6 : memref<1024x1024xf64>
  %c1_8 = arith.constant 1 : index
  %dim_9 = memref.dim %alloc_5, %c1_8 : memref<1024x1024xf64>
  %collapse_shape_10 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %alloc_11 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %c1_12 = arith.constant 1 : index
  %dim_13 = memref.dim %alloc_11, %c1_12 : memref<1024x1024xf64>
  %c1_14 = arith.constant 1 : index
  %dim_15 = memref.dim %alloc_11, %c1_14 : memref<1024x1024xf64>
  %collapse_shape_16 = memref.collapse_shape %alloc_11 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %0 = arith.subi %c1024, %c0 : index
  %c0_17 = arith.constant 0 : index
  scf.for %arg0 = %c0_17 to %0 step %c1 {
    %7 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg0)[%c0]
    %8 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    scf.for %arg1 = %c0_21 to %8 step %c1 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%c0]
      %10 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%7, %9)[%dim_4]
      memref.store %cst_1, %collapse_shape[%10] : memref<1048576xf64>
    }
  }
  %1 = arith.subi %c1024, %c0 : index
  %c0_18 = arith.constant 0 : index
  scf.for %arg0 = %c0_18 to %1 step %c1 {
    %7 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg0)[%c0]
    %8 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    scf.for %arg1 = %c0_21 to %8 step %c1 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%c0]
      %10 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%7, %9)[%dim_9]
      memref.store %cst_0, %collapse_shape_10[%10] : memref<1048576xf64>
    }
  }
  %2 = arith.subi %c1024, %c0 : index
  %c0_19 = arith.constant 0 : index
  scf.for %arg0 = %c0_19 to %2 step %c1 {
    %7 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg0)[%c0]
    %8 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    scf.for %arg1 = %c0_21 to %8 step %c1 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%c0]
      %10 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%7, %9)[%dim_15]
      memref.store %cst, %collapse_shape_16[%10] : memref<1048576xf64>
    }
  }
  %c8 = arith.constant 8 : index
  %c1_20 = arith.constant 1 : index
  %3 = arith.subi %c1024, %c0 : index
  %4 = arith.subi %c8, %c1_20 : index
  %5 = arith.addi %3, %4 : index
  %6 = arith.divsi %5, %c8 : index
  scf.parallel (%arg0) = (%c0) to (%6) step (%c1_20) {
    scf.parallel (%arg1) = (%c0) to (%c8) step (%c1_20) {
      %7 = affine.apply affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>(%arg0)[%c8, %arg1, %c0]
      %8 = arith.minui %7, %c1024 {GuardY} : index
      %c32 = arith.constant 32 : index
      %c1_21 = arith.constant 1 : index
      %9 = arith.subi %c1024, %c0 : index
      %10 = arith.subi %c32, %c1_21 : index
      %11 = arith.addi %9, %10 : index
      %12 = arith.divsi %11, %c32 : index
      scf.parallel (%arg2) = (%c0) to (%12) step (%c1_21) {
        scf.parallel (%arg3) = (%c0) to (%c32) step (%c1_21) {
          %13 = affine.apply affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>(%arg2)[%c32, %arg3, %c0]
          %14 = arith.minui %13, %c1024 {GuardX} : index
          %15 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%8, %14)[%dim]
          %16 = memref.load %collapse_shape[%15] : memref<1048576xf64>
          %17 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%8, %14)[%dim_7]
          %18 = memref.load %collapse_shape_10[%17] : memref<1048576xf64>
          %19 = arith.addf %16, %18 : f64
          %20 = affine.apply affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>(%8, %14)[%dim_13]
          memref.store %19, %collapse_shape_16[%20] : memref<1048576xf64>
          scf.reduce 
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimX_block"}
        scf.reduce 
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimX_grid"}
      scf.reduce 
    } {mapping = [#gpu.loop_dim_map<processor = thread_y, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimY_block"}
    scf.reduce 
  } {mapping = [#gpu.loop_dim_map<processor = block_y, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimY_grid"}
  %cast = memref.cast %alloc_11 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
#map = affine_map<(d0)[s0] -> (d0 + s0)>
#map1 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
#map2 = affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_2 = arith.constant 1 : index
    %dim = memref.dim %alloc, %c1_2 : memref<1024x1024xf64>
    %c1_3 = arith.constant 1 : index
    %dim_4 = memref.dim %alloc, %c1_3 : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_6 = arith.constant 1 : index
    %dim_7 = memref.dim %alloc_5, %c1_6 : memref<1024x1024xf64>
    %c1_8 = arith.constant 1 : index
    %dim_9 = memref.dim %alloc_5, %c1_8 : memref<1024x1024xf64>
    %collapse_shape_10 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_11 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_12 = arith.constant 1 : index
    %dim_13 = memref.dim %alloc_11, %c1_12 : memref<1024x1024xf64>
    %c1_14 = arith.constant 1 : index
    %dim_15 = memref.dim %alloc_11, %c1_14 : memref<1024x1024xf64>
    %collapse_shape_16 = memref.collapse_shape %alloc_11 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %0 = arith.subi %c1024, %c0 : index
    %c0_17 = arith.constant 0 : index
    %1 = arith.subi %c1024, %c0 : index
    %c0_18 = arith.constant 0 : index
    scf.for %arg0 = %c0_17 to %0 step %c1 {
      %14 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_18 to %1 step %c1 {
        %15 = affine.apply #map(%arg1)[%c0]
        %16 = affine.apply #map1(%14, %15)[%dim_4]
        memref.store %cst_1, %collapse_shape[%16] : memref<1048576xf64>
      }
    }
    %2 = arith.subi %c1024, %c0 : index
    %c0_19 = arith.constant 0 : index
    %3 = arith.subi %c1024, %c0 : index
    %c0_20 = arith.constant 0 : index
    scf.for %arg0 = %c0_19 to %2 step %c1 {
      %14 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_20 to %3 step %c1 {
        %15 = affine.apply #map(%arg1)[%c0]
        %16 = affine.apply #map1(%14, %15)[%dim_9]
        memref.store %cst_0, %collapse_shape_10[%16] : memref<1048576xf64>
      }
    }
    %4 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    %5 = arith.subi %c1024, %c0 : index
    %c0_22 = arith.constant 0 : index
    scf.for %arg0 = %c0_21 to %4 step %c1 {
      %14 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_22 to %5 step %c1 {
        %15 = affine.apply #map(%arg1)[%c0]
        %16 = affine.apply #map1(%14, %15)[%dim_15]
        memref.store %cst, %collapse_shape_16[%16] : memref<1048576xf64>
      }
    }
    %c8 = arith.constant 8 : index
    %c1_23 = arith.constant 1 : index
    %6 = arith.subi %c1024, %c0 : index
    %7 = arith.subi %c8, %c1_23 : index
    %8 = arith.addi %6, %7 : index
    %9 = arith.divsi %8, %c8 : index
    %c32 = arith.constant 32 : index
    %c1_24 = arith.constant 1 : index
    %10 = arith.subi %c1024, %c0 : index
    %11 = arith.subi %c32, %c1_24 : index
    %12 = arith.addi %10, %11 : index
    %13 = arith.divsi %12, %c32 : index
    scf.parallel (%arg0) = (%c0) to (%9) step (%c1_23) {
      scf.parallel (%arg1) = (%c0) to (%c8) step (%c1_23) {
        %14 = affine.apply #map2(%arg0)[%c8, %arg1, %c0]
        %15 = arith.minui %14, %c1024 {GuardY} : index
        scf.parallel (%arg2) = (%c0) to (%13) step (%c1_24) {
          scf.parallel (%arg3) = (%c0) to (%c32) step (%c1_24) {
            %16 = affine.apply #map2(%arg2)[%c32, %arg3, %c0]
            %17 = arith.minui %16, %c1024 {GuardX} : index
            %18 = affine.apply #map1(%15, %17)[%dim]
            %19 = memref.load %collapse_shape[%18] : memref<1048576xf64>
            %20 = affine.apply #map1(%15, %17)[%dim_7]
            %21 = memref.load %collapse_shape_10[%20] : memref<1048576xf64>
            %22 = arith.addf %19, %21 : f64
            %23 = affine.apply #map1(%15, %17)[%dim_13]
            memref.store %22, %collapse_shape_16[%23] : memref<1048576xf64>
            scf.reduce 
          } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimX_block"}
          scf.reduce 
        } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimX_grid"}
        scf.reduce 
      } {mapping = [#gpu.loop_dim_map<processor = thread_y, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimY_block"}
      scf.reduce 
    } {mapping = [#gpu.loop_dim_map<processor = block_y, map = (d0) -> (d0), bound = (d0) -> (d0)>], parallelDim = "dimY_grid"}
    %cast = memref.cast %alloc_11 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
#map = affine_map<(d0)[s0] -> (d0 + s0)>
#map1 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
#map2 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map3 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
#map4 = affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_2 = arith.constant 1 : index
    %dim = memref.dim %alloc, %c1_2 : memref<1024x1024xf64>
    %c1_3 = arith.constant 1 : index
    %dim_4 = memref.dim %alloc, %c1_3 : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_6 = arith.constant 1 : index
    %dim_7 = memref.dim %alloc_5, %c1_6 : memref<1024x1024xf64>
    %c1_8 = arith.constant 1 : index
    %dim_9 = memref.dim %alloc_5, %c1_8 : memref<1024x1024xf64>
    %collapse_shape_10 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_11 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_12 = arith.constant 1 : index
    %dim_13 = memref.dim %alloc_11, %c1_12 : memref<1024x1024xf64>
    %c1_14 = arith.constant 1 : index
    %dim_15 = memref.dim %alloc_11, %c1_14 : memref<1024x1024xf64>
    %collapse_shape_16 = memref.collapse_shape %alloc_11 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %0 = arith.subi %c1024, %c0 : index
    %c0_17 = arith.constant 0 : index
    %1 = arith.subi %c1024, %c0 : index
    %c0_18 = arith.constant 0 : index
    scf.for %arg0 = %c0_17 to %0 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_18 to %1 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_4]
        memref.store %cst_1, %collapse_shape[%20] : memref<1048576xf64>
      }
    }
    %2 = arith.subi %c1024, %c0 : index
    %c0_19 = arith.constant 0 : index
    %3 = arith.subi %c1024, %c0 : index
    %c0_20 = arith.constant 0 : index
    scf.for %arg0 = %c0_19 to %2 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_20 to %3 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_9]
        memref.store %cst_0, %collapse_shape_10[%20] : memref<1048576xf64>
      }
    }
    %4 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    %5 = arith.subi %c1024, %c0 : index
    %c0_22 = arith.constant 0 : index
    scf.for %arg0 = %c0_21 to %4 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_22 to %5 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_15]
        memref.store %cst, %collapse_shape_16[%20] : memref<1048576xf64>
      }
    }
    %c8 = arith.constant 8 : index
    %c1_23 = arith.constant 1 : index
    %6 = arith.subi %c1024, %c0 : index
    %7 = arith.subi %c8, %c1_23 : index
    %8 = arith.addi %6, %7 : index
    %9 = arith.divsi %8, %c8 : index
    %c32 = arith.constant 32 : index
    %c1_24 = arith.constant 1 : index
    %10 = arith.subi %c1024, %c0 : index
    %11 = arith.subi %c32, %c1_24 : index
    %12 = arith.addi %10, %11 : index
    %13 = arith.divsi %12, %c32 : index
    %c1_25 = arith.constant 1 : index
    %14 = affine.apply #map2(%9)[%c0, %c1_23]
    %15 = affine.apply #map2(%c8)[%c0, %c1_23]
    %16 = affine.apply #map2(%13)[%c0, %c1_24]
    %17 = affine.apply #map2(%c32)[%c0, %c1_24]
    gpu.launch blocks(%arg0, %arg1, %arg2) in (%arg6 = %16, %arg7 = %14, %arg8 = %c1_25) threads(%arg3, %arg4, %arg5) in (%arg9 = %17, %arg10 = %15, %arg11 = %c1_25) {
      %18 = affine.apply #map3(%arg1)[%c1_23, %c0]
      %19 = affine.apply #map3(%arg4)[%c1_23, %c0]
      %20 = affine.apply #map4(%18)[%c8, %19, %c0]
      %21 = arith.minui %20, %c1024 {GuardY} : index
      %22 = affine.apply #map3(%arg0)[%c1_24, %c0]
      %23 = affine.apply #map3(%arg3)[%c1_24, %c0]
      %24 = affine.apply #map4(%22)[%c32, %23, %c0]
      %25 = arith.minui %24, %c1024 {GuardX} : index
      %26 = affine.apply #map1(%21, %25)[%dim]
      %27 = memref.load %collapse_shape[%26] : memref<1048576xf64>
      %28 = affine.apply #map1(%21, %25)[%dim_7]
      %29 = memref.load %collapse_shape_10[%28] : memref<1048576xf64>
      %30 = arith.addf %27, %29 : f64
      %31 = affine.apply #map1(%21, %25)[%dim_13]
      memref.store %30, %collapse_shape_16[%31] : memref<1048576xf64>
      gpu.terminator
    } {SCFToGPU_visited, parallelDim = "dimX_block"}
    %cast = memref.cast %alloc_11 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
#map = affine_map<(d0)[s0] -> (d0 + s0)>
#map1 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
#map2 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map3 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
#map4 = affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_2 = arith.constant 1 : index
    %dim = memref.dim %alloc, %c1_2 : memref<1024x1024xf64>
    %c1_3 = arith.constant 1 : index
    %dim_4 = memref.dim %alloc, %c1_3 : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_6 = arith.constant 1 : index
    %dim_7 = memref.dim %alloc_5, %c1_6 : memref<1024x1024xf64>
    %c1_8 = arith.constant 1 : index
    %dim_9 = memref.dim %alloc_5, %c1_8 : memref<1024x1024xf64>
    %collapse_shape_10 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_11 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_12 = arith.constant 1 : index
    %dim_13 = memref.dim %alloc_11, %c1_12 : memref<1024x1024xf64>
    %c1_14 = arith.constant 1 : index
    %dim_15 = memref.dim %alloc_11, %c1_14 : memref<1024x1024xf64>
    %collapse_shape_16 = memref.collapse_shape %alloc_11 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %0 = arith.subi %c1024, %c0 : index
    %c0_17 = arith.constant 0 : index
    %1 = arith.subi %c1024, %c0 : index
    %c0_18 = arith.constant 0 : index
    scf.for %arg0 = %c0_17 to %0 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_18 to %1 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_4]
        memref.store %cst_1, %collapse_shape[%20] : memref<1048576xf64>
      }
    }
    %2 = arith.subi %c1024, %c0 : index
    %c0_19 = arith.constant 0 : index
    %3 = arith.subi %c1024, %c0 : index
    %c0_20 = arith.constant 0 : index
    scf.for %arg0 = %c0_19 to %2 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_20 to %3 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_9]
        memref.store %cst_0, %collapse_shape_10[%20] : memref<1048576xf64>
      }
    }
    %4 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    %5 = arith.subi %c1024, %c0 : index
    %c0_22 = arith.constant 0 : index
    scf.for %arg0 = %c0_21 to %4 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_22 to %5 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_15]
        memref.store %cst, %collapse_shape_16[%20] : memref<1048576xf64>
      }
    }
    %c8 = arith.constant 8 : index
    %c1_23 = arith.constant 1 : index
    %6 = arith.subi %c1024, %c0 : index
    %7 = arith.subi %c8, %c1_23 : index
    %8 = arith.addi %6, %7 : index
    %9 = arith.divsi %8, %c8 : index
    %c32 = arith.constant 32 : index
    %c1_24 = arith.constant 1 : index
    %10 = arith.subi %c1024, %c0 : index
    %11 = arith.subi %c32, %c1_24 : index
    %12 = arith.addi %10, %11 : index
    %13 = arith.divsi %12, %c32 : index
    %c1_25 = arith.constant 1 : index
    %14 = affine.apply #map2(%9)[%c0, %c1_23]
    %15 = affine.apply #map2(%c8)[%c0, %c1_23]
    %16 = affine.apply #map2(%13)[%c0, %c1_24]
    %17 = affine.apply #map2(%c32)[%c0, %c1_24]
    gpu.launch blocks(%arg0, %arg1, %arg2) in (%arg6 = %16, %arg7 = %14, %arg8 = %c1_25) threads(%arg3, %arg4, %arg5) in (%arg9 = %17, %arg10 = %15, %arg11 = %c1_25) {
      %18 = affine.apply #map3(%arg1)[%c1_23, %c0]
      %19 = affine.apply #map3(%arg4)[%c1_23, %c0]
      %20 = affine.apply #map4(%18)[%c8, %19, %c0]
      %21 = arith.minui %20, %c1024 {GuardY} : index
      %22 = affine.apply #map3(%arg0)[%c1_24, %c0]
      %23 = affine.apply #map3(%arg3)[%c1_24, %c0]
      %24 = affine.apply #map4(%22)[%c32, %23, %c0]
      %25 = arith.minui %24, %c1024 {GuardX} : index
      %26 = affine.apply #map1(%21, %25)[%dim]
      %27 = memref.load %collapse_shape[%26] : memref<1048576xf64>
      %28 = affine.apply #map1(%21, %25)[%dim_7]
      %29 = memref.load %collapse_shape_10[%28] : memref<1048576xf64>
      %30 = arith.addf %27, %29 : f64
      %31 = affine.apply #map1(%21, %25)[%dim_13]
      memref.store %30, %collapse_shape_16[%31] : memref<1048576xf64>
      gpu.terminator
    } {SCFToGPU_visited, parallelDim = "dimX_block"}
    %cast = memref.cast %alloc_11 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0] -> (d0 + s0)>
#map1 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
#map2 = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map3 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
#map4 = affine_map<(d0)[s0, s1, s2] -> (d0 * s0 + s1 + s2)>
module attributes {gpu.container_module} {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_2 = arith.constant 1 : index
    %dim = memref.dim %alloc, %c1_2 : memref<1024x1024xf64>
    %c1_3 = arith.constant 1 : index
    %dim_4 = memref.dim %alloc, %c1_3 : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_6 = arith.constant 1 : index
    %dim_7 = memref.dim %alloc_5, %c1_6 : memref<1024x1024xf64>
    %c1_8 = arith.constant 1 : index
    %dim_9 = memref.dim %alloc_5, %c1_8 : memref<1024x1024xf64>
    %collapse_shape_10 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_11 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %c1_12 = arith.constant 1 : index
    %dim_13 = memref.dim %alloc_11, %c1_12 : memref<1024x1024xf64>
    %c1_14 = arith.constant 1 : index
    %dim_15 = memref.dim %alloc_11, %c1_14 : memref<1024x1024xf64>
    %collapse_shape_16 = memref.collapse_shape %alloc_11 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %0 = arith.subi %c1024, %c0 : index
    %c0_17 = arith.constant 0 : index
    %1 = arith.subi %c1024, %c0 : index
    %c0_18 = arith.constant 0 : index
    scf.for %arg0 = %c0_17 to %0 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_18 to %1 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_4]
        memref.store %cst_1, %collapse_shape[%20] : memref<1048576xf64>
      }
    }
    %2 = arith.subi %c1024, %c0 : index
    %c0_19 = arith.constant 0 : index
    %3 = arith.subi %c1024, %c0 : index
    %c0_20 = arith.constant 0 : index
    scf.for %arg0 = %c0_19 to %2 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_20 to %3 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_9]
        memref.store %cst_0, %collapse_shape_10[%20] : memref<1048576xf64>
      }
    }
    %4 = arith.subi %c1024, %c0 : index
    %c0_21 = arith.constant 0 : index
    %5 = arith.subi %c1024, %c0 : index
    %c0_22 = arith.constant 0 : index
    scf.for %arg0 = %c0_21 to %4 step %c1 {
      %18 = affine.apply #map(%arg0)[%c0]
      scf.for %arg1 = %c0_22 to %5 step %c1 {
        %19 = affine.apply #map(%arg1)[%c0]
        %20 = affine.apply #map1(%18, %19)[%dim_15]
        memref.store %cst, %collapse_shape_16[%20] : memref<1048576xf64>
      }
    }
    %c8 = arith.constant 8 : index
    %c1_23 = arith.constant 1 : index
    %6 = arith.subi %c1024, %c0 : index
    %7 = arith.subi %c8, %c1_23 : index
    %8 = arith.addi %6, %7 : index
    %9 = arith.divsi %8, %c8 : index
    %c32 = arith.constant 32 : index
    %c1_24 = arith.constant 1 : index
    %10 = arith.subi %c1024, %c0 : index
    %11 = arith.subi %c32, %c1_24 : index
    %12 = arith.addi %10, %11 : index
    %13 = arith.divsi %12, %c32 : index
    %c1_25 = arith.constant 1 : index
    %14 = affine.apply #map2(%9)[%c0, %c1_23]
    %15 = affine.apply #map2(%c8)[%c0, %c1_23]
    %16 = affine.apply #map2(%13)[%c0, %c1_24]
    %17 = affine.apply #map2(%c32)[%c0, %c1_24]
    gpu.launch_func  @main_kernel::@main_kernel blocks in (%16, %14, %c1_25) threads in (%17, %15, %c1_25)  args(%c1_23 : index, %c0 : index, %c8 : index, %c1024 : index, %c1_24 : index, %c32 : index, %dim : index, %collapse_shape : memref<1048576xf64>, %dim_7 : index, %collapse_shape_10 : memref<1048576xf64>, %dim_13 : index, %collapse_shape_16 : memref<1048576xf64>)
    %cast = memref.cast %alloc_11 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: index, %arg6: index, %arg7: memref<1048576xf64>, %arg8: index, %arg9: memref<1048576xf64>, %arg10: index, %arg11: memref<1048576xf64>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %12 = affine.apply #map3(%1)[%arg0, %arg1]
      %13 = affine.apply #map3(%4)[%arg0, %arg1]
      %14 = affine.apply #map4(%12)[%arg2, %13, %arg1]
      %15 = arith.minui %14, %arg3 {GuardY} : index
      %16 = affine.apply #map3(%0)[%arg4, %arg1]
      %17 = affine.apply #map3(%3)[%arg4, %arg1]
      %18 = affine.apply #map4(%16)[%arg5, %17, %arg1]
      %19 = arith.minui %18, %arg3 {GuardX} : index
      %20 = affine.apply #map1(%15, %19)[%arg6]
      %21 = memref.load %arg7[%20] : memref<1048576xf64>
      %22 = affine.apply #map1(%15, %19)[%arg8]
      %23 = memref.load %arg9[%22] : memref<1048576xf64>
      %24 = arith.addf %21, %23 : f64
      %25 = affine.apply #map1(%15, %19)[%arg10]
      memref.store %24, %arg11[%25] : memref<1048576xf64>
      gpu.return
    }
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#map = affine_map<(d0, d1) -> (d0 * 1024 + d1)>
#map1 = affine_map<(d0, d1)[s0, s1, s2] -> (d1 * s2 + s1 * 2 + (d0 * s2 + s1) * s0)>
#map2 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
module attributes {gpu.container_module} {
  func.func @main() {
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_1, %collapse_shape[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_0, %collapse_shape_3[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst, %collapse_shape_5[%0] : memref<1048576xf64>
      }
    }
    gpu.launch_func  @main_kernel::@main_kernel blocks in (%c32, %c128, %c1) threads in (%c32, %c8, %c1)  args(%c1 : index, %c0 : index, %c8 : index, %c1024 : index, %c1 : index, %c32 : index, %c1024 : index, %collapse_shape : memref<1048576xf64>, %c1024 : index, %collapse_shape_3 : memref<1048576xf64>, %c1024 : index, %collapse_shape_5 : memref<1048576xf64>)
    %cast = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  #map = affine_map<(d0, d1) -> (d0 * 1024 + d1)>
#map1 = affine_map<(d0, d1)[s0, s1, s2] -> (d1 * s2 + s1 * 2 + (d0 * s2 + s1) * s0)>
#map2 = affine_map<(d0, d1)[s0] -> (d1 + d0 * s0)>
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: index, %arg3: index, %arg4: index, %arg5: index, %arg6: index, %arg7: memref<1048576xf64>, %arg8: index, %arg9: memref<1048576xf64>, %arg10: index, %arg11: memref<1048576xf64>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.thread_id  x
      %3 = gpu.thread_id  y
      %4 = affine.apply #map1(%1, %3)[%arg2, %arg1, %arg0]
      %5 = arith.minui %4, %arg3 {GuardY} : index
      %6 = affine.apply #map1(%0, %2)[%arg5, %arg1, %arg4]
      %7 = arith.minui %6, %arg3 {GuardX} : index
      %8 = affine.apply #map2(%5, %7)[%arg6]
      %9 = memref.load %arg7[%8] : memref<1048576xf64>
      %10 = affine.apply #map2(%5, %7)[%arg8]
      %11 = memref.load %arg9[%10] : memref<1048576xf64>
      %12 = arith.addf %9, %11 : f64
      %13 = affine.apply #map2(%5, %7)[%arg10]
      memref.store %12, %arg11[%13] : memref<1048576xf64>
      gpu.return
    }
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertGpuKernelToTritonPass (convert-gpu-kernel-to-triton) //----- //
#map = affine_map<(d0, d1) -> (d0 * 1024 + d1)>
module attributes {gpu.container_module} {
  func.func @main() {
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_1, %collapse_shape[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_0, %collapse_shape_3[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst, %collapse_shape_5[%0] : memref<1048576xf64>
      }
    }
    gpu.launch_func  @main_kernel::@main_kernel blocks in (%c32, %c128, %c1) threads in (%c32, %c8, %c1)  args(%c1024 : index, %c1024 : index, %collapse_shape : memref<1048576xf64>, %c1024 : index, %collapse_shape_3 : memref<1048576xf64>, %c1024 : index, %collapse_shape_5 : memref<1048576xf64>) {checked}
    %cast = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<1048576xf64>, %arg3: index, %arg4: memref<1048576xf64>, %arg5: index, %arg6: memref<1048576xf64>) kernel attributes {block_size_x = 32 : index, block_size_y = 8 : index, copied} {
      gpu.return
    }
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  tt.func @main_kernel0(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !tt.ptr<f64, 1>, %arg5: i32, %arg6: !tt.ptr<f64, 1>, %arg7: i32, %arg8: !tt.ptr<f64, 1>) attributes {block_size_x = 32 : index, block_size_y = 8 : index, origin = "main_kernel::main_kernel"} {
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %0 = tt.get_num_programs {axis = 1 : i32} : i32
    %1 = tt.get_num_programs {axis = 0 : i32} : i32
    %2 = tt.get_program_id y : i32
    %3 = tt.get_program_id x : i32
    %4 = arith.subi %arg0, %2 : i32
    %5 = arith.subi %arg1, %3 : i32
    %6 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
    %7 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>
    %8 = tt.splat %arg2 : (i32) -> tensor<1x32xi32>
    %9 = arith.muli %3, %c32_i32 : i32
    %10 = tt.splat %9 : (i32) -> tensor<1x32xi32>
    %11 = arith.addi %7, %10 : tensor<1x32xi32>
    %12 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32>
    %13 = tt.expand_dims %12 {axis = 1 : i32} : (tensor<8xi32>) -> tensor<8x1xi32>
    %14 = tt.splat %arg2 : (i32) -> tensor<8x1xi32>
    %15 = arith.muli %2, %c8_i32 : i32
    %16 = tt.splat %15 : (i32) -> tensor<8x1xi32>
    %17 = arith.addi %13, %16 : tensor<8x1xi32>
    %18 = tt.splat %arg3 : (i32) -> tensor<8x1xi32>
    %19 = tt.splat %arg4 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    %20 = tt.splat %arg5 : (i32) -> tensor<8x1xi32>
    %21 = tt.splat %arg6 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    %22 = tt.splat %arg7 : (i32) -> tensor<8x1xi32>
    %23 = tt.splat %arg8 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    scf.for %arg9 = %c0_i32 to %4 step %0  : i32 {
      %24 = arith.muli %arg9, %c8_i32 : i32
      %25 = tt.splat %24 : (i32) -> tensor<8x1xi32>
      %26 = arith.addi %17, %25 : tensor<8x1xi32>
      %27 = arith.cmpi slt, %26, %14 : tensor<8x1xi32>
      %28 = tt.broadcast %27 : (tensor<8x1xi1>) -> tensor<8x32xi1>
      %29 = arith.muli %26, %18 : tensor<8x1xi32>
      %30 = tt.broadcast %29 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      %31 = arith.muli %26, %20 : tensor<8x1xi32>
      %32 = tt.broadcast %31 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      %33 = arith.muli %26, %22 : tensor<8x1xi32>
      %34 = tt.broadcast %33 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      scf.for %arg10 = %c0_i32 to %5 step %1  : i32 {
        %35 = arith.muli %arg10, %c32_i32 : i32
        %36 = tt.splat %35 : (i32) -> tensor<1x32xi32>
        %37 = arith.addi %11, %36 : tensor<1x32xi32>
        %38 = arith.cmpi slt, %37, %8 : tensor<1x32xi32>
        %39 = tt.broadcast %38 : (tensor<1x32xi1>) -> tensor<8x32xi1>
        %40 = arith.andi %39, %28 : tensor<8x32xi1>
        %41 = tt.broadcast %37 : (tensor<1x32xi32>) -> tensor<8x32xi32>
        %42 = arith.addi %41, %30 : tensor<8x32xi32>
        %43 = tt.addptr %19, %42 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        %44 = tt.load %43, %40 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8x32xf64>
        %45 = arith.addi %41, %32 : tensor<8x32xi32>
        %46 = tt.addptr %21, %45 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        %47 = tt.load %46, %40 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8x32xf64>
        %48 = arith.addf %44, %47 : tensor<8x32xf64>
        %49 = arith.addi %41, %34 : tensor<8x32xi32>
        %50 = tt.addptr %23, %49 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        tt.store %50, %48, %40 {cache = 1 : i32, evict = 1 : i32} : tensor<8x32xf64>
      } {programs_loop_x}
    } {programs_loop_y}
    tt.return
  }
}


// -----// IR Dump After LowerTritonDeviceToCuda (lower-triton-device-to-cuda) //----- //
#map = affine_map<(d0, d1) -> (d0 * 1024 + d1)>
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_1, %collapse_shape[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_0, %collapse_shape_3[%0] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = affine.apply #map(%arg0, %arg1)
        memref.store %cst, %collapse_shape_5[%0] : memref<1048576xf64>
      }
    }
    gpu.launch_func  @main_kernel::@main_kernel blocks in (%c32, %c128, %c1) threads in (%c32, %c8, %c1)  args(%c1024 : index, %c1024 : index, %collapse_shape : memref<1048576xf64>, %c1024 : index, %collapse_shape_3 : memref<1048576xf64>, %c1024 : index, %collapse_shape_5 : memref<1048576xf64>) {checked}
    %cast = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<1048576xf64>, %arg3: index, %arg4: memref<1048576xf64>, %arg5: index, %arg6: memref<1048576xf64>) kernel attributes {block_size_x = 32 : index, block_size_y = 8 : index, copied} {
      gpu.return
    }
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  tt.func @main_kernel0(%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32, %arg4: !tt.ptr<f64, 1>, %arg5: i32, %arg6: !tt.ptr<f64, 1>, %arg7: i32, %arg8: !tt.ptr<f64, 1>) attributes {block_size_x = 32 : index, block_size_y = 8 : index, origin = "main_kernel::main_kernel"} {
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %0 = tt.get_num_programs {axis = 1 : i32} : i32
    %1 = tt.get_num_programs {axis = 0 : i32} : i32
    %2 = tt.get_program_id y : i32
    %3 = tt.get_program_id x : i32
    %4 = arith.subi %arg0, %2 : i32
    %5 = arith.subi %arg1, %3 : i32
    %6 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
    %7 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>
    %8 = tt.splat %arg2 : (i32) -> tensor<1x32xi32>
    %9 = arith.muli %3, %c32_i32 : i32
    %10 = tt.splat %9 : (i32) -> tensor<1x32xi32>
    %11 = arith.addi %7, %10 : tensor<1x32xi32>
    %12 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32>
    %13 = tt.expand_dims %12 {axis = 1 : i32} : (tensor<8xi32>) -> tensor<8x1xi32>
    %14 = tt.splat %arg2 : (i32) -> tensor<8x1xi32>
    %15 = arith.muli %2, %c8_i32 : i32
    %16 = tt.splat %15 : (i32) -> tensor<8x1xi32>
    %17 = arith.addi %13, %16 : tensor<8x1xi32>
    %18 = tt.splat %arg3 : (i32) -> tensor<8x1xi32>
    %19 = tt.splat %arg4 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    %20 = tt.splat %arg5 : (i32) -> tensor<8x1xi32>
    %21 = tt.splat %arg6 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    %22 = tt.splat %arg7 : (i32) -> tensor<8x1xi32>
    %23 = tt.splat %arg8 : (!tt.ptr<f64, 1>) -> tensor<8x32x!tt.ptr<f64, 1>>
    scf.for %arg9 = %c0_i32 to %4 step %0  : i32 {
      %24 = arith.muli %arg9, %c8_i32 : i32
      %25 = tt.splat %24 : (i32) -> tensor<8x1xi32>
      %26 = arith.addi %17, %25 : tensor<8x1xi32>
      %27 = arith.cmpi slt, %26, %14 : tensor<8x1xi32>
      %28 = tt.broadcast %27 : (tensor<8x1xi1>) -> tensor<8x32xi1>
      %29 = arith.muli %26, %18 : tensor<8x1xi32>
      %30 = tt.broadcast %29 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      %31 = arith.muli %26, %20 : tensor<8x1xi32>
      %32 = tt.broadcast %31 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      %33 = arith.muli %26, %22 : tensor<8x1xi32>
      %34 = tt.broadcast %33 : (tensor<8x1xi32>) -> tensor<8x32xi32>
      scf.for %arg10 = %c0_i32 to %5 step %1  : i32 {
        %35 = arith.muli %arg10, %c32_i32 : i32
        %36 = tt.splat %35 : (i32) -> tensor<1x32xi32>
        %37 = arith.addi %11, %36 : tensor<1x32xi32>
        %38 = arith.cmpi slt, %37, %8 : tensor<1x32xi32>
        %39 = tt.broadcast %38 : (tensor<1x32xi1>) -> tensor<8x32xi1>
        %40 = arith.andi %39, %28 : tensor<8x32xi1>
        %41 = tt.broadcast %37 : (tensor<1x32xi32>) -> tensor<8x32xi32>
        %42 = arith.addi %41, %30 : tensor<8x32xi32>
        %43 = tt.addptr %19, %42 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        %44 = tt.load %43, %40 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8x32xf64>
        %45 = arith.addi %41, %32 : tensor<8x32xi32>
        %46 = tt.addptr %21, %45 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        %47 = tt.load %46, %40 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8x32xf64>
        %48 = arith.addf %44, %47 : tensor<8x32xf64>
        %49 = arith.addi %41, %34 : tensor<8x32xi32>
        %50 = tt.addptr %23, %49 : tensor<8x32x!tt.ptr<f64, 1>>, tensor<8x32xi32>
        tt.store %50, %48, %40 {cache = 1 : i32, evict = 1 : i32} : tensor<8x32xf64>
      } {programs_loop_x}
    } {programs_loop_y}
    tt.return
  }
}


// -----// IR Dump After LowerHostToCuda (lower-gpu-host-to-cuda) //----- //
#map = affine_map<(d0, d1) -> (d0 * 1024 + d1)>
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%0) : (!llvm.ptr) -> ()
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %c1048576 = arith.constant 1048576 : index
    %1 = call @cudaMallocF64(%c1048576) : (index) -> index
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %c1048576_4 = arith.constant 1048576 : index
    %2 = call @cudaMallocF64(%c1048576_4) : (index) -> index
    %alloc_5 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_6 = memref.collapse_shape %alloc_5 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %c1048576_7 = arith.constant 1048576 : index
    %3 = call @cudaMallocF64(%c1048576_7) : (index) -> index
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %6 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_1, %collapse_shape[%6] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %6 = affine.apply #map(%arg0, %arg1)
        memref.store %cst_0, %collapse_shape_3[%6] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %6 = affine.apply #map(%arg0, %arg1)
        memref.store %cst, %collapse_shape_6[%6] : memref<1048576xf64>
      }
    }
    %c0_i32 = arith.constant 0 : i32
    %c0_8 = arith.constant 0 : index
    %c1_9 = arith.constant 1 : index
    %cast = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast, %c0_8) : (index, memref<?xf64>, index) -> ()
    %c0_10 = arith.constant 0 : index
    %c1_11 = arith.constant 1 : index
    %cast_12 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_12, %c0_10) : (index, memref<?xf64>, index) -> ()
    %c0_13 = arith.constant 0 : index
    %c1_14 = arith.constant 1 : index
    %cast_15 = memref.cast %collapse_shape_6 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_15, %c0_13) : (index, memref<?xf64>, index) -> ()
    %c0_16 = arith.constant 0 : index
    %alloca = memref.alloca() : memref<1xindex>
    memref.store %c128, %alloca[%c0_16] : memref<1xindex>
    %intptr = memref.extract_aligned_pointer_as_index %alloca : memref<1xindex> -> index
    %alloca_17 = memref.alloca() : memref<1xindex>
    memref.store %c32, %alloca_17[%c0_16] : memref<1xindex>
    %intptr_18 = memref.extract_aligned_pointer_as_index %alloca_17 : memref<1xindex> -> index
    %alloca_19 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_19[%c0_16] : memref<1xindex>
    %intptr_20 = memref.extract_aligned_pointer_as_index %alloca_19 : memref<1xindex> -> index
    %alloca_21 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_21[%c0_16] : memref<1xindex>
    %intptr_22 = memref.extract_aligned_pointer_as_index %alloca_21 : memref<1xindex> -> index
    %alloca_23 = memref.alloca() : memref<1xindex>
    memref.store %1, %alloca_23[%c0_16] : memref<1xindex>
    %intptr_24 = memref.extract_aligned_pointer_as_index %alloca_23 : memref<1xindex> -> index
    %alloca_25 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_25[%c0_16] : memref<1xindex>
    %intptr_26 = memref.extract_aligned_pointer_as_index %alloca_25 : memref<1xindex> -> index
    %alloca_27 = memref.alloca() : memref<1xindex>
    memref.store %2, %alloca_27[%c0_16] : memref<1xindex>
    %intptr_28 = memref.extract_aligned_pointer_as_index %alloca_27 : memref<1xindex> -> index
    %alloca_29 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_29[%c0_16] : memref<1xindex>
    %intptr_30 = memref.extract_aligned_pointer_as_index %alloca_29 : memref<1xindex> -> index
    %alloca_31 = memref.alloca() : memref<1xindex>
    memref.store %3, %alloca_31[%c0_16] : memref<1xindex>
    %intptr_32 = memref.extract_aligned_pointer_as_index %alloca_31 : memref<1xindex> -> index
    %alloca_33 = memref.alloca() : memref<9xindex>
    %cast_34 = memref.cast %alloca_33 : memref<9xindex> to memref<?xindex>
    %c0_35 = arith.constant 0 : index
    memref.store %intptr, %alloca_33[%c0_35] : memref<9xindex>
    %c1_36 = arith.constant 1 : index
    memref.store %intptr_18, %alloca_33[%c1_36] : memref<9xindex>
    %c2 = arith.constant 2 : index
    memref.store %intptr_20, %alloca_33[%c2] : memref<9xindex>
    %c3 = arith.constant 3 : index
    memref.store %intptr_22, %alloca_33[%c3] : memref<9xindex>
    %c4 = arith.constant 4 : index
    memref.store %intptr_24, %alloca_33[%c4] : memref<9xindex>
    %c5 = arith.constant 5 : index
    memref.store %intptr_26, %alloca_33[%c5] : memref<9xindex>
    %c6 = arith.constant 6 : index
    memref.store %intptr_28, %alloca_33[%c6] : memref<9xindex>
    %c7 = arith.constant 7 : index
    memref.store %intptr_30, %alloca_33[%c7] : memref<9xindex>
    %c8_37 = arith.constant 8 : index
    memref.store %intptr_32, %alloca_33[%c8_37] : memref<9xindex>
    %4 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %5 = llvm.getelementptr %4[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    %c4_38 = arith.constant 4 : index
    %c32_39 = arith.constant 32 : index
    %c12 = arith.constant 12 : index
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %cast_34, %5, %c12, %c0_i32, %c4_38, %c32_39) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %c0_40 = arith.constant 0 : index
    %c1_41 = arith.constant 1 : index
    %cast_42 = memref.cast %collapse_shape_6 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_42, %c1_41) : (index, memref<?xf64>, index) -> ()
    %c0_43 = arith.constant 0 : index
    %c1_44 = arith.constant 1 : index
    %cast_45 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_45, %c1_44) : (index, memref<?xf64>, index) -> ()
    %c0_46 = arith.constant 0 : index
    %c1_47 = arith.constant 1 : index
    %cast_48 = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast_48, %c1_47) : (index, memref<?xf64>, index) -> ()
    %cast_49 = memref.cast %alloc_5 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast_49) : (memref<*xf64>) -> ()
    call @cudaFree(%1) : (index) -> ()
    call @cudaFree(%2) : (index) -> ()
    call @cudaFree(%3) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMallocI32(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMallocF32(index) -> index

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMallocF32(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMallocI64(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMemcpyI32(index, memref<?xi32>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMemcpyI32(index, memref<?xi32>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMemcpyF64(index, memref<?xf64>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMemcpyF64(index, memref<?xf64>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMallocI32(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMallocF64(index) -> index

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMallocI64(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMemcpyF32(index, memref<?xf32>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMemcpyF32(index, memref<?xf32>, index)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaFree(index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMallocF64(index) -> index

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMemcpyI64(index, memref<?xi64>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMemcpyI64(index, memref<?xi64>, index)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaSetModuleImage(!llvm.ptr)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaFree(index)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func @main() {
  %c12 = arith.constant 12 : index
  %c7 = arith.constant 7 : index
  %c6 = arith.constant 6 : index
  %c5 = arith.constant 5 : index
  %c4 = arith.constant 4 : index
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c0_i32 = arith.constant 0 : i32
  %c1048576 = arith.constant 1048576 : index
  %cst = arith.constant 2.200000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 0.000000e+00 : f64
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1024 = arith.constant 1024 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c128 = arith.constant 128 : index
  %0 = llvm.mlir.addressof @ptx : !llvm.ptr
  call @cudaSetModuleImage(%0) : (!llvm.ptr) -> ()
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %1 = call @cudaMallocF64(%c1048576) : (index) -> index
  %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %2 = call @cudaMallocF64(%c1048576) : (index) -> index
  %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %3 = call @cudaMallocF64(%c1048576) : (index) -> index
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst, %collapse_shape[%6] : memref<1048576xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst_0, %collapse_shape_3[%6] : memref<1048576xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst_1, %collapse_shape_5[%6] : memref<1048576xf64>
    }
  }
  %cast = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%1, %cast, %c0) : (index, memref<?xf64>, index) -> ()
  %cast_6 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%2, %cast_6, %c0) : (index, memref<?xf64>, index) -> ()
  %cast_7 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%3, %cast_7, %c0) : (index, memref<?xf64>, index) -> ()
  %alloca = memref.alloca() : memref<1xindex>
  memref.store %c128, %alloca[%c0] : memref<1xindex>
  %intptr = memref.extract_aligned_pointer_as_index %alloca : memref<1xindex> -> index
  %alloca_8 = memref.alloca() : memref<1xindex>
  memref.store %c32, %alloca_8[%c0] : memref<1xindex>
  %intptr_9 = memref.extract_aligned_pointer_as_index %alloca_8 : memref<1xindex> -> index
  %alloca_10 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_10[%c0] : memref<1xindex>
  %intptr_11 = memref.extract_aligned_pointer_as_index %alloca_10 : memref<1xindex> -> index
  %alloca_12 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_12[%c0] : memref<1xindex>
  %intptr_13 = memref.extract_aligned_pointer_as_index %alloca_12 : memref<1xindex> -> index
  %alloca_14 = memref.alloca() : memref<1xindex>
  memref.store %1, %alloca_14[%c0] : memref<1xindex>
  %intptr_15 = memref.extract_aligned_pointer_as_index %alloca_14 : memref<1xindex> -> index
  %alloca_16 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_16[%c0] : memref<1xindex>
  %intptr_17 = memref.extract_aligned_pointer_as_index %alloca_16 : memref<1xindex> -> index
  %alloca_18 = memref.alloca() : memref<1xindex>
  memref.store %2, %alloca_18[%c0] : memref<1xindex>
  %intptr_19 = memref.extract_aligned_pointer_as_index %alloca_18 : memref<1xindex> -> index
  %alloca_20 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_20[%c0] : memref<1xindex>
  %intptr_21 = memref.extract_aligned_pointer_as_index %alloca_20 : memref<1xindex> -> index
  %alloca_22 = memref.alloca() : memref<1xindex>
  memref.store %3, %alloca_22[%c0] : memref<1xindex>
  %intptr_23 = memref.extract_aligned_pointer_as_index %alloca_22 : memref<1xindex> -> index
  %alloca_24 = memref.alloca() : memref<9xindex>
  %cast_25 = memref.cast %alloca_24 : memref<9xindex> to memref<?xindex>
  memref.store %intptr, %alloca_24[%c0] : memref<9xindex>
  memref.store %intptr_9, %alloca_24[%c1] : memref<9xindex>
  memref.store %intptr_11, %alloca_24[%c2] : memref<9xindex>
  memref.store %intptr_13, %alloca_24[%c3] : memref<9xindex>
  memref.store %intptr_15, %alloca_24[%c4] : memref<9xindex>
  memref.store %intptr_17, %alloca_24[%c5] : memref<9xindex>
  memref.store %intptr_19, %alloca_24[%c6] : memref<9xindex>
  memref.store %intptr_21, %alloca_24[%c7] : memref<9xindex>
  memref.store %intptr_23, %alloca_24[%c8] : memref<9xindex>
  %4 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
  %5 = llvm.getelementptr %4[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
  call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %cast_25, %5, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
  %cast_26 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%3, %cast_26, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_27 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%2, %cast_27, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_28 = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%1, %cast_28, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_29 = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast_29) : (memref<*xf64>) -> ()
  call @cudaFree(%1) : (index) -> ()
  call @cudaFree(%2) : (index) -> ()
  call @cudaFree(%3) : (index) -> ()
  return
}

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaSetModuleImage(!llvm.ptr)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @main() {
  %c12 = arith.constant 12 : index
  %c7 = arith.constant 7 : index
  %c6 = arith.constant 6 : index
  %c5 = arith.constant 5 : index
  %c4 = arith.constant 4 : index
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c0_i32 = arith.constant 0 : i32
  %c1048576 = arith.constant 1048576 : index
  %cst = arith.constant 2.200000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 0.000000e+00 : f64
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1024 = arith.constant 1024 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c128 = arith.constant 128 : index
  %0 = llvm.mlir.addressof @ptx : !llvm.ptr
  call @cudaSetModuleImage(%0) : (!llvm.ptr) -> ()
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %1 = call @cudaMallocF64(%c1048576) : (index) -> index
  %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %2 = call @cudaMallocF64(%c1048576) : (index) -> index
  %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %3 = call @cudaMallocF64(%c1048576) : (index) -> index
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst, %collapse_shape[%6] : memref<1048576xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst_0, %collapse_shape_3[%6] : memref<1048576xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %6 = affine.apply affine_map<(d0, d1) -> (d0 * 1024 + d1)>(%arg0, %arg1)
      memref.store %cst_1, %collapse_shape_5[%6] : memref<1048576xf64>
    }
  }
  %cast = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%1, %cast, %c0) : (index, memref<?xf64>, index) -> ()
  %cast_6 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%2, %cast_6, %c0) : (index, memref<?xf64>, index) -> ()
  %cast_7 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%3, %cast_7, %c0) : (index, memref<?xf64>, index) -> ()
  %alloca = memref.alloca() : memref<1xindex>
  memref.store %c128, %alloca[%c0] : memref<1xindex>
  %intptr = memref.extract_aligned_pointer_as_index %alloca : memref<1xindex> -> index
  %alloca_8 = memref.alloca() : memref<1xindex>
  memref.store %c32, %alloca_8[%c0] : memref<1xindex>
  %intptr_9 = memref.extract_aligned_pointer_as_index %alloca_8 : memref<1xindex> -> index
  %alloca_10 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_10[%c0] : memref<1xindex>
  %intptr_11 = memref.extract_aligned_pointer_as_index %alloca_10 : memref<1xindex> -> index
  %alloca_12 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_12[%c0] : memref<1xindex>
  %intptr_13 = memref.extract_aligned_pointer_as_index %alloca_12 : memref<1xindex> -> index
  %alloca_14 = memref.alloca() : memref<1xindex>
  memref.store %1, %alloca_14[%c0] : memref<1xindex>
  %intptr_15 = memref.extract_aligned_pointer_as_index %alloca_14 : memref<1xindex> -> index
  %alloca_16 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_16[%c0] : memref<1xindex>
  %intptr_17 = memref.extract_aligned_pointer_as_index %alloca_16 : memref<1xindex> -> index
  %alloca_18 = memref.alloca() : memref<1xindex>
  memref.store %2, %alloca_18[%c0] : memref<1xindex>
  %intptr_19 = memref.extract_aligned_pointer_as_index %alloca_18 : memref<1xindex> -> index
  %alloca_20 = memref.alloca() : memref<1xindex>
  memref.store %c1024, %alloca_20[%c0] : memref<1xindex>
  %intptr_21 = memref.extract_aligned_pointer_as_index %alloca_20 : memref<1xindex> -> index
  %alloca_22 = memref.alloca() : memref<1xindex>
  memref.store %3, %alloca_22[%c0] : memref<1xindex>
  %intptr_23 = memref.extract_aligned_pointer_as_index %alloca_22 : memref<1xindex> -> index
  %alloca_24 = memref.alloca() : memref<9xindex>
  %cast_25 = memref.cast %alloca_24 : memref<9xindex> to memref<?xindex>
  memref.store %intptr, %alloca_24[%c0] : memref<9xindex>
  memref.store %intptr_9, %alloca_24[%c1] : memref<9xindex>
  memref.store %intptr_11, %alloca_24[%c2] : memref<9xindex>
  memref.store %intptr_13, %alloca_24[%c3] : memref<9xindex>
  memref.store %intptr_15, %alloca_24[%c4] : memref<9xindex>
  memref.store %intptr_17, %alloca_24[%c5] : memref<9xindex>
  memref.store %intptr_19, %alloca_24[%c6] : memref<9xindex>
  memref.store %intptr_21, %alloca_24[%c7] : memref<9xindex>
  memref.store %intptr_23, %alloca_24[%c8] : memref<9xindex>
  %4 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
  %5 = llvm.getelementptr %4[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
  call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %cast_25, %5, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
  %cast_26 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%3, %cast_26, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_27 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%2, %cast_27, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_28 = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
  call @cudaMemcpyF64(%1, %cast_28, %c1) : (index, memref<?xf64>, index) -> ()
  %cast_29 = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast_29) : (memref<*xf64>) -> ()
  call @cudaFree(%1) : (index) -> ()
  call @cudaFree(%2) : (index) -> ()
  call @cudaFree(%3) : (index) -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %c6 = arith.constant 6 : index
    %c5 = arith.constant 5 : index
    %c4 = arith.constant 4 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c0_i32 = arith.constant 0 : i32
    %c1048576 = arith.constant 1048576 : index
    %cst = arith.constant 2.200000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 0.000000e+00 : f64
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c128 = arith.constant 128 : index
    %0 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%0) : (!llvm.ptr) -> ()
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %1 = call @cudaMallocF64(%c1048576) : (index) -> index
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %2 = call @cudaMallocF64(%c1048576) : (index) -> index
    %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %3 = call @cudaMallocF64(%c1048576) : (index) -> index
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst, %collapse_shape[%7] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst_0, %collapse_shape_3[%7] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst_1, %collapse_shape_5[%7] : memref<1048576xf64>
      }
    }
    %cast = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast, %c0) : (index, memref<?xf64>, index) -> ()
    %cast_6 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_6, %c0) : (index, memref<?xf64>, index) -> ()
    %cast_7 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_7, %c0) : (index, memref<?xf64>, index) -> ()
    %alloca = memref.alloca() : memref<1xindex>
    memref.store %c128, %alloca[%c0] : memref<1xindex>
    %intptr = memref.extract_aligned_pointer_as_index %alloca : memref<1xindex> -> index
    %alloca_8 = memref.alloca() : memref<1xindex>
    memref.store %c32, %alloca_8[%c0] : memref<1xindex>
    %intptr_9 = memref.extract_aligned_pointer_as_index %alloca_8 : memref<1xindex> -> index
    %alloca_10 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_10[%c0] : memref<1xindex>
    %intptr_11 = memref.extract_aligned_pointer_as_index %alloca_10 : memref<1xindex> -> index
    %alloca_12 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_12[%c0] : memref<1xindex>
    %intptr_13 = memref.extract_aligned_pointer_as_index %alloca_12 : memref<1xindex> -> index
    %alloca_14 = memref.alloca() : memref<1xindex>
    memref.store %1, %alloca_14[%c0] : memref<1xindex>
    %intptr_15 = memref.extract_aligned_pointer_as_index %alloca_14 : memref<1xindex> -> index
    %alloca_16 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_16[%c0] : memref<1xindex>
    %intptr_17 = memref.extract_aligned_pointer_as_index %alloca_16 : memref<1xindex> -> index
    %alloca_18 = memref.alloca() : memref<1xindex>
    memref.store %2, %alloca_18[%c0] : memref<1xindex>
    %intptr_19 = memref.extract_aligned_pointer_as_index %alloca_18 : memref<1xindex> -> index
    %alloca_20 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_20[%c0] : memref<1xindex>
    %intptr_21 = memref.extract_aligned_pointer_as_index %alloca_20 : memref<1xindex> -> index
    %alloca_22 = memref.alloca() : memref<1xindex>
    memref.store %3, %alloca_22[%c0] : memref<1xindex>
    %intptr_23 = memref.extract_aligned_pointer_as_index %alloca_22 : memref<1xindex> -> index
    %alloca_24 = memref.alloca() : memref<9xindex>
    %cast_25 = memref.cast %alloca_24 : memref<9xindex> to memref<?xindex>
    memref.store %intptr, %alloca_24[%c0] : memref<9xindex>
    memref.store %intptr_9, %alloca_24[%c1] : memref<9xindex>
    memref.store %intptr_11, %alloca_24[%c2] : memref<9xindex>
    memref.store %intptr_13, %alloca_24[%c3] : memref<9xindex>
    memref.store %intptr_15, %alloca_24[%c4] : memref<9xindex>
    memref.store %intptr_17, %alloca_24[%c5] : memref<9xindex>
    memref.store %intptr_19, %alloca_24[%c6] : memref<9xindex>
    memref.store %intptr_21, %alloca_24[%c7] : memref<9xindex>
    memref.store %intptr_23, %alloca_24[%c8] : memref<9xindex>
    %4 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %5 = llvm.getelementptr %4[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %cast_25, %5, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %cast_26 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_26, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_27 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_27, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_28 = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast_28, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_29 = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast_29) : (memref<*xf64>) -> ()
    call @cudaFree(%1) : (index) -> ()
    call @cudaFree(%2) : (index) -> ()
    call @cudaFree(%3) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertSCFToOpenMPPass (convert-scf-to-openmp) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %c6 = arith.constant 6 : index
    %c5 = arith.constant 5 : index
    %c4 = arith.constant 4 : index
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c0_i32 = arith.constant 0 : i32
    %c1048576 = arith.constant 1048576 : index
    %cst = arith.constant 2.200000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 0.000000e+00 : f64
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c128 = arith.constant 128 : index
    %0 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%0) : (!llvm.ptr) -> ()
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %alloc [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %1 = call @cudaMallocF64(%c1048576) : (index) -> index
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %alloc_2 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %2 = call @cudaMallocF64(%c1048576) : (index) -> index
    %alloc_4 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %collapse_shape_5 = memref.collapse_shape %alloc_4 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %3 = call @cudaMallocF64(%c1048576) : (index) -> index
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst, %collapse_shape[%7] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst_0, %collapse_shape_3[%7] : memref<1048576xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_30 = arith.constant 1024 : index
        %6 = arith.muli %arg0, %c1024_30 : index
        %7 = arith.addi %6, %arg1 : index
        memref.store %cst_1, %collapse_shape_5[%7] : memref<1048576xf64>
      }
    }
    %cast = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast, %c0) : (index, memref<?xf64>, index) -> ()
    %cast_6 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_6, %c0) : (index, memref<?xf64>, index) -> ()
    %cast_7 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_7, %c0) : (index, memref<?xf64>, index) -> ()
    %alloca = memref.alloca() : memref<1xindex>
    memref.store %c128, %alloca[%c0] : memref<1xindex>
    %intptr = memref.extract_aligned_pointer_as_index %alloca : memref<1xindex> -> index
    %alloca_8 = memref.alloca() : memref<1xindex>
    memref.store %c32, %alloca_8[%c0] : memref<1xindex>
    %intptr_9 = memref.extract_aligned_pointer_as_index %alloca_8 : memref<1xindex> -> index
    %alloca_10 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_10[%c0] : memref<1xindex>
    %intptr_11 = memref.extract_aligned_pointer_as_index %alloca_10 : memref<1xindex> -> index
    %alloca_12 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_12[%c0] : memref<1xindex>
    %intptr_13 = memref.extract_aligned_pointer_as_index %alloca_12 : memref<1xindex> -> index
    %alloca_14 = memref.alloca() : memref<1xindex>
    memref.store %1, %alloca_14[%c0] : memref<1xindex>
    %intptr_15 = memref.extract_aligned_pointer_as_index %alloca_14 : memref<1xindex> -> index
    %alloca_16 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_16[%c0] : memref<1xindex>
    %intptr_17 = memref.extract_aligned_pointer_as_index %alloca_16 : memref<1xindex> -> index
    %alloca_18 = memref.alloca() : memref<1xindex>
    memref.store %2, %alloca_18[%c0] : memref<1xindex>
    %intptr_19 = memref.extract_aligned_pointer_as_index %alloca_18 : memref<1xindex> -> index
    %alloca_20 = memref.alloca() : memref<1xindex>
    memref.store %c1024, %alloca_20[%c0] : memref<1xindex>
    %intptr_21 = memref.extract_aligned_pointer_as_index %alloca_20 : memref<1xindex> -> index
    %alloca_22 = memref.alloca() : memref<1xindex>
    memref.store %3, %alloca_22[%c0] : memref<1xindex>
    %intptr_23 = memref.extract_aligned_pointer_as_index %alloca_22 : memref<1xindex> -> index
    %alloca_24 = memref.alloca() : memref<9xindex>
    %cast_25 = memref.cast %alloca_24 : memref<9xindex> to memref<?xindex>
    memref.store %intptr, %alloca_24[%c0] : memref<9xindex>
    memref.store %intptr_9, %alloca_24[%c1] : memref<9xindex>
    memref.store %intptr_11, %alloca_24[%c2] : memref<9xindex>
    memref.store %intptr_13, %alloca_24[%c3] : memref<9xindex>
    memref.store %intptr_15, %alloca_24[%c4] : memref<9xindex>
    memref.store %intptr_17, %alloca_24[%c5] : memref<9xindex>
    memref.store %intptr_19, %alloca_24[%c6] : memref<9xindex>
    memref.store %intptr_21, %alloca_24[%c7] : memref<9xindex>
    memref.store %intptr_23, %alloca_24[%c8] : memref<9xindex>
    %4 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %5 = llvm.getelementptr %4[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %cast_25, %5, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %cast_26 = memref.cast %collapse_shape_5 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%3, %cast_26, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_27 = memref.cast %collapse_shape_3 : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%2, %cast_27, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_28 = memref.cast %collapse_shape : memref<1048576xf64> to memref<?xf64>
    call @cudaMemcpyF64(%1, %cast_28, %c1) : (index, memref<?xf64>, index) -> ()
    %cast_29 = memref.cast %alloc_4 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast_29) : (memref<*xf64>) -> ()
    call @cudaFree(%1) : (index) -> ()
    call @cudaFree(%2) : (index) -> ()
    call @cudaFree(%3) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %0 = builtin.unrealized_conversion_cast %c7 : index to i64
    %c6 = arith.constant 6 : index
    %1 = builtin.unrealized_conversion_cast %c6 : index to i64
    %c5 = arith.constant 5 : index
    %2 = builtin.unrealized_conversion_cast %c5 : index to i64
    %c4 = arith.constant 4 : index
    %3 = builtin.unrealized_conversion_cast %c4 : index to i64
    %c3 = arith.constant 3 : index
    %4 = builtin.unrealized_conversion_cast %c3 : index to i64
    %c2 = arith.constant 2 : index
    %5 = builtin.unrealized_conversion_cast %c2 : index to i64
    %c0_i32 = arith.constant 0 : i32
    %c1048576 = arith.constant 1048576 : index
    %cst = arith.constant 2.200000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 0.000000e+00 : f64
    %c0 = arith.constant 0 : index
    %6 = builtin.unrealized_conversion_cast %c0 : index to i64
    %c1 = arith.constant 1 : index
    %7 = builtin.unrealized_conversion_cast %c1 : index to i64
    %c1024 = arith.constant 1024 : index
    %8 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %c8 = arith.constant 8 : index
    %9 = builtin.unrealized_conversion_cast %c8 : index to i64
    %c32 = arith.constant 32 : index
    %10 = builtin.unrealized_conversion_cast %c32 : index to i64
    %c128 = arith.constant 128 : index
    %11 = builtin.unrealized_conversion_cast %c128 : index to i64
    %12 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%12) : (!llvm.ptr) -> ()
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(1024 : index) : i64
    %15 = llvm.mlir.constant(1 : index) : i64
    %16 = llvm.mlir.constant(1048576 : index) : i64
    %17 = llvm.mlir.zero : !llvm.ptr
    %18 = llvm.getelementptr %17[%16] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %19 = llvm.ptrtoint %18 : !llvm.ptr to i64
    %20 = llvm.mlir.constant(32 : index) : i64
    %21 = llvm.add %19, %20  : i64
    %22 = llvm.call @malloc(%21) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.mlir.constant(1 : index) : i64
    %25 = llvm.sub %20, %24  : i64
    %26 = llvm.add %23, %25  : i64
    %27 = llvm.urem %26, %20  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %22, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.mlir.constant(0 : index) : i64
    %34 = llvm.insertvalue %33, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %13, %34[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %14, %35[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %14, %36[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %15, %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = builtin.unrealized_conversion_cast %38 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %39 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %40 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %41 = call @cudaMallocF64(%c1048576) : (index) -> index
    %42 = builtin.unrealized_conversion_cast %41 : index to i64
    %43 = llvm.mlir.constant(1024 : index) : i64
    %44 = llvm.mlir.constant(1024 : index) : i64
    %45 = llvm.mlir.constant(1 : index) : i64
    %46 = llvm.mlir.constant(1048576 : index) : i64
    %47 = llvm.mlir.zero : !llvm.ptr
    %48 = llvm.getelementptr %47[%46] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %49 = llvm.ptrtoint %48 : !llvm.ptr to i64
    %50 = llvm.mlir.constant(32 : index) : i64
    %51 = llvm.add %49, %50  : i64
    %52 = llvm.call @malloc(%51) : (i64) -> !llvm.ptr
    %53 = llvm.ptrtoint %52 : !llvm.ptr to i64
    %54 = llvm.mlir.constant(1 : index) : i64
    %55 = llvm.sub %50, %54  : i64
    %56 = llvm.add %53, %55  : i64
    %57 = llvm.urem %56, %50  : i64
    %58 = llvm.sub %56, %57  : i64
    %59 = llvm.inttoptr %58 : i64 to !llvm.ptr
    %60 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %61 = llvm.insertvalue %52, %60[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.mlir.constant(0 : index) : i64
    %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.insertvalue %43, %64[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %44, %65[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %44, %66[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %45, %67[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = builtin.unrealized_conversion_cast %68 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %69 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %70 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %71 = call @cudaMallocF64(%c1048576) : (index) -> index
    %72 = builtin.unrealized_conversion_cast %71 : index to i64
    %73 = llvm.mlir.constant(1024 : index) : i64
    %74 = llvm.mlir.constant(1024 : index) : i64
    %75 = llvm.mlir.constant(1 : index) : i64
    %76 = llvm.mlir.constant(1048576 : index) : i64
    %77 = llvm.mlir.zero : !llvm.ptr
    %78 = llvm.getelementptr %77[%76] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %79 = llvm.ptrtoint %78 : !llvm.ptr to i64
    %80 = llvm.mlir.constant(32 : index) : i64
    %81 = llvm.add %79, %80  : i64
    %82 = llvm.call @malloc(%81) : (i64) -> !llvm.ptr
    %83 = llvm.ptrtoint %82 : !llvm.ptr to i64
    %84 = llvm.mlir.constant(1 : index) : i64
    %85 = llvm.sub %80, %84  : i64
    %86 = llvm.add %83, %85  : i64
    %87 = llvm.urem %86, %80  : i64
    %88 = llvm.sub %86, %87  : i64
    %89 = llvm.inttoptr %88 : i64 to !llvm.ptr
    %90 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %91 = llvm.insertvalue %82, %90[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %89, %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.mlir.constant(0 : index) : i64
    %94 = llvm.insertvalue %93, %92[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = llvm.insertvalue %73, %94[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = llvm.insertvalue %74, %95[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %97 = llvm.insertvalue %74, %96[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.insertvalue %75, %97[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %99 = builtin.unrealized_conversion_cast %98 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %99 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %100 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %101 = call @cudaMallocF64(%c1048576) : (index) -> index
    %102 = builtin.unrealized_conversion_cast %101 : index to i64
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_4 = arith.constant 1024 : index
        %273 = arith.muli %arg0, %c1024_4 : index
        %274 = arith.addi %273, %arg1 : index
        %275 = builtin.unrealized_conversion_cast %274 : index to i64
        %276 = llvm.extractvalue %40[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %277 = llvm.getelementptr %276[%275] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst, %277 : f64, !llvm.ptr
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_4 = arith.constant 1024 : index
        %273 = arith.muli %arg0, %c1024_4 : index
        %274 = arith.addi %273, %arg1 : index
        %275 = builtin.unrealized_conversion_cast %274 : index to i64
        %276 = llvm.extractvalue %70[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %277 = llvm.getelementptr %276[%275] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst_0, %277 : f64, !llvm.ptr
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %c1024_4 = arith.constant 1024 : index
        %273 = arith.muli %arg0, %c1024_4 : index
        %274 = arith.addi %273, %arg1 : index
        %275 = builtin.unrealized_conversion_cast %274 : index to i64
        %276 = llvm.extractvalue %100[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %277 = llvm.getelementptr %276[%275] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst_1, %277 : f64, !llvm.ptr
      }
    }
    %103 = builtin.unrealized_conversion_cast %40 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%41, %103, %c0) : (index, memref<?xf64>, index) -> ()
    %104 = builtin.unrealized_conversion_cast %70 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%71, %104, %c0) : (index, memref<?xf64>, index) -> ()
    %105 = builtin.unrealized_conversion_cast %100 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%101, %105, %c0) : (index, memref<?xf64>, index) -> ()
    %106 = llvm.mlir.constant(1 : index) : i64
    %107 = llvm.mlir.constant(1 : index) : i64
    %108 = llvm.alloca %106 x i64 : (i64) -> !llvm.ptr
    %109 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %110 = llvm.insertvalue %108, %109[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %111 = llvm.insertvalue %108, %110[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %112 = llvm.mlir.constant(0 : index) : i64
    %113 = llvm.insertvalue %112, %111[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %114 = llvm.insertvalue %106, %113[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %115 = llvm.insertvalue %107, %114[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %116 = llvm.extractvalue %115[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %117 = llvm.getelementptr %116[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %11, %117 : i64, !llvm.ptr
    %118 = llvm.extractvalue %115[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %119 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %120 = llvm.mlir.constant(1 : index) : i64
    %121 = llvm.mlir.constant(1 : index) : i64
    %122 = llvm.alloca %120 x i64 : (i64) -> !llvm.ptr
    %123 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %124 = llvm.insertvalue %122, %123[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %125 = llvm.insertvalue %122, %124[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %126 = llvm.mlir.constant(0 : index) : i64
    %127 = llvm.insertvalue %126, %125[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %128 = llvm.insertvalue %120, %127[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %129 = llvm.insertvalue %121, %128[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %130 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %131 = llvm.getelementptr %130[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %10, %131 : i64, !llvm.ptr
    %132 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
    %134 = llvm.mlir.constant(1 : index) : i64
    %135 = llvm.mlir.constant(1 : index) : i64
    %136 = llvm.alloca %134 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %138 = llvm.insertvalue %136, %137[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %139 = llvm.insertvalue %136, %138[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %140 = llvm.mlir.constant(0 : index) : i64
    %141 = llvm.insertvalue %140, %139[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %134, %141[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %135, %142[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.getelementptr %144[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %145 : i64, !llvm.ptr
    %146 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
    %148 = llvm.mlir.constant(1 : index) : i64
    %149 = llvm.mlir.constant(1 : index) : i64
    %150 = llvm.alloca %148 x i64 : (i64) -> !llvm.ptr
    %151 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %152 = llvm.insertvalue %150, %151[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %153 = llvm.insertvalue %150, %152[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %154 = llvm.mlir.constant(0 : index) : i64
    %155 = llvm.insertvalue %154, %153[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %156 = llvm.insertvalue %148, %155[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %157 = llvm.insertvalue %149, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %158 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %159 = llvm.getelementptr %158[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %159 : i64, !llvm.ptr
    %160 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %161 = llvm.ptrtoint %160 : !llvm.ptr to i64
    %162 = llvm.mlir.constant(1 : index) : i64
    %163 = llvm.mlir.constant(1 : index) : i64
    %164 = llvm.alloca %162 x i64 : (i64) -> !llvm.ptr
    %165 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %166 = llvm.insertvalue %164, %165[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %167 = llvm.insertvalue %164, %166[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %168 = llvm.mlir.constant(0 : index) : i64
    %169 = llvm.insertvalue %168, %167[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %170 = llvm.insertvalue %162, %169[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %171 = llvm.insertvalue %163, %170[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %172 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %173 = llvm.getelementptr %172[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %42, %173 : i64, !llvm.ptr
    %174 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
    %176 = llvm.mlir.constant(1 : index) : i64
    %177 = llvm.mlir.constant(1 : index) : i64
    %178 = llvm.alloca %176 x i64 : (i64) -> !llvm.ptr
    %179 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %180 = llvm.insertvalue %178, %179[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %181 = llvm.insertvalue %178, %180[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %182 = llvm.mlir.constant(0 : index) : i64
    %183 = llvm.insertvalue %182, %181[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %184 = llvm.insertvalue %176, %183[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %185 = llvm.insertvalue %177, %184[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %186 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %187 = llvm.getelementptr %186[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %187 : i64, !llvm.ptr
    %188 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.ptrtoint %188 : !llvm.ptr to i64
    %190 = llvm.mlir.constant(1 : index) : i64
    %191 = llvm.mlir.constant(1 : index) : i64
    %192 = llvm.alloca %190 x i64 : (i64) -> !llvm.ptr
    %193 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %194 = llvm.insertvalue %192, %193[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %195 = llvm.insertvalue %192, %194[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %196 = llvm.mlir.constant(0 : index) : i64
    %197 = llvm.insertvalue %196, %195[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.insertvalue %190, %197[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %199 = llvm.insertvalue %191, %198[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %200 = llvm.extractvalue %199[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %201 = llvm.getelementptr %200[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %72, %201 : i64, !llvm.ptr
    %202 = llvm.extractvalue %199[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %203 = llvm.ptrtoint %202 : !llvm.ptr to i64
    %204 = llvm.mlir.constant(1 : index) : i64
    %205 = llvm.mlir.constant(1 : index) : i64
    %206 = llvm.alloca %204 x i64 : (i64) -> !llvm.ptr
    %207 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %208 = llvm.insertvalue %206, %207[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %209 = llvm.insertvalue %206, %208[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %210 = llvm.mlir.constant(0 : index) : i64
    %211 = llvm.insertvalue %210, %209[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %212 = llvm.insertvalue %204, %211[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %213 = llvm.insertvalue %205, %212[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %214 = llvm.extractvalue %213[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %215 = llvm.getelementptr %214[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %215 : i64, !llvm.ptr
    %216 = llvm.extractvalue %213[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %217 = llvm.ptrtoint %216 : !llvm.ptr to i64
    %218 = llvm.mlir.constant(1 : index) : i64
    %219 = llvm.mlir.constant(1 : index) : i64
    %220 = llvm.alloca %218 x i64 : (i64) -> !llvm.ptr
    %221 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %222 = llvm.insertvalue %220, %221[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %223 = llvm.insertvalue %220, %222[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %224 = llvm.mlir.constant(0 : index) : i64
    %225 = llvm.insertvalue %224, %223[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %226 = llvm.insertvalue %218, %225[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %227 = llvm.insertvalue %219, %226[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %228 = llvm.extractvalue %227[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %229 = llvm.getelementptr %228[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %102, %229 : i64, !llvm.ptr
    %230 = llvm.extractvalue %227[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %231 = llvm.ptrtoint %230 : !llvm.ptr to i64
    %232 = llvm.mlir.constant(9 : index) : i64
    %233 = llvm.mlir.constant(1 : index) : i64
    %234 = llvm.alloca %232 x i64 : (i64) -> !llvm.ptr
    %235 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %236 = llvm.insertvalue %234, %235[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %237 = llvm.insertvalue %234, %236[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %238 = llvm.mlir.constant(0 : index) : i64
    %239 = llvm.insertvalue %238, %237[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %240 = llvm.insertvalue %232, %239[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %241 = llvm.insertvalue %233, %240[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %242 = builtin.unrealized_conversion_cast %241 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %243 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %244 = llvm.getelementptr %243[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %119, %244 : i64, !llvm.ptr
    %245 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %246 = llvm.getelementptr %245[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %133, %246 : i64, !llvm.ptr
    %247 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %248 = llvm.getelementptr %247[%5] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %147, %248 : i64, !llvm.ptr
    %249 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %250 = llvm.getelementptr %249[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %161, %250 : i64, !llvm.ptr
    %251 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %252 = llvm.getelementptr %251[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %175, %252 : i64, !llvm.ptr
    %253 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %254 = llvm.getelementptr %253[%2] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %189, %254 : i64, !llvm.ptr
    %255 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %256 = llvm.getelementptr %255[%1] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %203, %256 : i64, !llvm.ptr
    %257 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %258 = llvm.getelementptr %257[%0] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %217, %258 : i64, !llvm.ptr
    %259 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %260 = llvm.getelementptr %259[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %231, %260 : i64, !llvm.ptr
    %261 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %262 = llvm.getelementptr %261[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %242, %262, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %263 = builtin.unrealized_conversion_cast %100 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%101, %263, %c1) : (index, memref<?xf64>, index) -> ()
    %264 = builtin.unrealized_conversion_cast %70 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%71, %264, %c1) : (index, memref<?xf64>, index) -> ()
    %265 = builtin.unrealized_conversion_cast %40 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%41, %265, %c1) : (index, memref<?xf64>, index) -> ()
    %266 = llvm.mlir.constant(1 : index) : i64
    %267 = llvm.alloca %266 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %98, %267 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %268 = llvm.mlir.constant(2 : index) : i64
    %269 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %270 = llvm.insertvalue %268, %269[0] : !llvm.struct<(i64, ptr)> 
    %271 = llvm.insertvalue %267, %270[1] : !llvm.struct<(i64, ptr)> 
    %272 = builtin.unrealized_conversion_cast %271 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%272) : (memref<*xf64>) -> ()
    call @cudaFree(%41) : (index) -> ()
    call @cudaFree(%71) : (index) -> ()
    call @cudaFree(%101) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %0 = builtin.unrealized_conversion_cast %c7 : index to i64
    %c6 = arith.constant 6 : index
    %1 = builtin.unrealized_conversion_cast %c6 : index to i64
    %c5 = arith.constant 5 : index
    %2 = builtin.unrealized_conversion_cast %c5 : index to i64
    %c4 = arith.constant 4 : index
    %3 = builtin.unrealized_conversion_cast %c4 : index to i64
    %c3 = arith.constant 3 : index
    %4 = builtin.unrealized_conversion_cast %c3 : index to i64
    %c2 = arith.constant 2 : index
    %5 = builtin.unrealized_conversion_cast %c2 : index to i64
    %c0_i32 = arith.constant 0 : i32
    %c1048576 = arith.constant 1048576 : index
    %cst = arith.constant 2.200000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 0.000000e+00 : f64
    %c0 = arith.constant 0 : index
    %6 = builtin.unrealized_conversion_cast %c0 : index to i64
    %c1 = arith.constant 1 : index
    %7 = builtin.unrealized_conversion_cast %c1 : index to i64
    %c1024 = arith.constant 1024 : index
    %8 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %c8 = arith.constant 8 : index
    %9 = builtin.unrealized_conversion_cast %c8 : index to i64
    %c32 = arith.constant 32 : index
    %10 = builtin.unrealized_conversion_cast %c32 : index to i64
    %c128 = arith.constant 128 : index
    %11 = builtin.unrealized_conversion_cast %c128 : index to i64
    %12 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%12) : (!llvm.ptr) -> ()
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(1024 : index) : i64
    %15 = llvm.mlir.constant(1 : index) : i64
    %16 = llvm.mlir.constant(1048576 : index) : i64
    %17 = llvm.mlir.zero : !llvm.ptr
    %18 = llvm.getelementptr %17[%16] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %19 = llvm.ptrtoint %18 : !llvm.ptr to i64
    %20 = llvm.mlir.constant(32 : index) : i64
    %21 = llvm.add %19, %20  : i64
    %22 = llvm.call @malloc(%21) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.mlir.constant(1 : index) : i64
    %25 = llvm.sub %20, %24  : i64
    %26 = llvm.add %23, %25  : i64
    %27 = llvm.urem %26, %20  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %22, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.mlir.constant(0 : index) : i64
    %34 = llvm.insertvalue %33, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %13, %34[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %14, %35[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %14, %36[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %15, %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = builtin.unrealized_conversion_cast %38 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %39 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %40 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %41 = call @cudaMallocF64(%c1048576) : (index) -> index
    %42 = builtin.unrealized_conversion_cast %41 : index to i64
    %43 = llvm.mlir.constant(1024 : index) : i64
    %44 = llvm.mlir.constant(1024 : index) : i64
    %45 = llvm.mlir.constant(1 : index) : i64
    %46 = llvm.mlir.constant(1048576 : index) : i64
    %47 = llvm.mlir.zero : !llvm.ptr
    %48 = llvm.getelementptr %47[%46] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %49 = llvm.ptrtoint %48 : !llvm.ptr to i64
    %50 = llvm.mlir.constant(32 : index) : i64
    %51 = llvm.add %49, %50  : i64
    %52 = llvm.call @malloc(%51) : (i64) -> !llvm.ptr
    %53 = llvm.ptrtoint %52 : !llvm.ptr to i64
    %54 = llvm.mlir.constant(1 : index) : i64
    %55 = llvm.sub %50, %54  : i64
    %56 = llvm.add %53, %55  : i64
    %57 = llvm.urem %56, %50  : i64
    %58 = llvm.sub %56, %57  : i64
    %59 = llvm.inttoptr %58 : i64 to !llvm.ptr
    %60 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %61 = llvm.insertvalue %52, %60[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.mlir.constant(0 : index) : i64
    %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.insertvalue %43, %64[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %44, %65[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %44, %66[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %45, %67[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = builtin.unrealized_conversion_cast %68 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %69 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %70 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %71 = call @cudaMallocF64(%c1048576) : (index) -> index
    %72 = builtin.unrealized_conversion_cast %71 : index to i64
    %73 = llvm.mlir.constant(1024 : index) : i64
    %74 = llvm.mlir.constant(1024 : index) : i64
    %75 = llvm.mlir.constant(1 : index) : i64
    %76 = llvm.mlir.constant(1048576 : index) : i64
    %77 = llvm.mlir.zero : !llvm.ptr
    %78 = llvm.getelementptr %77[%76] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %79 = llvm.ptrtoint %78 : !llvm.ptr to i64
    %80 = llvm.mlir.constant(32 : index) : i64
    %81 = llvm.add %79, %80  : i64
    %82 = llvm.call @malloc(%81) : (i64) -> !llvm.ptr
    %83 = llvm.ptrtoint %82 : !llvm.ptr to i64
    %84 = llvm.mlir.constant(1 : index) : i64
    %85 = llvm.sub %80, %84  : i64
    %86 = llvm.add %83, %85  : i64
    %87 = llvm.urem %86, %80  : i64
    %88 = llvm.sub %86, %87  : i64
    %89 = llvm.inttoptr %88 : i64 to !llvm.ptr
    %90 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %91 = llvm.insertvalue %82, %90[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %89, %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.mlir.constant(0 : index) : i64
    %94 = llvm.insertvalue %93, %92[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = llvm.insertvalue %73, %94[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = llvm.insertvalue %74, %95[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %97 = llvm.insertvalue %74, %96[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.insertvalue %75, %97[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %99 = builtin.unrealized_conversion_cast %98 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %99 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %100 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %101 = call @cudaMallocF64(%c1048576) : (index) -> index
    %102 = builtin.unrealized_conversion_cast %101 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%103: index):  // 2 preds: ^bb0, ^bb5
    %104 = arith.cmpi slt, %103, %c1024 : index
    cf.cond_br %104, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    cf.br ^bb3(%c0 : index)
  ^bb3(%105: index):  // 2 preds: ^bb2, ^bb4
    %106 = arith.cmpi slt, %105, %c1024 : index
    cf.cond_br %106, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %c1024_4 = arith.constant 1024 : index
    %107 = arith.muli %103, %c1024_4 : index
    %108 = arith.addi %107, %105 : index
    %109 = builtin.unrealized_conversion_cast %108 : index to i64
    %110 = llvm.extractvalue %40[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %111 = llvm.getelementptr %110[%109] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %111 : f64, !llvm.ptr
    %112 = arith.addi %105, %c1 : index
    cf.br ^bb3(%112 : index)
  ^bb5:  // pred: ^bb3
    %113 = arith.addi %103, %c1 : index
    cf.br ^bb1(%113 : index)
  ^bb6:  // pred: ^bb1
    cf.br ^bb7(%c0 : index)
  ^bb7(%114: index):  // 2 preds: ^bb6, ^bb11
    %115 = arith.cmpi slt, %114, %c1024 : index
    cf.cond_br %115, ^bb8, ^bb12
  ^bb8:  // pred: ^bb7
    cf.br ^bb9(%c0 : index)
  ^bb9(%116: index):  // 2 preds: ^bb8, ^bb10
    %117 = arith.cmpi slt, %116, %c1024 : index
    cf.cond_br %117, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %c1024_5 = arith.constant 1024 : index
    %118 = arith.muli %114, %c1024_5 : index
    %119 = arith.addi %118, %116 : index
    %120 = builtin.unrealized_conversion_cast %119 : index to i64
    %121 = llvm.extractvalue %70[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %122 = llvm.getelementptr %121[%120] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %122 : f64, !llvm.ptr
    %123 = arith.addi %116, %c1 : index
    cf.br ^bb9(%123 : index)
  ^bb11:  // pred: ^bb9
    %124 = arith.addi %114, %c1 : index
    cf.br ^bb7(%124 : index)
  ^bb12:  // pred: ^bb7
    cf.br ^bb13(%c0 : index)
  ^bb13(%125: index):  // 2 preds: ^bb12, ^bb17
    %126 = arith.cmpi slt, %125, %c1024 : index
    cf.cond_br %126, ^bb14, ^bb18
  ^bb14:  // pred: ^bb13
    cf.br ^bb15(%c0 : index)
  ^bb15(%127: index):  // 2 preds: ^bb14, ^bb16
    %128 = arith.cmpi slt, %127, %c1024 : index
    cf.cond_br %128, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %c1024_6 = arith.constant 1024 : index
    %129 = arith.muli %125, %c1024_6 : index
    %130 = arith.addi %129, %127 : index
    %131 = builtin.unrealized_conversion_cast %130 : index to i64
    %132 = llvm.extractvalue %100[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %133 = llvm.getelementptr %132[%131] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %133 : f64, !llvm.ptr
    %134 = arith.addi %127, %c1 : index
    cf.br ^bb15(%134 : index)
  ^bb17:  // pred: ^bb15
    %135 = arith.addi %125, %c1 : index
    cf.br ^bb13(%135 : index)
  ^bb18:  // pred: ^bb13
    %136 = builtin.unrealized_conversion_cast %40 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%41, %136, %c0) : (index, memref<?xf64>, index) -> ()
    %137 = builtin.unrealized_conversion_cast %70 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%71, %137, %c0) : (index, memref<?xf64>, index) -> ()
    %138 = builtin.unrealized_conversion_cast %100 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%101, %138, %c0) : (index, memref<?xf64>, index) -> ()
    %139 = llvm.mlir.constant(1 : index) : i64
    %140 = llvm.mlir.constant(1 : index) : i64
    %141 = llvm.alloca %139 x i64 : (i64) -> !llvm.ptr
    %142 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %143 = llvm.insertvalue %141, %142[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %141, %143[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.mlir.constant(0 : index) : i64
    %146 = llvm.insertvalue %145, %144[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %147 = llvm.insertvalue %139, %146[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %148 = llvm.insertvalue %140, %147[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %149 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %150 = llvm.getelementptr %149[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %11, %150 : i64, !llvm.ptr
    %151 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %152 = llvm.ptrtoint %151 : !llvm.ptr to i64
    %153 = llvm.mlir.constant(1 : index) : i64
    %154 = llvm.mlir.constant(1 : index) : i64
    %155 = llvm.alloca %153 x i64 : (i64) -> !llvm.ptr
    %156 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %157 = llvm.insertvalue %155, %156[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %158 = llvm.insertvalue %155, %157[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %159 = llvm.mlir.constant(0 : index) : i64
    %160 = llvm.insertvalue %159, %158[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %161 = llvm.insertvalue %153, %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %162 = llvm.insertvalue %154, %161[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %163 = llvm.extractvalue %162[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %164 = llvm.getelementptr %163[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %10, %164 : i64, !llvm.ptr
    %165 = llvm.extractvalue %162[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %166 = llvm.ptrtoint %165 : !llvm.ptr to i64
    %167 = llvm.mlir.constant(1 : index) : i64
    %168 = llvm.mlir.constant(1 : index) : i64
    %169 = llvm.alloca %167 x i64 : (i64) -> !llvm.ptr
    %170 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %171 = llvm.insertvalue %169, %170[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %172 = llvm.insertvalue %169, %171[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %173 = llvm.mlir.constant(0 : index) : i64
    %174 = llvm.insertvalue %173, %172[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %175 = llvm.insertvalue %167, %174[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %176 = llvm.insertvalue %168, %175[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %177 = llvm.extractvalue %176[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %178 = llvm.getelementptr %177[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %178 : i64, !llvm.ptr
    %179 = llvm.extractvalue %176[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %180 = llvm.ptrtoint %179 : !llvm.ptr to i64
    %181 = llvm.mlir.constant(1 : index) : i64
    %182 = llvm.mlir.constant(1 : index) : i64
    %183 = llvm.alloca %181 x i64 : (i64) -> !llvm.ptr
    %184 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %185 = llvm.insertvalue %183, %184[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %186 = llvm.insertvalue %183, %185[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %187 = llvm.mlir.constant(0 : index) : i64
    %188 = llvm.insertvalue %187, %186[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.insertvalue %181, %188[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.insertvalue %182, %189[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %191 = llvm.extractvalue %190[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %192 = llvm.getelementptr %191[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %192 : i64, !llvm.ptr
    %193 = llvm.extractvalue %190[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %194 = llvm.ptrtoint %193 : !llvm.ptr to i64
    %195 = llvm.mlir.constant(1 : index) : i64
    %196 = llvm.mlir.constant(1 : index) : i64
    %197 = llvm.alloca %195 x i64 : (i64) -> !llvm.ptr
    %198 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %199 = llvm.insertvalue %197, %198[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %200 = llvm.insertvalue %197, %199[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %201 = llvm.mlir.constant(0 : index) : i64
    %202 = llvm.insertvalue %201, %200[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %203 = llvm.insertvalue %195, %202[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %204 = llvm.insertvalue %196, %203[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %205 = llvm.extractvalue %204[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %206 = llvm.getelementptr %205[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %42, %206 : i64, !llvm.ptr
    %207 = llvm.extractvalue %204[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %208 = llvm.ptrtoint %207 : !llvm.ptr to i64
    %209 = llvm.mlir.constant(1 : index) : i64
    %210 = llvm.mlir.constant(1 : index) : i64
    %211 = llvm.alloca %209 x i64 : (i64) -> !llvm.ptr
    %212 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %213 = llvm.insertvalue %211, %212[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %214 = llvm.insertvalue %211, %213[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %215 = llvm.mlir.constant(0 : index) : i64
    %216 = llvm.insertvalue %215, %214[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %217 = llvm.insertvalue %209, %216[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %218 = llvm.insertvalue %210, %217[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %219 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %220 = llvm.getelementptr %219[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %220 : i64, !llvm.ptr
    %221 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %222 = llvm.ptrtoint %221 : !llvm.ptr to i64
    %223 = llvm.mlir.constant(1 : index) : i64
    %224 = llvm.mlir.constant(1 : index) : i64
    %225 = llvm.alloca %223 x i64 : (i64) -> !llvm.ptr
    %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %227 = llvm.insertvalue %225, %226[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %228 = llvm.insertvalue %225, %227[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %229 = llvm.mlir.constant(0 : index) : i64
    %230 = llvm.insertvalue %229, %228[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %231 = llvm.insertvalue %223, %230[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %232 = llvm.insertvalue %224, %231[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %233 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %234 = llvm.getelementptr %233[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %72, %234 : i64, !llvm.ptr
    %235 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %236 = llvm.ptrtoint %235 : !llvm.ptr to i64
    %237 = llvm.mlir.constant(1 : index) : i64
    %238 = llvm.mlir.constant(1 : index) : i64
    %239 = llvm.alloca %237 x i64 : (i64) -> !llvm.ptr
    %240 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %241 = llvm.insertvalue %239, %240[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %242 = llvm.insertvalue %239, %241[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %243 = llvm.mlir.constant(0 : index) : i64
    %244 = llvm.insertvalue %243, %242[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %245 = llvm.insertvalue %237, %244[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %246 = llvm.insertvalue %238, %245[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %247 = llvm.extractvalue %246[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %248 = llvm.getelementptr %247[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %248 : i64, !llvm.ptr
    %249 = llvm.extractvalue %246[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %250 = llvm.ptrtoint %249 : !llvm.ptr to i64
    %251 = llvm.mlir.constant(1 : index) : i64
    %252 = llvm.mlir.constant(1 : index) : i64
    %253 = llvm.alloca %251 x i64 : (i64) -> !llvm.ptr
    %254 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %255 = llvm.insertvalue %253, %254[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %256 = llvm.insertvalue %253, %255[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %257 = llvm.mlir.constant(0 : index) : i64
    %258 = llvm.insertvalue %257, %256[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %259 = llvm.insertvalue %251, %258[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %260 = llvm.insertvalue %252, %259[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %261 = llvm.extractvalue %260[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %262 = llvm.getelementptr %261[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %102, %262 : i64, !llvm.ptr
    %263 = llvm.extractvalue %260[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %264 = llvm.ptrtoint %263 : !llvm.ptr to i64
    %265 = llvm.mlir.constant(9 : index) : i64
    %266 = llvm.mlir.constant(1 : index) : i64
    %267 = llvm.alloca %265 x i64 : (i64) -> !llvm.ptr
    %268 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %269 = llvm.insertvalue %267, %268[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %270 = llvm.insertvalue %267, %269[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %271 = llvm.mlir.constant(0 : index) : i64
    %272 = llvm.insertvalue %271, %270[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %273 = llvm.insertvalue %265, %272[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %274 = llvm.insertvalue %266, %273[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %275 = builtin.unrealized_conversion_cast %274 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %276 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %277 = llvm.getelementptr %276[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %152, %277 : i64, !llvm.ptr
    %278 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %279 = llvm.getelementptr %278[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %166, %279 : i64, !llvm.ptr
    %280 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %281 = llvm.getelementptr %280[%5] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %180, %281 : i64, !llvm.ptr
    %282 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %283 = llvm.getelementptr %282[%4] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %194, %283 : i64, !llvm.ptr
    %284 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %285 = llvm.getelementptr %284[%3] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %208, %285 : i64, !llvm.ptr
    %286 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %287 = llvm.getelementptr %286[%2] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %222, %287 : i64, !llvm.ptr
    %288 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %289 = llvm.getelementptr %288[%1] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %236, %289 : i64, !llvm.ptr
    %290 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %291 = llvm.getelementptr %290[%0] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %250, %291 : i64, !llvm.ptr
    %292 = llvm.extractvalue %274[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %293 = llvm.getelementptr %292[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %264, %293 : i64, !llvm.ptr
    %294 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %295 = llvm.getelementptr %294[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %275, %295, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %296 = builtin.unrealized_conversion_cast %100 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%101, %296, %c1) : (index, memref<?xf64>, index) -> ()
    %297 = builtin.unrealized_conversion_cast %70 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%71, %297, %c1) : (index, memref<?xf64>, index) -> ()
    %298 = builtin.unrealized_conversion_cast %40 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%41, %298, %c1) : (index, memref<?xf64>, index) -> ()
    %299 = llvm.mlir.constant(1 : index) : i64
    %300 = llvm.alloca %299 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %98, %300 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %301 = llvm.mlir.constant(2 : index) : i64
    %302 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %303 = llvm.insertvalue %301, %302[0] : !llvm.struct<(i64, ptr)> 
    %304 = llvm.insertvalue %300, %303[1] : !llvm.struct<(i64, ptr)> 
    %305 = builtin.unrealized_conversion_cast %304 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%305) : (memref<*xf64>) -> ()
    call @cudaFree(%41) : (index) -> ()
    call @cudaFree(%71) : (index) -> ()
    call @cudaFree(%101) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %38 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.mlir.zero : !llvm.ptr
    %43 = llvm.getelementptr %42[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %44 = llvm.ptrtoint %43 : !llvm.ptr to i64
    %45 = llvm.add %44, %3  : i64
    %46 = llvm.call @malloc(%45) : (i64) -> !llvm.ptr
    %47 = llvm.ptrtoint %46 : !llvm.ptr to i64
    %48 = llvm.sub %3, %4  : i64
    %49 = llvm.add %47, %48  : i64
    %50 = llvm.urem %49, %3  : i64
    %51 = llvm.sub %49, %50  : i64
    %52 = llvm.inttoptr %51 : i64 to !llvm.ptr
    %53 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %54 = llvm.insertvalue %46, %53[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %52, %54[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %2, %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %5, %56[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %5, %57[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %5, %58[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %4, %59[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = builtin.unrealized_conversion_cast %60 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %61 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %62 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %63 = call @cudaMallocF64(%c1048576) : (index) -> index
    %64 = builtin.unrealized_conversion_cast %63 : index to i64
    %65 = llvm.mlir.zero : !llvm.ptr
    %66 = llvm.getelementptr %65[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %67 = llvm.ptrtoint %66 : !llvm.ptr to i64
    %68 = llvm.add %67, %3  : i64
    %69 = llvm.call @malloc(%68) : (i64) -> !llvm.ptr
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.sub %3, %4  : i64
    %72 = llvm.add %70, %71  : i64
    %73 = llvm.urem %72, %3  : i64
    %74 = llvm.sub %72, %73  : i64
    %75 = llvm.inttoptr %74 : i64 to !llvm.ptr
    %76 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %77 = llvm.insertvalue %69, %76[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %78 = llvm.insertvalue %75, %77[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = llvm.insertvalue %2, %78[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %80 = llvm.insertvalue %5, %79[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %81 = llvm.insertvalue %5, %80[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %82 = llvm.insertvalue %5, %81[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %83 = llvm.insertvalue %4, %82[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %84 = builtin.unrealized_conversion_cast %83 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %84 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %85 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %86 = call @cudaMallocF64(%c1048576) : (index) -> index
    %87 = builtin.unrealized_conversion_cast %86 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%88: index):  // 2 preds: ^bb0, ^bb4
    %89 = arith.cmpi slt, %88, %c1024 : index
    cf.cond_br %89, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%90: index):  // 2 preds: ^bb1, ^bb3
    %91 = arith.cmpi slt, %90, %c1024 : index
    cf.cond_br %91, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %92 = arith.muli %88, %c1024 : index
    %93 = arith.addi %92, %90 : index
    %94 = builtin.unrealized_conversion_cast %93 : index to i64
    %95 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %96 = llvm.getelementptr %95[%94] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %96 : f64, !llvm.ptr
    %97 = arith.addi %90, %c1 : index
    cf.br ^bb2(%97 : index)
  ^bb4:  // pred: ^bb2
    %98 = arith.addi %88, %c1 : index
    cf.br ^bb1(%98 : index)
  ^bb5(%99: index):  // 2 preds: ^bb1, ^bb8
    %100 = arith.cmpi slt, %99, %c1024 : index
    cf.cond_br %100, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%101: index):  // 2 preds: ^bb5, ^bb7
    %102 = arith.cmpi slt, %101, %c1024 : index
    cf.cond_br %102, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %103 = arith.muli %99, %c1024 : index
    %104 = arith.addi %103, %101 : index
    %105 = builtin.unrealized_conversion_cast %104 : index to i64
    %106 = llvm.extractvalue %62[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %107 = llvm.getelementptr %106[%105] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %107 : f64, !llvm.ptr
    %108 = arith.addi %101, %c1 : index
    cf.br ^bb6(%108 : index)
  ^bb8:  // pred: ^bb6
    %109 = arith.addi %99, %c1 : index
    cf.br ^bb5(%109 : index)
  ^bb9(%110: index):  // 2 preds: ^bb5, ^bb12
    %111 = arith.cmpi slt, %110, %c1024 : index
    cf.cond_br %111, ^bb10(%c0 : index), ^bb13
  ^bb10(%112: index):  // 2 preds: ^bb9, ^bb11
    %113 = arith.cmpi slt, %112, %c1024 : index
    cf.cond_br %113, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %114 = arith.muli %110, %c1024 : index
    %115 = arith.addi %114, %112 : index
    %116 = builtin.unrealized_conversion_cast %115 : index to i64
    %117 = llvm.extractvalue %85[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %118 = llvm.getelementptr %117[%116] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %118 : f64, !llvm.ptr
    %119 = arith.addi %112, %c1 : index
    cf.br ^bb10(%119 : index)
  ^bb12:  // pred: ^bb10
    %120 = arith.addi %110, %c1 : index
    cf.br ^bb9(%120 : index)
  ^bb13:  // pred: ^bb9
    %121 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %121, %c0) : (index, memref<?xf64>, index) -> ()
    %122 = builtin.unrealized_conversion_cast %62 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%63, %122, %c0) : (index, memref<?xf64>, index) -> ()
    %123 = builtin.unrealized_conversion_cast %85 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%86, %123, %c0) : (index, memref<?xf64>, index) -> ()
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %140 : i64, !llvm.ptr
    %141 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %142 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %143 = llvm.getelementptr %142[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %64, %143 : i64, !llvm.ptr
    %144 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %145 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %146 = llvm.getelementptr %145[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %146 : i64, !llvm.ptr
    %147 = llvm.ptrtoint %145 : !llvm.ptr to i64
    %148 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %149 = llvm.getelementptr %148[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %87, %149 : i64, !llvm.ptr
    %150 = llvm.ptrtoint %148 : !llvm.ptr to i64
    %151 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %152 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %153 = llvm.insertvalue %151, %152[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %154 = llvm.insertvalue %151, %153[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %155 = llvm.insertvalue %2, %154[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %156 = llvm.insertvalue %1, %155[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %157 = llvm.insertvalue %4, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %158 = builtin.unrealized_conversion_cast %157 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %159 = llvm.getelementptr %151[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %159 : i64, !llvm.ptr
    %160 = llvm.getelementptr %151[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %160 : i64, !llvm.ptr
    %161 = llvm.getelementptr %151[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %161 : i64, !llvm.ptr
    %162 = llvm.getelementptr %151[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %162 : i64, !llvm.ptr
    %163 = llvm.getelementptr %151[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %163 : i64, !llvm.ptr
    %164 = llvm.getelementptr %151[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %141, %164 : i64, !llvm.ptr
    %165 = llvm.getelementptr %151[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %144, %165 : i64, !llvm.ptr
    %166 = llvm.getelementptr %151[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %147, %166 : i64, !llvm.ptr
    %167 = llvm.getelementptr %151[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %150, %167 : i64, !llvm.ptr
    %168 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %169 = llvm.getelementptr %168[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %158, %169, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    %170 = builtin.unrealized_conversion_cast %85 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%86, %170, %c1) : (index, memref<?xf64>, index) -> ()
    %171 = builtin.unrealized_conversion_cast %62 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%63, %171, %c1) : (index, memref<?xf64>, index) -> ()
    %172 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %172, %c1) : (index, memref<?xf64>, index) -> ()
    %173 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %83, %173 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %174 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %175 = llvm.insertvalue %0, %174[0] : !llvm.struct<(i64, ptr)> 
    %176 = llvm.insertvalue %173, %175[1] : !llvm.struct<(i64, ptr)> 
    %177 = builtin.unrealized_conversion_cast %176 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%177) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%63) : (index) -> ()
    call @cudaFree(%86) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After CSE (cse) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %38 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
    %44 = llvm.add %43, %25  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %55 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %56 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %57 = call @cudaMallocF64(%c1048576) : (index) -> index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %25  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %72 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %73 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %74 = call @cudaMallocF64(%c1048576) : (index) -> index
    %75 = builtin.unrealized_conversion_cast %74 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
    %77 = arith.cmpi slt, %76, %c1024 : index
    cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
    %79 = arith.cmpi slt, %78, %c1024 : index
    cf.cond_br %79, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %80 = arith.muli %76, %c1024 : index
    %81 = arith.addi %80, %78 : index
    %82 = builtin.unrealized_conversion_cast %81 : index to i64
    %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %84 : f64, !llvm.ptr
    %85 = arith.addi %78, %c1 : index
    cf.br ^bb2(%85 : index)
  ^bb4:  // pred: ^bb2
    %86 = arith.addi %76, %c1 : index
    cf.br ^bb1(%86 : index)
  ^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
    %88 = arith.cmpi slt, %87, %c1024 : index
    cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
    %90 = arith.cmpi slt, %89, %c1024 : index
    cf.cond_br %90, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %91 = arith.muli %87, %c1024 : index
    %92 = arith.addi %91, %89 : index
    %93 = builtin.unrealized_conversion_cast %92 : index to i64
    %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %95 : f64, !llvm.ptr
    %96 = arith.addi %89, %c1 : index
    cf.br ^bb6(%96 : index)
  ^bb8:  // pred: ^bb6
    %97 = arith.addi %87, %c1 : index
    cf.br ^bb5(%97 : index)
  ^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
    %99 = arith.cmpi slt, %98, %c1024 : index
    cf.cond_br %99, ^bb10(%c0 : index), ^bb13
  ^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
    %101 = arith.cmpi slt, %100, %c1024 : index
    cf.cond_br %101, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %102 = arith.muli %98, %c1024 : index
    %103 = arith.addi %102, %100 : index
    %104 = builtin.unrealized_conversion_cast %103 : index to i64
    %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %106 : f64, !llvm.ptr
    %107 = arith.addi %100, %c1 : index
    cf.br ^bb10(%107 : index)
  ^bb12:  // pred: ^bb10
    %108 = arith.addi %98, %c1 : index
    cf.br ^bb9(%108 : index)
  ^bb13:  // pred: ^bb9
    %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
    %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
    %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
    %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %113 : i64, !llvm.ptr
    %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
    %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %116 : i64, !llvm.ptr
    %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %119 : i64, !llvm.ptr
    %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %122 : i64, !llvm.ptr
    %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %58, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %75, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %114, %147 : i64, !llvm.ptr
    %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %117, %148 : i64, !llvm.ptr
    %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %120, %149 : i64, !llvm.ptr
    %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %123, %150 : i64, !llvm.ptr
    %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %151 : i64, !llvm.ptr
    %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %152 : i64, !llvm.ptr
    %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %153 : i64, !llvm.ptr
    %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %154 : i64, !llvm.ptr
    %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %155 : i64, !llvm.ptr
    %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
    %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
    %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
    %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%57) : (index) -> ()
    call @cudaFree(%74) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertControlFlowToLLVMPass (convert-cf-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %38 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
    %44 = llvm.add %43, %25  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %55 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %56 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %57 = call @cudaMallocF64(%c1048576) : (index) -> index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %25  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %72 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %73 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %74 = call @cudaMallocF64(%c1048576) : (index) -> index
    %75 = builtin.unrealized_conversion_cast %74 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
    %77 = arith.cmpi slt, %76, %c1024 : index
    cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
    %79 = arith.cmpi slt, %78, %c1024 : index
    llvm.cond_br %79, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %80 = arith.muli %76, %c1024 : index
    %81 = arith.addi %80, %78 : index
    %82 = builtin.unrealized_conversion_cast %81 : index to i64
    %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %84 : f64, !llvm.ptr
    %85 = arith.addi %78, %c1 : index
    cf.br ^bb2(%85 : index)
  ^bb4:  // pred: ^bb2
    %86 = arith.addi %76, %c1 : index
    cf.br ^bb1(%86 : index)
  ^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
    %88 = arith.cmpi slt, %87, %c1024 : index
    cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
    %90 = arith.cmpi slt, %89, %c1024 : index
    llvm.cond_br %90, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %91 = arith.muli %87, %c1024 : index
    %92 = arith.addi %91, %89 : index
    %93 = builtin.unrealized_conversion_cast %92 : index to i64
    %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %95 : f64, !llvm.ptr
    %96 = arith.addi %89, %c1 : index
    cf.br ^bb6(%96 : index)
  ^bb8:  // pred: ^bb6
    %97 = arith.addi %87, %c1 : index
    cf.br ^bb5(%97 : index)
  ^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
    %99 = arith.cmpi slt, %98, %c1024 : index
    cf.cond_br %99, ^bb10(%c0 : index), ^bb13
  ^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
    %101 = arith.cmpi slt, %100, %c1024 : index
    llvm.cond_br %101, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %102 = arith.muli %98, %c1024 : index
    %103 = arith.addi %102, %100 : index
    %104 = builtin.unrealized_conversion_cast %103 : index to i64
    %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %106 : f64, !llvm.ptr
    %107 = arith.addi %100, %c1 : index
    cf.br ^bb10(%107 : index)
  ^bb12:  // pred: ^bb10
    %108 = arith.addi %98, %c1 : index
    cf.br ^bb9(%108 : index)
  ^bb13:  // pred: ^bb9
    %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
    %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
    %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
    %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %113 : i64, !llvm.ptr
    %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
    %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %116 : i64, !llvm.ptr
    %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %119 : i64, !llvm.ptr
    %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %122 : i64, !llvm.ptr
    %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %58, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %75, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %114, %147 : i64, !llvm.ptr
    %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %117, %148 : i64, !llvm.ptr
    %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %120, %149 : i64, !llvm.ptr
    %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %123, %150 : i64, !llvm.ptr
    %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %151 : i64, !llvm.ptr
    %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %152 : i64, !llvm.ptr
    %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %153 : i64, !llvm.ptr
    %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %154 : i64, !llvm.ptr
    %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %155 : i64, !llvm.ptr
    %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
    %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
    %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
    %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%57) : (index) -> ()
    call @cudaFree(%74) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape = memref.collapse_shape %38 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
    %44 = llvm.add %43, %25  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_2 = memref.collapse_shape %55 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %56 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %57 = call @cudaMallocF64(%c1048576) : (index) -> index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %25  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %collapse_shape_3 = memref.collapse_shape %72 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
    %73 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %74 = call @cudaMallocF64(%c1048576) : (index) -> index
    %75 = builtin.unrealized_conversion_cast %74 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
    %77 = arith.cmpi slt, %76, %c1024 : index
    cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
    %79 = arith.cmpi slt, %78, %c1024 : index
    llvm.cond_br %79, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %80 = arith.muli %76, %c1024 : index
    %81 = arith.addi %80, %78 : index
    %82 = builtin.unrealized_conversion_cast %81 : index to i64
    %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %84 : f64, !llvm.ptr
    %85 = arith.addi %78, %c1 : index
    cf.br ^bb2(%85 : index)
  ^bb4:  // pred: ^bb2
    %86 = arith.addi %76, %c1 : index
    cf.br ^bb1(%86 : index)
  ^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
    %88 = arith.cmpi slt, %87, %c1024 : index
    cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
    %90 = arith.cmpi slt, %89, %c1024 : index
    llvm.cond_br %90, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %91 = arith.muli %87, %c1024 : index
    %92 = arith.addi %91, %89 : index
    %93 = builtin.unrealized_conversion_cast %92 : index to i64
    %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %95 : f64, !llvm.ptr
    %96 = arith.addi %89, %c1 : index
    cf.br ^bb6(%96 : index)
  ^bb8:  // pred: ^bb6
    %97 = arith.addi %87, %c1 : index
    cf.br ^bb5(%97 : index)
  ^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
    %99 = arith.cmpi slt, %98, %c1024 : index
    cf.cond_br %99, ^bb10(%c0 : index), ^bb13
  ^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
    %101 = arith.cmpi slt, %100, %c1024 : index
    llvm.cond_br %101, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %102 = arith.muli %98, %c1024 : index
    %103 = arith.addi %102, %100 : index
    %104 = builtin.unrealized_conversion_cast %103 : index to i64
    %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %106 : f64, !llvm.ptr
    %107 = arith.addi %100, %c1 : index
    cf.br ^bb10(%107 : index)
  ^bb12:  // pred: ^bb10
    %108 = arith.addi %98, %c1 : index
    cf.br ^bb9(%108 : index)
  ^bb13:  // pred: ^bb9
    %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
    %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
    %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
    %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %113 : i64, !llvm.ptr
    %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
    %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %116 : i64, !llvm.ptr
    %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %119 : i64, !llvm.ptr
    %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %122 : i64, !llvm.ptr
    %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %58, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %75, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %114, %147 : i64, !llvm.ptr
    %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %117, %148 : i64, !llvm.ptr
    %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %120, %149 : i64, !llvm.ptr
    %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %123, %150 : i64, !llvm.ptr
    %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %151 : i64, !llvm.ptr
    %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %152 : i64, !llvm.ptr
    %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %153 : i64, !llvm.ptr
    %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %154 : i64, !llvm.ptr
    %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %155 : i64, !llvm.ptr
    %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
    %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
    %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
    %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%57) : (index) -> ()
    call @cudaFree(%74) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMallocI32(index) -> index

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMallocI64(index) -> index

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMallocF64(index) -> index

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMallocF32(index) -> index

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMemcpyF32(index, memref<?xf32>, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMemcpyI64(index, memref<?xi64>, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMemcpyI32(index, memref<?xi32>, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaFree(index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaSetModuleImage(!llvm.ptr)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @cudaMemcpyF64(index, memref<?xf64>, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func @main() {
  %0 = llvm.mlir.constant(2 : index) : i64
  %1 = llvm.mlir.constant(9 : index) : i64
  %2 = llvm.mlir.constant(0 : index) : i64
  %3 = llvm.mlir.constant(32 : index) : i64
  %4 = llvm.mlir.constant(1 : index) : i64
  %5 = llvm.mlir.constant(1024 : index) : i64
  %c128 = arith.constant 128 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 2.200000e+00 : f64
  %c1048576 = arith.constant 1048576 : index
  %c0_i32 = arith.constant 0 : i32
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c4 = arith.constant 4 : index
  %c5 = arith.constant 5 : index
  %c6 = arith.constant 6 : index
  %c12 = arith.constant 12 : index
  %c7 = arith.constant 7 : index
  %6 = builtin.unrealized_conversion_cast %c7 : index to i64
  %7 = builtin.unrealized_conversion_cast %c6 : index to i64
  %8 = builtin.unrealized_conversion_cast %c5 : index to i64
  %9 = builtin.unrealized_conversion_cast %c4 : index to i64
  %10 = builtin.unrealized_conversion_cast %c3 : index to i64
  %11 = builtin.unrealized_conversion_cast %c2 : index to i64
  %12 = builtin.unrealized_conversion_cast %c0 : index to i64
  %13 = builtin.unrealized_conversion_cast %c1 : index to i64
  %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
  %15 = builtin.unrealized_conversion_cast %c8 : index to i64
  %16 = builtin.unrealized_conversion_cast %c32 : index to i64
  %17 = builtin.unrealized_conversion_cast %c128 : index to i64
  %18 = llvm.mlir.addressof @ptx : !llvm.ptr
  call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
  %19 = llvm.mlir.zero : !llvm.ptr
  %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
  %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
  %22 = llvm.add %21, %3  : i64
  %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
  %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
  %25 = llvm.sub %3, %4  : i64
  %26 = llvm.add %24, %25  : i64
  %27 = llvm.urem %26, %3  : i64
  %28 = llvm.sub %26, %27  : i64
  %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
  %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
  %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
  %collapse_shape = memref.collapse_shape %38 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %39 = builtin.unrealized_conversion_cast %collapse_shape : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %40 = call @cudaMallocF64(%c1048576) : (index) -> index
  %41 = builtin.unrealized_conversion_cast %40 : index to i64
  %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
  %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
  %44 = llvm.add %43, %25  : i64
  %45 = llvm.urem %44, %3  : i64
  %46 = llvm.sub %44, %45  : i64
  %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
  %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
  %collapse_shape_2 = memref.collapse_shape %55 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %56 = builtin.unrealized_conversion_cast %collapse_shape_2 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %57 = call @cudaMallocF64(%c1048576) : (index) -> index
  %58 = builtin.unrealized_conversion_cast %57 : index to i64
  %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
  %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
  %61 = llvm.add %60, %25  : i64
  %62 = llvm.urem %61, %3  : i64
  %63 = llvm.sub %61, %62  : i64
  %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
  %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
  %collapse_shape_3 = memref.collapse_shape %72 [[0, 1]] : memref<1024x1024xf64> into memref<1048576xf64>
  %73 = builtin.unrealized_conversion_cast %collapse_shape_3 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %74 = call @cudaMallocF64(%c1048576) : (index) -> index
  %75 = builtin.unrealized_conversion_cast %74 : index to i64
  cf.br ^bb1(%c0 : index)
^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
  %77 = arith.cmpi slt, %76, %c1024 : index
  cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
  %79 = arith.cmpi slt, %78, %c1024 : index
  llvm.cond_br %79, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %80 = arith.muli %76, %c1024 : index
  %81 = arith.addi %80, %78 : index
  %82 = builtin.unrealized_conversion_cast %81 : index to i64
  %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst_1, %84 : f64, !llvm.ptr
  %85 = arith.addi %78, %c1 : index
  cf.br ^bb2(%85 : index)
^bb4:  // pred: ^bb2
  %86 = arith.addi %76, %c1 : index
  cf.br ^bb1(%86 : index)
^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
  %88 = arith.cmpi slt, %87, %c1024 : index
  cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
  %90 = arith.cmpi slt, %89, %c1024 : index
  llvm.cond_br %90, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  %91 = arith.muli %87, %c1024 : index
  %92 = arith.addi %91, %89 : index
  %93 = builtin.unrealized_conversion_cast %92 : index to i64
  %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst_0, %95 : f64, !llvm.ptr
  %96 = arith.addi %89, %c1 : index
  cf.br ^bb6(%96 : index)
^bb8:  // pred: ^bb6
  %97 = arith.addi %87, %c1 : index
  cf.br ^bb5(%97 : index)
^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
  %99 = arith.cmpi slt, %98, %c1024 : index
  cf.cond_br %99, ^bb10(%c0 : index), ^bb13
^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
  %101 = arith.cmpi slt, %100, %c1024 : index
  llvm.cond_br %101, ^bb11, ^bb12
^bb11:  // pred: ^bb10
  %102 = arith.muli %98, %c1024 : index
  %103 = arith.addi %102, %100 : index
  %104 = builtin.unrealized_conversion_cast %103 : index to i64
  %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst, %106 : f64, !llvm.ptr
  %107 = arith.addi %100, %c1 : index
  cf.br ^bb10(%107 : index)
^bb12:  // pred: ^bb10
  %108 = arith.addi %98, %c1 : index
  cf.br ^bb9(%108 : index)
^bb13:  // pred: ^bb9
  %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
  call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
  %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
  call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
  %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
  call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
  %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %17, %113 : i64, !llvm.ptr
  %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
  %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %16, %116 : i64, !llvm.ptr
  %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
  %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %14, %119 : i64, !llvm.ptr
  %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
  %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %14, %122 : i64, !llvm.ptr
  %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
  %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %41, %125 : i64, !llvm.ptr
  %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
  %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %14, %128 : i64, !llvm.ptr
  %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
  %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %58, %131 : i64, !llvm.ptr
  %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
  %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %14, %134 : i64, !llvm.ptr
  %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
  %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
  %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %75, %137 : i64, !llvm.ptr
  %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
  %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
  %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
  %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
  %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %114, %147 : i64, !llvm.ptr
  %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %117, %148 : i64, !llvm.ptr
  %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %120, %149 : i64, !llvm.ptr
  %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %123, %150 : i64, !llvm.ptr
  %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %126, %151 : i64, !llvm.ptr
  %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %129, %152 : i64, !llvm.ptr
  %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %132, %153 : i64, !llvm.ptr
  %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %135, %154 : i64, !llvm.ptr
  %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
  llvm.store %138, %155 : i64, !llvm.ptr
  %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
  %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
  call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
  call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
  call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
  call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
  %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
  llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
  %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
  %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
  %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
  %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
  call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
  call @cudaFree(%40) : (index) -> ()
  call @cudaFree(%57) : (index) -> ()
  call @cudaFree(%74) : (index) -> ()
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %38 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
    %44 = llvm.add %43, %25  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer_2, %offset_3, %sizes_4:2, %strides_5:2 = memref.extract_strided_metadata %55 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast_6 = memref.reinterpret_cast %base_buffer_2 to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %56 = builtin.unrealized_conversion_cast %reinterpret_cast_6 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %57 = call @cudaMallocF64(%c1048576) : (index) -> index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %25  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer_7, %offset_8, %sizes_9:2, %strides_10:2 = memref.extract_strided_metadata %72 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast_11 = memref.reinterpret_cast %base_buffer_7 to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %73 = builtin.unrealized_conversion_cast %reinterpret_cast_11 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %74 = call @cudaMallocF64(%c1048576) : (index) -> index
    %75 = builtin.unrealized_conversion_cast %74 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
    %77 = arith.cmpi slt, %76, %c1024 : index
    cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
    %79 = arith.cmpi slt, %78, %c1024 : index
    llvm.cond_br %79, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %80 = arith.muli %76, %c1024 : index
    %81 = arith.addi %80, %78 : index
    %82 = builtin.unrealized_conversion_cast %81 : index to i64
    %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %84 : f64, !llvm.ptr
    %85 = arith.addi %78, %c1 : index
    cf.br ^bb2(%85 : index)
  ^bb4:  // pred: ^bb2
    %86 = arith.addi %76, %c1 : index
    cf.br ^bb1(%86 : index)
  ^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
    %88 = arith.cmpi slt, %87, %c1024 : index
    cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
    %90 = arith.cmpi slt, %89, %c1024 : index
    llvm.cond_br %90, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %91 = arith.muli %87, %c1024 : index
    %92 = arith.addi %91, %89 : index
    %93 = builtin.unrealized_conversion_cast %92 : index to i64
    %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %95 : f64, !llvm.ptr
    %96 = arith.addi %89, %c1 : index
    cf.br ^bb6(%96 : index)
  ^bb8:  // pred: ^bb6
    %97 = arith.addi %87, %c1 : index
    cf.br ^bb5(%97 : index)
  ^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
    %99 = arith.cmpi slt, %98, %c1024 : index
    cf.cond_br %99, ^bb10(%c0 : index), ^bb13
  ^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
    %101 = arith.cmpi slt, %100, %c1024 : index
    llvm.cond_br %101, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %102 = arith.muli %98, %c1024 : index
    %103 = arith.addi %102, %100 : index
    %104 = builtin.unrealized_conversion_cast %103 : index to i64
    %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %106 : f64, !llvm.ptr
    %107 = arith.addi %100, %c1 : index
    cf.br ^bb10(%107 : index)
  ^bb12:  // pred: ^bb10
    %108 = arith.addi %98, %c1 : index
    cf.br ^bb9(%108 : index)
  ^bb13:  // pred: ^bb9
    %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
    %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
    %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
    %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %113 : i64, !llvm.ptr
    %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
    %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %116 : i64, !llvm.ptr
    %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %119 : i64, !llvm.ptr
    %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %122 : i64, !llvm.ptr
    %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %58, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %75, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %114, %147 : i64, !llvm.ptr
    %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %117, %148 : i64, !llvm.ptr
    %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %120, %149 : i64, !llvm.ptr
    %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %123, %150 : i64, !llvm.ptr
    %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %151 : i64, !llvm.ptr
    %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %152 : i64, !llvm.ptr
    %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %153 : i64, !llvm.ptr
    %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %154 : i64, !llvm.ptr
    %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %155 : i64, !llvm.ptr
    %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
    %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
    %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
    %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%57) : (index) -> ()
    call @cudaFree(%74) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %38 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %39 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %40 = call @cudaMallocF64(%c1048576) : (index) -> index
    %41 = builtin.unrealized_conversion_cast %40 : index to i64
    %42 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %43 = llvm.ptrtoint %42 : !llvm.ptr to i64
    %44 = llvm.add %43, %25  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.insertvalue %42, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %47, %48[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %2, %49[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %5, %50[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %4, %53[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer_2, %offset_3, %sizes_4:2, %strides_5:2 = memref.extract_strided_metadata %55 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast_6 = memref.reinterpret_cast %base_buffer_2 to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %56 = builtin.unrealized_conversion_cast %reinterpret_cast_6 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %57 = call @cudaMallocF64(%c1048576) : (index) -> index
    %58 = builtin.unrealized_conversion_cast %57 : index to i64
    %59 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %25  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = builtin.unrealized_conversion_cast %71 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %base_buffer_7, %offset_8, %sizes_9:2, %strides_10:2 = memref.extract_strided_metadata %72 : memref<1024x1024xf64> -> memref<f64>, index, index, index, index, index
    %reinterpret_cast_11 = memref.reinterpret_cast %base_buffer_7 to offset: [0], sizes: [1048576], strides: [1] : memref<f64> to memref<1048576xf64>
    %73 = builtin.unrealized_conversion_cast %reinterpret_cast_11 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %74 = call @cudaMallocF64(%c1048576) : (index) -> index
    %75 = builtin.unrealized_conversion_cast %74 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%76: index):  // 2 preds: ^bb0, ^bb4
    %77 = arith.cmpi slt, %76, %c1024 : index
    cf.cond_br %77, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%78: index):  // 2 preds: ^bb1, ^bb3
    %79 = arith.cmpi slt, %78, %c1024 : index
    llvm.cond_br %79, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %80 = arith.muli %76, %c1024 : index
    %81 = arith.addi %80, %78 : index
    %82 = builtin.unrealized_conversion_cast %81 : index to i64
    %83 = llvm.extractvalue %39[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.getelementptr %83[%82] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %84 : f64, !llvm.ptr
    %85 = arith.addi %78, %c1 : index
    cf.br ^bb2(%85 : index)
  ^bb4:  // pred: ^bb2
    %86 = arith.addi %76, %c1 : index
    cf.br ^bb1(%86 : index)
  ^bb5(%87: index):  // 2 preds: ^bb1, ^bb8
    %88 = arith.cmpi slt, %87, %c1024 : index
    cf.cond_br %88, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%89: index):  // 2 preds: ^bb5, ^bb7
    %90 = arith.cmpi slt, %89, %c1024 : index
    llvm.cond_br %90, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %91 = arith.muli %87, %c1024 : index
    %92 = arith.addi %91, %89 : index
    %93 = builtin.unrealized_conversion_cast %92 : index to i64
    %94 = llvm.extractvalue %56[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %95 = llvm.getelementptr %94[%93] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %95 : f64, !llvm.ptr
    %96 = arith.addi %89, %c1 : index
    cf.br ^bb6(%96 : index)
  ^bb8:  // pred: ^bb6
    %97 = arith.addi %87, %c1 : index
    cf.br ^bb5(%97 : index)
  ^bb9(%98: index):  // 2 preds: ^bb5, ^bb12
    %99 = arith.cmpi slt, %98, %c1024 : index
    cf.cond_br %99, ^bb10(%c0 : index), ^bb13
  ^bb10(%100: index):  // 2 preds: ^bb9, ^bb11
    %101 = arith.cmpi slt, %100, %c1024 : index
    llvm.cond_br %101, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %102 = arith.muli %98, %c1024 : index
    %103 = arith.addi %102, %100 : index
    %104 = builtin.unrealized_conversion_cast %103 : index to i64
    %105 = llvm.extractvalue %73[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.getelementptr %105[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %106 : f64, !llvm.ptr
    %107 = arith.addi %100, %c1 : index
    cf.br ^bb10(%107 : index)
  ^bb12:  // pred: ^bb10
    %108 = arith.addi %98, %c1 : index
    cf.br ^bb9(%108 : index)
  ^bb13:  // pred: ^bb9
    %109 = builtin.unrealized_conversion_cast %39 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%40, %109, %c0) : (index, memref<?xf64>, index) -> ()
    %110 = builtin.unrealized_conversion_cast %56 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%57, %110, %c0) : (index, memref<?xf64>, index) -> ()
    %111 = builtin.unrealized_conversion_cast %73 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%74, %111, %c0) : (index, memref<?xf64>, index) -> ()
    %112 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %113 = llvm.getelementptr %112[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %113 : i64, !llvm.ptr
    %114 = llvm.ptrtoint %112 : !llvm.ptr to i64
    %115 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %116 = llvm.getelementptr %115[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %116 : i64, !llvm.ptr
    %117 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %118 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %119 = llvm.getelementptr %118[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %119 : i64, !llvm.ptr
    %120 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %121 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %122 = llvm.getelementptr %121[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %122 : i64, !llvm.ptr
    %123 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %124 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %125 = llvm.getelementptr %124[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %41, %125 : i64, !llvm.ptr
    %126 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %127 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %128 = llvm.getelementptr %127[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %128 : i64, !llvm.ptr
    %129 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %130 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %131 = llvm.getelementptr %130[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %58, %131 : i64, !llvm.ptr
    %132 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %133 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %134 = llvm.getelementptr %133[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %134 : i64, !llvm.ptr
    %135 = llvm.ptrtoint %133 : !llvm.ptr to i64
    %136 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %137 = llvm.getelementptr %136[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %75, %137 : i64, !llvm.ptr
    %138 = llvm.ptrtoint %136 : !llvm.ptr to i64
    %139 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %140 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %141 = llvm.insertvalue %139, %140[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.insertvalue %139, %141[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %143 = llvm.insertvalue %2, %142[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = llvm.insertvalue %1, %143[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %145 = llvm.insertvalue %4, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %146 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %147 = llvm.getelementptr %139[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %114, %147 : i64, !llvm.ptr
    %148 = llvm.getelementptr %139[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %117, %148 : i64, !llvm.ptr
    %149 = llvm.getelementptr %139[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %120, %149 : i64, !llvm.ptr
    %150 = llvm.getelementptr %139[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %123, %150 : i64, !llvm.ptr
    %151 = llvm.getelementptr %139[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %126, %151 : i64, !llvm.ptr
    %152 = llvm.getelementptr %139[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %129, %152 : i64, !llvm.ptr
    %153 = llvm.getelementptr %139[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %132, %153 : i64, !llvm.ptr
    %154 = llvm.getelementptr %139[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %135, %154 : i64, !llvm.ptr
    %155 = llvm.getelementptr %139[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %155 : i64, !llvm.ptr
    %156 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %157 = llvm.getelementptr %156[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %146, %157, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%74, %111, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%57, %110, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%40, %109, %c1) : (index, memref<?xf64>, index) -> ()
    %158 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %71, %158 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %159 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %160 = llvm.insertvalue %0, %159[0] : !llvm.struct<(i64, ptr)> 
    %161 = llvm.insertvalue %158, %160[1] : !llvm.struct<(i64, ptr)> 
    %162 = builtin.unrealized_conversion_cast %161 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%162) : (memref<*xf64>) -> ()
    call @cudaFree(%40) : (index) -> ()
    call @cudaFree(%57) : (index) -> ()
    call @cudaFree(%74) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %c128 = arith.constant 128 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %c1048576 = arith.constant 1048576 : index
    %c0_i32 = arith.constant 0 : i32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c4 = arith.constant 4 : index
    %c5 = arith.constant 5 : index
    %c6 = arith.constant 6 : index
    %c12 = arith.constant 12 : index
    %c7 = arith.constant 7 : index
    %6 = builtin.unrealized_conversion_cast %c7 : index to i64
    %7 = builtin.unrealized_conversion_cast %c6 : index to i64
    %8 = builtin.unrealized_conversion_cast %c5 : index to i64
    %9 = builtin.unrealized_conversion_cast %c4 : index to i64
    %10 = builtin.unrealized_conversion_cast %c3 : index to i64
    %11 = builtin.unrealized_conversion_cast %c2 : index to i64
    %12 = builtin.unrealized_conversion_cast %c0 : index to i64
    %13 = builtin.unrealized_conversion_cast %c1 : index to i64
    %14 = builtin.unrealized_conversion_cast %c1024 : index to i64
    %15 = builtin.unrealized_conversion_cast %c8 : index to i64
    %16 = builtin.unrealized_conversion_cast %c32 : index to i64
    %17 = builtin.unrealized_conversion_cast %c128 : index to i64
    %18 = llvm.mlir.addressof @ptx : !llvm.ptr
    call @cudaSetModuleImage(%18) : (!llvm.ptr) -> ()
    %19 = llvm.mlir.zero : !llvm.ptr
    %20 = llvm.getelementptr %19[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.add %21, %3  : i64
    %23 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %24 = llvm.ptrtoint %23 : !llvm.ptr to i64
    %25 = llvm.sub %3, %4  : i64
    %26 = llvm.add %24, %25  : i64
    %27 = llvm.urem %26, %3  : i64
    %28 = llvm.sub %26, %27  : i64
    %29 = llvm.inttoptr %28 : i64 to !llvm.ptr
    %30 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %31 = llvm.insertvalue %23, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %29, %31[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %2, %32[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %5, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %5, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %5, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %4, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %39 = llvm.extractvalue %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.extractvalue %37[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %42 = llvm.insertvalue %39, %41[0] : !llvm.struct<(ptr, ptr, i64)> 
    %43 = llvm.insertvalue %40, %42[1] : !llvm.struct<(ptr, ptr, i64)> 
    %44 = llvm.mlir.constant(0 : index) : i64
    %45 = llvm.insertvalue %44, %43[2] : !llvm.struct<(ptr, ptr, i64)> 
    %46 = llvm.extractvalue %37[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %47 = llvm.extractvalue %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.extractvalue %37[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.extractvalue %37[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.extractvalue %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %52 = llvm.extractvalue %45[0] : !llvm.struct<(ptr, ptr, i64)> 
    %53 = llvm.extractvalue %45[1] : !llvm.struct<(ptr, ptr, i64)> 
    %54 = llvm.insertvalue %52, %51[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %55 = llvm.insertvalue %53, %54[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %56 = llvm.mlir.constant(0 : index) : i64
    %57 = llvm.insertvalue %56, %55[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %58 = llvm.mlir.constant(1048576 : index) : i64
    %59 = llvm.insertvalue %58, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %60 = llvm.mlir.constant(1 : index) : i64
    %61 = llvm.insertvalue %60, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %63 = builtin.unrealized_conversion_cast %62 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %64 = call @cudaMallocF64(%c1048576) : (index) -> index
    %65 = builtin.unrealized_conversion_cast %64 : index to i64
    %66 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %67 = llvm.ptrtoint %66 : !llvm.ptr to i64
    %68 = llvm.add %67, %25  : i64
    %69 = llvm.urem %68, %3  : i64
    %70 = llvm.sub %68, %69  : i64
    %71 = llvm.inttoptr %70 : i64 to !llvm.ptr
    %72 = llvm.insertvalue %66, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %73 = llvm.insertvalue %71, %72[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %74 = llvm.insertvalue %2, %73[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.insertvalue %5, %74[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %76 = llvm.insertvalue %5, %75[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.insertvalue %5, %76[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %78 = llvm.insertvalue %4, %77[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = builtin.unrealized_conversion_cast %78 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %80 = llvm.extractvalue %78[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %81 = llvm.extractvalue %78[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %82 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %83 = llvm.insertvalue %80, %82[0] : !llvm.struct<(ptr, ptr, i64)> 
    %84 = llvm.insertvalue %81, %83[1] : !llvm.struct<(ptr, ptr, i64)> 
    %85 = llvm.mlir.constant(0 : index) : i64
    %86 = llvm.insertvalue %85, %84[2] : !llvm.struct<(ptr, ptr, i64)> 
    %87 = llvm.extractvalue %78[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.extractvalue %78[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.extractvalue %78[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.extractvalue %78[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.extractvalue %78[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %93 = llvm.extractvalue %86[0] : !llvm.struct<(ptr, ptr, i64)> 
    %94 = llvm.extractvalue %86[1] : !llvm.struct<(ptr, ptr, i64)> 
    %95 = llvm.insertvalue %93, %92[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %96 = llvm.insertvalue %94, %95[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %97 = llvm.mlir.constant(0 : index) : i64
    %98 = llvm.insertvalue %97, %96[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %99 = llvm.mlir.constant(1048576 : index) : i64
    %100 = llvm.insertvalue %99, %98[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %101 = llvm.mlir.constant(1 : index) : i64
    %102 = llvm.insertvalue %101, %100[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %103 = builtin.unrealized_conversion_cast %102 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %104 = builtin.unrealized_conversion_cast %103 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %105 = call @cudaMallocF64(%c1048576) : (index) -> index
    %106 = builtin.unrealized_conversion_cast %105 : index to i64
    %107 = llvm.call @malloc(%22) : (i64) -> !llvm.ptr
    %108 = llvm.ptrtoint %107 : !llvm.ptr to i64
    %109 = llvm.add %108, %25  : i64
    %110 = llvm.urem %109, %3  : i64
    %111 = llvm.sub %109, %110  : i64
    %112 = llvm.inttoptr %111 : i64 to !llvm.ptr
    %113 = llvm.insertvalue %107, %30[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %114 = llvm.insertvalue %112, %113[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %115 = llvm.insertvalue %2, %114[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %116 = llvm.insertvalue %5, %115[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %117 = llvm.insertvalue %5, %116[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %118 = llvm.insertvalue %5, %117[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %119 = llvm.insertvalue %4, %118[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %120 = builtin.unrealized_conversion_cast %119 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %121 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %122 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %123 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %124 = llvm.insertvalue %121, %123[0] : !llvm.struct<(ptr, ptr, i64)> 
    %125 = llvm.insertvalue %122, %124[1] : !llvm.struct<(ptr, ptr, i64)> 
    %126 = llvm.mlir.constant(0 : index) : i64
    %127 = llvm.insertvalue %126, %125[2] : !llvm.struct<(ptr, ptr, i64)> 
    %128 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.extractvalue %119[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.extractvalue %119[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %134 = llvm.extractvalue %127[0] : !llvm.struct<(ptr, ptr, i64)> 
    %135 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64)> 
    %136 = llvm.insertvalue %134, %133[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %137 = llvm.insertvalue %135, %136[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %138 = llvm.mlir.constant(0 : index) : i64
    %139 = llvm.insertvalue %138, %137[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %140 = llvm.mlir.constant(1048576 : index) : i64
    %141 = llvm.insertvalue %140, %139[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = llvm.mlir.constant(1 : index) : i64
    %143 = llvm.insertvalue %142, %141[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %144 = builtin.unrealized_conversion_cast %143 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %145 = builtin.unrealized_conversion_cast %144 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %146 = call @cudaMallocF64(%c1048576) : (index) -> index
    %147 = builtin.unrealized_conversion_cast %146 : index to i64
    cf.br ^bb1(%c0 : index)
  ^bb1(%148: index):  // 2 preds: ^bb0, ^bb4
    %149 = arith.cmpi slt, %148, %c1024 : index
    cf.cond_br %149, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%150: index):  // 2 preds: ^bb1, ^bb3
    %151 = arith.cmpi slt, %150, %c1024 : index
    llvm.cond_br %151, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %152 = arith.muli %148, %c1024 : index
    %153 = arith.addi %152, %150 : index
    %154 = builtin.unrealized_conversion_cast %153 : index to i64
    %155 = llvm.extractvalue %63[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %156 = llvm.getelementptr %155[%154] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %156 : f64, !llvm.ptr
    %157 = arith.addi %150, %c1 : index
    cf.br ^bb2(%157 : index)
  ^bb4:  // pred: ^bb2
    %158 = arith.addi %148, %c1 : index
    cf.br ^bb1(%158 : index)
  ^bb5(%159: index):  // 2 preds: ^bb1, ^bb8
    %160 = arith.cmpi slt, %159, %c1024 : index
    cf.cond_br %160, ^bb6(%c0 : index), ^bb9(%c0 : index)
  ^bb6(%161: index):  // 2 preds: ^bb5, ^bb7
    %162 = arith.cmpi slt, %161, %c1024 : index
    llvm.cond_br %162, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %163 = arith.muli %159, %c1024 : index
    %164 = arith.addi %163, %161 : index
    %165 = builtin.unrealized_conversion_cast %164 : index to i64
    %166 = llvm.extractvalue %104[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %167 = llvm.getelementptr %166[%165] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %167 : f64, !llvm.ptr
    %168 = arith.addi %161, %c1 : index
    cf.br ^bb6(%168 : index)
  ^bb8:  // pred: ^bb6
    %169 = arith.addi %159, %c1 : index
    cf.br ^bb5(%169 : index)
  ^bb9(%170: index):  // 2 preds: ^bb5, ^bb12
    %171 = arith.cmpi slt, %170, %c1024 : index
    cf.cond_br %171, ^bb10(%c0 : index), ^bb13
  ^bb10(%172: index):  // 2 preds: ^bb9, ^bb11
    %173 = arith.cmpi slt, %172, %c1024 : index
    llvm.cond_br %173, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %174 = arith.muli %170, %c1024 : index
    %175 = arith.addi %174, %172 : index
    %176 = builtin.unrealized_conversion_cast %175 : index to i64
    %177 = llvm.extractvalue %145[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %178 = llvm.getelementptr %177[%176] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %178 : f64, !llvm.ptr
    %179 = arith.addi %172, %c1 : index
    cf.br ^bb10(%179 : index)
  ^bb12:  // pred: ^bb10
    %180 = arith.addi %170, %c1 : index
    cf.br ^bb9(%180 : index)
  ^bb13:  // pred: ^bb9
    %181 = builtin.unrealized_conversion_cast %63 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%64, %181, %c0) : (index, memref<?xf64>, index) -> ()
    %182 = builtin.unrealized_conversion_cast %104 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%105, %182, %c0) : (index, memref<?xf64>, index) -> ()
    %183 = builtin.unrealized_conversion_cast %145 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    call @cudaMemcpyF64(%146, %183, %c0) : (index, memref<?xf64>, index) -> ()
    %184 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %185 = llvm.getelementptr %184[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %17, %185 : i64, !llvm.ptr
    %186 = llvm.ptrtoint %184 : !llvm.ptr to i64
    %187 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %188 = llvm.getelementptr %187[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %16, %188 : i64, !llvm.ptr
    %189 = llvm.ptrtoint %187 : !llvm.ptr to i64
    %190 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %191 = llvm.getelementptr %190[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %191 : i64, !llvm.ptr
    %192 = llvm.ptrtoint %190 : !llvm.ptr to i64
    %193 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %194 = llvm.getelementptr %193[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %194 : i64, !llvm.ptr
    %195 = llvm.ptrtoint %193 : !llvm.ptr to i64
    %196 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %197 = llvm.getelementptr %196[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %65, %197 : i64, !llvm.ptr
    %198 = llvm.ptrtoint %196 : !llvm.ptr to i64
    %199 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %200 = llvm.getelementptr %199[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %200 : i64, !llvm.ptr
    %201 = llvm.ptrtoint %199 : !llvm.ptr to i64
    %202 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %203 = llvm.getelementptr %202[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %106, %203 : i64, !llvm.ptr
    %204 = llvm.ptrtoint %202 : !llvm.ptr to i64
    %205 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %206 = llvm.getelementptr %205[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %14, %206 : i64, !llvm.ptr
    %207 = llvm.ptrtoint %205 : !llvm.ptr to i64
    %208 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %209 = llvm.getelementptr %208[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %147, %209 : i64, !llvm.ptr
    %210 = llvm.ptrtoint %208 : !llvm.ptr to i64
    %211 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %212 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %213 = llvm.insertvalue %211, %212[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %214 = llvm.insertvalue %211, %213[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %215 = llvm.insertvalue %2, %214[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %216 = llvm.insertvalue %1, %215[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %217 = llvm.insertvalue %4, %216[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %218 = builtin.unrealized_conversion_cast %217 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %219 = llvm.getelementptr %211[%12] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %186, %219 : i64, !llvm.ptr
    %220 = llvm.getelementptr %211[%13] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %189, %220 : i64, !llvm.ptr
    %221 = llvm.getelementptr %211[%11] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %192, %221 : i64, !llvm.ptr
    %222 = llvm.getelementptr %211[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %195, %222 : i64, !llvm.ptr
    %223 = llvm.getelementptr %211[%9] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %198, %223 : i64, !llvm.ptr
    %224 = llvm.getelementptr %211[%8] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %201, %224 : i64, !llvm.ptr
    %225 = llvm.getelementptr %211[%7] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %204, %225 : i64, !llvm.ptr
    %226 = llvm.getelementptr %211[%6] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %207, %226 : i64, !llvm.ptr
    %227 = llvm.getelementptr %211[%15] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %210, %227 : i64, !llvm.ptr
    %228 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %229 = llvm.getelementptr %228[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    call @cudaLaunchKernel(%c32, %c128, %c1, %c32, %c8, %c1, %218, %229, %c12, %c0_i32, %c4, %c32) : (index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index) -> ()
    call @cudaMemcpyF64(%146, %183, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%105, %182, %c1) : (index, memref<?xf64>, index) -> ()
    call @cudaMemcpyF64(%64, %181, %c1) : (index, memref<?xf64>, index) -> ()
    %230 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %119, %230 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %231 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %232 = llvm.insertvalue %0, %231[0] : !llvm.struct<(i64, ptr)> 
    %233 = llvm.insertvalue %230, %232[1] : !llvm.struct<(i64, ptr)> 
    %234 = builtin.unrealized_conversion_cast %233 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%234) : (memref<*xf64>) -> ()
    call @cudaFree(%64) : (index) -> ()
    call @cudaFree(%105) : (index) -> ()
    call @cudaFree(%146) : (index) -> ()
    return
  }
  func.func private @comet_print_memref_f64(memref<*xf64>)
  func.func private @cudaMallocI32(index) -> index
  func.func private @cudaMallocI64(index) -> index
  func.func private @cudaMallocF32(index) -> index
  func.func private @cudaMallocF64(index) -> index
  func.func private @cudaMemcpyI32(index, memref<?xi32>, index)
  func.func private @cudaMemcpyI64(index, memref<?xi64>, index)
  func.func private @cudaMemcpyIndex(index, memref<?xindex>, index)
  func.func private @cudaMemcpyF32(index, memref<?xf32>, index)
  func.func private @cudaMemcpyF64(index, memref<?xf64>, index)
  func.func private @cudaLaunchKernel(index, index, index, index, index, index, memref<?xindex>, !llvm.ptr, index, i32, index, index)
  func.func private @cudaSetModuleImage(!llvm.ptr)
  func.func private @cudaFree(index)
}


// -----// IR Dump After ConvertFuncToLLVMPass (convert-func-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = builtin.unrealized_conversion_cast %6 : i64 to index
    %8 = llvm.mlir.constant(32 : index) : i64
    %9 = builtin.unrealized_conversion_cast %8 : i64 to index
    %10 = llvm.mlir.constant(8 : index) : i64
    %11 = builtin.unrealized_conversion_cast %10 : i64 to index
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = builtin.unrealized_conversion_cast %12 : i64 to index
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = builtin.unrealized_conversion_cast %14 : i64 to index
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = builtin.unrealized_conversion_cast %16 : i64 to index
    %18 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %19 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %20 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %21 = llvm.mlir.constant(1048576 : index) : i64
    %22 = llvm.mlir.constant(0 : i32) : i32
    %23 = llvm.mlir.constant(2 : index) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(3 : index) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(4 : index) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(5 : index) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(6 : index) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(12 : index) : i64
    %34 = llvm.mlir.constant(7 : index) : i64
    %35 = builtin.unrealized_conversion_cast %34 : i64 to index
    %36 = builtin.unrealized_conversion_cast %35 : index to i64
    %37 = builtin.unrealized_conversion_cast %32 : index to i64
    %38 = builtin.unrealized_conversion_cast %30 : index to i64
    %39 = builtin.unrealized_conversion_cast %28 : index to i64
    %40 = builtin.unrealized_conversion_cast %26 : index to i64
    %41 = builtin.unrealized_conversion_cast %24 : index to i64
    %42 = builtin.unrealized_conversion_cast %17 : index to i64
    %43 = builtin.unrealized_conversion_cast %15 : index to i64
    %44 = builtin.unrealized_conversion_cast %13 : index to i64
    %45 = builtin.unrealized_conversion_cast %11 : index to i64
    %46 = builtin.unrealized_conversion_cast %9 : index to i64
    %47 = builtin.unrealized_conversion_cast %7 : index to i64
    %48 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%48) : (!llvm.ptr) -> ()
    %49 = llvm.mlir.zero : !llvm.ptr
    %50 = llvm.getelementptr %49[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %51 = llvm.ptrtoint %50 : !llvm.ptr to i64
    %52 = llvm.add %51, %3  : i64
    %53 = llvm.call @malloc(%52) : (i64) -> !llvm.ptr
    %54 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %55 = llvm.sub %3, %4  : i64
    %56 = llvm.add %54, %55  : i64
    %57 = llvm.urem %56, %3  : i64
    %58 = llvm.sub %56, %57  : i64
    %59 = llvm.inttoptr %58 : i64 to !llvm.ptr
    %60 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %61 = llvm.insertvalue %53, %60[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.insertvalue %2, %62[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.insertvalue %5, %63[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.insertvalue %5, %64[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %5, %65[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %4, %66[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = builtin.unrealized_conversion_cast %67 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %69 = llvm.extractvalue %67[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.extractvalue %67[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %72 = llvm.insertvalue %69, %71[0] : !llvm.struct<(ptr, ptr, i64)> 
    %73 = llvm.insertvalue %70, %72[1] : !llvm.struct<(ptr, ptr, i64)> 
    %74 = llvm.mlir.constant(0 : index) : i64
    %75 = llvm.insertvalue %74, %73[2] : !llvm.struct<(ptr, ptr, i64)> 
    %76 = llvm.extractvalue %67[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.extractvalue %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %78 = llvm.extractvalue %67[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = llvm.extractvalue %67[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %80 = llvm.extractvalue %67[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %81 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %82 = llvm.extractvalue %75[0] : !llvm.struct<(ptr, ptr, i64)> 
    %83 = llvm.extractvalue %75[1] : !llvm.struct<(ptr, ptr, i64)> 
    %84 = llvm.insertvalue %82, %81[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %85 = llvm.insertvalue %83, %84[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %86 = llvm.mlir.constant(0 : index) : i64
    %87 = llvm.insertvalue %86, %85[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %88 = llvm.mlir.constant(1048576 : index) : i64
    %89 = llvm.insertvalue %88, %87[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %90 = llvm.mlir.constant(1 : index) : i64
    %91 = llvm.insertvalue %90, %89[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %92 = builtin.unrealized_conversion_cast %91 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %93 = builtin.unrealized_conversion_cast %92 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %94 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %95 = builtin.unrealized_conversion_cast %94 : i64 to index
    %96 = builtin.unrealized_conversion_cast %95 : index to i64
    %97 = llvm.call @malloc(%52) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.add %98, %55  : i64
    %100 = llvm.urem %99, %3  : i64
    %101 = llvm.sub %99, %100  : i64
    %102 = llvm.inttoptr %101 : i64 to !llvm.ptr
    %103 = llvm.insertvalue %97, %60[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %104 = llvm.insertvalue %102, %103[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %105 = llvm.insertvalue %2, %104[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %106 = llvm.insertvalue %5, %105[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %107 = llvm.insertvalue %5, %106[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.insertvalue %5, %107[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %109 = llvm.insertvalue %4, %108[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %110 = builtin.unrealized_conversion_cast %109 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %111 = llvm.extractvalue %109[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %112 = llvm.extractvalue %109[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %113 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %114 = llvm.insertvalue %111, %113[0] : !llvm.struct<(ptr, ptr, i64)> 
    %115 = llvm.insertvalue %112, %114[1] : !llvm.struct<(ptr, ptr, i64)> 
    %116 = llvm.mlir.constant(0 : index) : i64
    %117 = llvm.insertvalue %116, %115[2] : !llvm.struct<(ptr, ptr, i64)> 
    %118 = llvm.extractvalue %109[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %119 = llvm.extractvalue %109[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %120 = llvm.extractvalue %109[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %121 = llvm.extractvalue %109[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %122 = llvm.extractvalue %109[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %123 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %124 = llvm.extractvalue %117[0] : !llvm.struct<(ptr, ptr, i64)> 
    %125 = llvm.extractvalue %117[1] : !llvm.struct<(ptr, ptr, i64)> 
    %126 = llvm.insertvalue %124, %123[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %127 = llvm.insertvalue %125, %126[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %128 = llvm.mlir.constant(0 : index) : i64
    %129 = llvm.insertvalue %128, %127[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %130 = llvm.mlir.constant(1048576 : index) : i64
    %131 = llvm.insertvalue %130, %129[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %132 = llvm.mlir.constant(1 : index) : i64
    %133 = llvm.insertvalue %132, %131[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %134 = builtin.unrealized_conversion_cast %133 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %135 = builtin.unrealized_conversion_cast %134 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %136 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %137 = builtin.unrealized_conversion_cast %136 : i64 to index
    %138 = builtin.unrealized_conversion_cast %137 : index to i64
    %139 = llvm.call @malloc(%52) : (i64) -> !llvm.ptr
    %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %141 = llvm.add %140, %55  : i64
    %142 = llvm.urem %141, %3  : i64
    %143 = llvm.sub %141, %142  : i64
    %144 = llvm.inttoptr %143 : i64 to !llvm.ptr
    %145 = llvm.insertvalue %139, %60[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %146 = llvm.insertvalue %144, %145[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %147 = llvm.insertvalue %2, %146[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %148 = llvm.insertvalue %5, %147[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %149 = llvm.insertvalue %5, %148[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %150 = llvm.insertvalue %5, %149[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %151 = llvm.insertvalue %4, %150[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %152 = builtin.unrealized_conversion_cast %151 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %153 = llvm.extractvalue %151[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %154 = llvm.extractvalue %151[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %155 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %156 = llvm.insertvalue %153, %155[0] : !llvm.struct<(ptr, ptr, i64)> 
    %157 = llvm.insertvalue %154, %156[1] : !llvm.struct<(ptr, ptr, i64)> 
    %158 = llvm.mlir.constant(0 : index) : i64
    %159 = llvm.insertvalue %158, %157[2] : !llvm.struct<(ptr, ptr, i64)> 
    %160 = llvm.extractvalue %151[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %161 = llvm.extractvalue %151[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %162 = llvm.extractvalue %151[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %163 = llvm.extractvalue %151[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %164 = llvm.extractvalue %151[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %165 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %166 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64)> 
    %167 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64)> 
    %168 = llvm.insertvalue %166, %165[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %169 = llvm.insertvalue %167, %168[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %170 = llvm.mlir.constant(0 : index) : i64
    %171 = llvm.insertvalue %170, %169[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %172 = llvm.mlir.constant(1048576 : index) : i64
    %173 = llvm.insertvalue %172, %171[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %174 = llvm.mlir.constant(1 : index) : i64
    %175 = llvm.insertvalue %174, %173[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %176 = builtin.unrealized_conversion_cast %175 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %177 = builtin.unrealized_conversion_cast %176 : memref<1048576xf64> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %178 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %179 = builtin.unrealized_conversion_cast %178 : i64 to index
    %180 = builtin.unrealized_conversion_cast %179 : index to i64
    llvm.br ^bb1(%16 : i64)
  ^bb1(%181: i64):  // 2 preds: ^bb0, ^bb4
    %182 = llvm.icmp "slt" %181, %12 : i64
    llvm.cond_br %182, ^bb2(%16 : i64), ^bb5(%16 : i64)
  ^bb2(%183: i64):  // 2 preds: ^bb1, ^bb3
    %184 = llvm.icmp "slt" %183, %12 : i64
    llvm.cond_br %184, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %185 = llvm.mul %181, %12  : i64
    %186 = llvm.add %185, %183  : i64
    %187 = builtin.unrealized_conversion_cast %186 : i64 to index
    %188 = builtin.unrealized_conversion_cast %187 : index to i64
    %189 = llvm.extractvalue %93[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.getelementptr %189[%188] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %20, %190 : f64, !llvm.ptr
    %191 = llvm.add %183, %14  : i64
    llvm.br ^bb2(%191 : i64)
  ^bb4:  // pred: ^bb2
    %192 = llvm.add %181, %14  : i64
    llvm.br ^bb1(%192 : i64)
  ^bb5(%193: i64):  // 2 preds: ^bb1, ^bb8
    %194 = llvm.icmp "slt" %193, %12 : i64
    llvm.cond_br %194, ^bb6(%16 : i64), ^bb9(%16 : i64)
  ^bb6(%195: i64):  // 2 preds: ^bb5, ^bb7
    %196 = llvm.icmp "slt" %195, %12 : i64
    llvm.cond_br %196, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %197 = llvm.mul %193, %12  : i64
    %198 = llvm.add %197, %195  : i64
    %199 = builtin.unrealized_conversion_cast %198 : i64 to index
    %200 = builtin.unrealized_conversion_cast %199 : index to i64
    %201 = llvm.extractvalue %135[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %202 = llvm.getelementptr %201[%200] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %19, %202 : f64, !llvm.ptr
    %203 = llvm.add %195, %14  : i64
    llvm.br ^bb6(%203 : i64)
  ^bb8:  // pred: ^bb6
    %204 = llvm.add %193, %14  : i64
    llvm.br ^bb5(%204 : i64)
  ^bb9(%205: i64):  // 2 preds: ^bb5, ^bb12
    %206 = llvm.icmp "slt" %205, %12 : i64
    llvm.cond_br %206, ^bb10(%16 : i64), ^bb13
  ^bb10(%207: i64):  // 2 preds: ^bb9, ^bb11
    %208 = llvm.icmp "slt" %207, %12 : i64
    llvm.cond_br %208, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %209 = llvm.mul %205, %12  : i64
    %210 = llvm.add %209, %207  : i64
    %211 = builtin.unrealized_conversion_cast %210 : i64 to index
    %212 = builtin.unrealized_conversion_cast %211 : index to i64
    %213 = llvm.extractvalue %177[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %214 = llvm.getelementptr %213[%212] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %18, %214 : f64, !llvm.ptr
    %215 = llvm.add %207, %14  : i64
    llvm.br ^bb10(%215 : i64)
  ^bb12:  // pred: ^bb10
    %216 = llvm.add %205, %14  : i64
    llvm.br ^bb9(%216 : i64)
  ^bb13:  // pred: ^bb9
    %217 = builtin.unrealized_conversion_cast %93 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %218 = llvm.extractvalue %93[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %219 = llvm.extractvalue %93[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %220 = llvm.extractvalue %93[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %221 = llvm.extractvalue %93[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %222 = llvm.extractvalue %93[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%94, %218, %219, %220, %221, %222, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %223 = builtin.unrealized_conversion_cast %135 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %224 = llvm.extractvalue %135[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %225 = llvm.extractvalue %135[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %226 = llvm.extractvalue %135[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %227 = llvm.extractvalue %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %228 = llvm.extractvalue %135[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%136, %224, %225, %226, %227, %228, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %229 = builtin.unrealized_conversion_cast %177 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %230 = llvm.extractvalue %177[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %231 = llvm.extractvalue %177[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %232 = llvm.extractvalue %177[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %233 = llvm.extractvalue %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %234 = llvm.extractvalue %177[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%178, %230, %231, %232, %233, %234, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %235 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %236 = llvm.getelementptr %235[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %47, %236 : i64, !llvm.ptr
    %237 = llvm.ptrtoint %235 : !llvm.ptr to i64
    %238 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %239 = llvm.getelementptr %238[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %46, %239 : i64, !llvm.ptr
    %240 = llvm.ptrtoint %238 : !llvm.ptr to i64
    %241 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %242 = llvm.getelementptr %241[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %44, %242 : i64, !llvm.ptr
    %243 = llvm.ptrtoint %241 : !llvm.ptr to i64
    %244 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %245 = llvm.getelementptr %244[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %44, %245 : i64, !llvm.ptr
    %246 = llvm.ptrtoint %244 : !llvm.ptr to i64
    %247 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %248 = llvm.getelementptr %247[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %96, %248 : i64, !llvm.ptr
    %249 = llvm.ptrtoint %247 : !llvm.ptr to i64
    %250 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %251 = llvm.getelementptr %250[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %44, %251 : i64, !llvm.ptr
    %252 = llvm.ptrtoint %250 : !llvm.ptr to i64
    %253 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %254 = llvm.getelementptr %253[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %138, %254 : i64, !llvm.ptr
    %255 = llvm.ptrtoint %253 : !llvm.ptr to i64
    %256 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %257 = llvm.getelementptr %256[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %44, %257 : i64, !llvm.ptr
    %258 = llvm.ptrtoint %256 : !llvm.ptr to i64
    %259 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %260 = llvm.getelementptr %259[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %180, %260 : i64, !llvm.ptr
    %261 = llvm.ptrtoint %259 : !llvm.ptr to i64
    %262 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %263 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %264 = llvm.insertvalue %262, %263[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %265 = llvm.insertvalue %262, %264[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %266 = llvm.insertvalue %2, %265[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %267 = llvm.insertvalue %1, %266[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %268 = llvm.insertvalue %4, %267[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %269 = builtin.unrealized_conversion_cast %268 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %270 = llvm.getelementptr %262[%42] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %237, %270 : i64, !llvm.ptr
    %271 = llvm.getelementptr %262[%43] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %240, %271 : i64, !llvm.ptr
    %272 = llvm.getelementptr %262[%41] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %243, %272 : i64, !llvm.ptr
    %273 = llvm.getelementptr %262[%40] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %246, %273 : i64, !llvm.ptr
    %274 = llvm.getelementptr %262[%39] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %249, %274 : i64, !llvm.ptr
    %275 = llvm.getelementptr %262[%38] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %252, %275 : i64, !llvm.ptr
    %276 = llvm.getelementptr %262[%37] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %255, %276 : i64, !llvm.ptr
    %277 = llvm.getelementptr %262[%36] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %258, %277 : i64, !llvm.ptr
    %278 = llvm.getelementptr %262[%45] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %261, %278 : i64, !llvm.ptr
    %279 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %280 = llvm.getelementptr %279[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    %281 = llvm.extractvalue %268[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %282 = llvm.extractvalue %268[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %283 = llvm.extractvalue %268[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %284 = llvm.extractvalue %268[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %285 = llvm.extractvalue %268[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaLaunchKernel(%8, %6, %14, %8, %10, %14, %281, %282, %283, %284, %285, %280, %33, %22, %27, %8) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    %286 = llvm.extractvalue %177[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %287 = llvm.extractvalue %177[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %288 = llvm.extractvalue %177[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %289 = llvm.extractvalue %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %290 = llvm.extractvalue %177[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%178, %286, %287, %288, %289, %290, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %291 = llvm.extractvalue %135[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %292 = llvm.extractvalue %135[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %293 = llvm.extractvalue %135[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %294 = llvm.extractvalue %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %295 = llvm.extractvalue %135[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%136, %291, %292, %293, %294, %295, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %296 = llvm.extractvalue %93[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %297 = llvm.extractvalue %93[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %298 = llvm.extractvalue %93[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %299 = llvm.extractvalue %93[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %300 = llvm.extractvalue %93[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%94, %296, %297, %298, %299, %300, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %301 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %151, %301 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %302 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %303 = llvm.insertvalue %0, %302[0] : !llvm.struct<(i64, ptr)> 
    %304 = llvm.insertvalue %301, %303[1] : !llvm.struct<(i64, ptr)> 
    %305 = builtin.unrealized_conversion_cast %304 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %306 = llvm.extractvalue %304[0] : !llvm.struct<(i64, ptr)> 
    %307 = llvm.extractvalue %304[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%306, %307) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%94) : (i64) -> ()
    llvm.call @cudaFree(%136) : (i64) -> ()
    llvm.call @cudaFree(%178) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ConvertIndexToLLVMPass (convert-index-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = builtin.unrealized_conversion_cast %6 : i64 to index
    %8 = llvm.mlir.constant(32 : index) : i64
    %9 = builtin.unrealized_conversion_cast %8 : i64 to index
    %10 = llvm.mlir.constant(8 : index) : i64
    %11 = builtin.unrealized_conversion_cast %10 : i64 to index
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = builtin.unrealized_conversion_cast %12 : i64 to index
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = builtin.unrealized_conversion_cast %14 : i64 to index
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = builtin.unrealized_conversion_cast %16 : i64 to index
    %18 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %19 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %20 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %21 = llvm.mlir.constant(1048576 : index) : i64
    %22 = llvm.mlir.constant(0 : i32) : i32
    %23 = llvm.mlir.constant(2 : index) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(3 : index) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(4 : index) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(5 : index) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(6 : index) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(12 : index) : i64
    %34 = llvm.mlir.constant(7 : index) : i64
    %35 = builtin.unrealized_conversion_cast %34 : i64 to index
    %36 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%36) : (!llvm.ptr) -> ()
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %3  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %3, %4  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %2, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %5, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %4, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %57 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %60 = llvm.insertvalue %57, %59[0] : !llvm.struct<(ptr, ptr, i64)> 
    %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr, ptr, i64)> 
    %62 = llvm.mlir.constant(0 : index) : i64
    %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %70 = llvm.extractvalue %63[0] : !llvm.struct<(ptr, ptr, i64)> 
    %71 = llvm.extractvalue %63[1] : !llvm.struct<(ptr, ptr, i64)> 
    %72 = llvm.insertvalue %70, %69[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %73 = llvm.insertvalue %71, %72[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %74 = llvm.mlir.constant(0 : index) : i64
    %75 = llvm.insertvalue %74, %73[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %76 = llvm.mlir.constant(1048576 : index) : i64
    %77 = llvm.insertvalue %76, %75[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %78 = llvm.mlir.constant(1 : index) : i64
    %79 = llvm.insertvalue %78, %77[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %80 = builtin.unrealized_conversion_cast %79 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %81 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %82 = builtin.unrealized_conversion_cast %81 : i64 to index
    %83 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.add %84, %43  : i64
    %86 = llvm.urem %85, %3  : i64
    %87 = llvm.sub %85, %86  : i64
    %88 = llvm.inttoptr %87 : i64 to !llvm.ptr
    %89 = llvm.insertvalue %83, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %88, %89[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %2, %90[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %5, %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.insertvalue %5, %92[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.insertvalue %5, %93[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = llvm.insertvalue %4, %94[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = builtin.unrealized_conversion_cast %95 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %97 = llvm.extractvalue %95[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.extractvalue %95[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %99 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %100 = llvm.insertvalue %97, %99[0] : !llvm.struct<(ptr, ptr, i64)> 
    %101 = llvm.insertvalue %98, %100[1] : !llvm.struct<(ptr, ptr, i64)> 
    %102 = llvm.mlir.constant(0 : index) : i64
    %103 = llvm.insertvalue %102, %101[2] : !llvm.struct<(ptr, ptr, i64)> 
    %104 = llvm.extractvalue %95[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %105 = llvm.extractvalue %95[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %106 = llvm.extractvalue %95[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %107 = llvm.extractvalue %95[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.extractvalue %95[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %109 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %110 = llvm.extractvalue %103[0] : !llvm.struct<(ptr, ptr, i64)> 
    %111 = llvm.extractvalue %103[1] : !llvm.struct<(ptr, ptr, i64)> 
    %112 = llvm.insertvalue %110, %109[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %113 = llvm.insertvalue %111, %112[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %114 = llvm.mlir.constant(0 : index) : i64
    %115 = llvm.insertvalue %114, %113[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %116 = llvm.mlir.constant(1048576 : index) : i64
    %117 = llvm.insertvalue %116, %115[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %118 = llvm.mlir.constant(1 : index) : i64
    %119 = llvm.insertvalue %118, %117[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %120 = builtin.unrealized_conversion_cast %119 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %121 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %122 = builtin.unrealized_conversion_cast %121 : i64 to index
    %123 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %124 = llvm.ptrtoint %123 : !llvm.ptr to i64
    %125 = llvm.add %124, %43  : i64
    %126 = llvm.urem %125, %3  : i64
    %127 = llvm.sub %125, %126  : i64
    %128 = llvm.inttoptr %127 : i64 to !llvm.ptr
    %129 = llvm.insertvalue %123, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %128, %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %2, %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %5, %131[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = llvm.insertvalue %5, %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %134 = llvm.insertvalue %5, %133[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %135 = llvm.insertvalue %4, %134[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %136 = builtin.unrealized_conversion_cast %135 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %137 = llvm.extractvalue %135[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %138 = llvm.extractvalue %135[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %139 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %140 = llvm.insertvalue %137, %139[0] : !llvm.struct<(ptr, ptr, i64)> 
    %141 = llvm.insertvalue %138, %140[1] : !llvm.struct<(ptr, ptr, i64)> 
    %142 = llvm.mlir.constant(0 : index) : i64
    %143 = llvm.insertvalue %142, %141[2] : !llvm.struct<(ptr, ptr, i64)> 
    %144 = llvm.extractvalue %135[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %145 = llvm.extractvalue %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %146 = llvm.extractvalue %135[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %147 = llvm.extractvalue %135[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %148 = llvm.extractvalue %135[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %150 = llvm.extractvalue %143[0] : !llvm.struct<(ptr, ptr, i64)> 
    %151 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64)> 
    %152 = llvm.insertvalue %150, %149[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %153 = llvm.insertvalue %151, %152[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %154 = llvm.mlir.constant(0 : index) : i64
    %155 = llvm.insertvalue %154, %153[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %156 = llvm.mlir.constant(1048576 : index) : i64
    %157 = llvm.insertvalue %156, %155[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %158 = llvm.mlir.constant(1 : index) : i64
    %159 = llvm.insertvalue %158, %157[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %160 = builtin.unrealized_conversion_cast %159 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %161 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %162 = builtin.unrealized_conversion_cast %161 : i64 to index
    llvm.br ^bb1(%16 : i64)
  ^bb1(%163: i64):  // 2 preds: ^bb0, ^bb4
    %164 = llvm.icmp "slt" %163, %12 : i64
    llvm.cond_br %164, ^bb2(%16 : i64), ^bb5(%16 : i64)
  ^bb2(%165: i64):  // 2 preds: ^bb1, ^bb3
    %166 = llvm.icmp "slt" %165, %12 : i64
    llvm.cond_br %166, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %167 = llvm.mul %163, %12  : i64
    %168 = llvm.add %167, %165  : i64
    %169 = builtin.unrealized_conversion_cast %168 : i64 to index
    %170 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %171 = llvm.getelementptr %170[%168] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %20, %171 : f64, !llvm.ptr
    %172 = llvm.add %165, %14  : i64
    llvm.br ^bb2(%172 : i64)
  ^bb4:  // pred: ^bb2
    %173 = llvm.add %163, %14  : i64
    llvm.br ^bb1(%173 : i64)
  ^bb5(%174: i64):  // 2 preds: ^bb1, ^bb8
    %175 = llvm.icmp "slt" %174, %12 : i64
    llvm.cond_br %175, ^bb6(%16 : i64), ^bb9(%16 : i64)
  ^bb6(%176: i64):  // 2 preds: ^bb5, ^bb7
    %177 = llvm.icmp "slt" %176, %12 : i64
    llvm.cond_br %177, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %178 = llvm.mul %174, %12  : i64
    %179 = llvm.add %178, %176  : i64
    %180 = builtin.unrealized_conversion_cast %179 : i64 to index
    %181 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %182 = llvm.getelementptr %181[%179] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %19, %182 : f64, !llvm.ptr
    %183 = llvm.add %176, %14  : i64
    llvm.br ^bb6(%183 : i64)
  ^bb8:  // pred: ^bb6
    %184 = llvm.add %174, %14  : i64
    llvm.br ^bb5(%184 : i64)
  ^bb9(%185: i64):  // 2 preds: ^bb5, ^bb12
    %186 = llvm.icmp "slt" %185, %12 : i64
    llvm.cond_br %186, ^bb10(%16 : i64), ^bb13
  ^bb10(%187: i64):  // 2 preds: ^bb9, ^bb11
    %188 = llvm.icmp "slt" %187, %12 : i64
    llvm.cond_br %188, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %189 = llvm.mul %185, %12  : i64
    %190 = llvm.add %189, %187  : i64
    %191 = builtin.unrealized_conversion_cast %190 : i64 to index
    %192 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %193 = llvm.getelementptr %192[%190] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %18, %193 : f64, !llvm.ptr
    %194 = llvm.add %187, %14  : i64
    llvm.br ^bb10(%194 : i64)
  ^bb12:  // pred: ^bb10
    %195 = llvm.add %185, %14  : i64
    llvm.br ^bb9(%195 : i64)
  ^bb13:  // pred: ^bb9
    %196 = builtin.unrealized_conversion_cast %79 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %197 = llvm.extractvalue %79[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %199 = llvm.extractvalue %79[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %200 = llvm.extractvalue %79[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %201 = llvm.extractvalue %79[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%81, %197, %198, %199, %200, %201, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %202 = builtin.unrealized_conversion_cast %119 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %203 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %204 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %205 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %206 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %207 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%121, %203, %204, %205, %206, %207, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %208 = builtin.unrealized_conversion_cast %159 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %209 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %210 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %211 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %212 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %213 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%161, %209, %210, %211, %212, %213, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %214 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %215 = llvm.getelementptr %214[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %6, %215 : i64, !llvm.ptr
    %216 = llvm.ptrtoint %214 : !llvm.ptr to i64
    %217 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %218 = llvm.getelementptr %217[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %218 : i64, !llvm.ptr
    %219 = llvm.ptrtoint %217 : !llvm.ptr to i64
    %220 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %221 = llvm.getelementptr %220[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %221 : i64, !llvm.ptr
    %222 = llvm.ptrtoint %220 : !llvm.ptr to i64
    %223 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %224 = llvm.getelementptr %223[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %224 : i64, !llvm.ptr
    %225 = llvm.ptrtoint %223 : !llvm.ptr to i64
    %226 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %227 = llvm.getelementptr %226[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %81, %227 : i64, !llvm.ptr
    %228 = llvm.ptrtoint %226 : !llvm.ptr to i64
    %229 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %230 = llvm.getelementptr %229[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %230 : i64, !llvm.ptr
    %231 = llvm.ptrtoint %229 : !llvm.ptr to i64
    %232 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %233 = llvm.getelementptr %232[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %121, %233 : i64, !llvm.ptr
    %234 = llvm.ptrtoint %232 : !llvm.ptr to i64
    %235 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %236 = llvm.getelementptr %235[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %236 : i64, !llvm.ptr
    %237 = llvm.ptrtoint %235 : !llvm.ptr to i64
    %238 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %239 = llvm.getelementptr %238[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %161, %239 : i64, !llvm.ptr
    %240 = llvm.ptrtoint %238 : !llvm.ptr to i64
    %241 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %242 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %243 = llvm.insertvalue %241, %242[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %244 = llvm.insertvalue %241, %243[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %245 = llvm.insertvalue %2, %244[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %246 = llvm.insertvalue %1, %245[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %247 = llvm.insertvalue %4, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %248 = builtin.unrealized_conversion_cast %247 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %249 = llvm.getelementptr %241[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %216, %249 : i64, !llvm.ptr
    %250 = llvm.getelementptr %241[%14] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %219, %250 : i64, !llvm.ptr
    %251 = llvm.getelementptr %241[%23] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %222, %251 : i64, !llvm.ptr
    %252 = llvm.getelementptr %241[%25] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %225, %252 : i64, !llvm.ptr
    %253 = llvm.getelementptr %241[%27] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %228, %253 : i64, !llvm.ptr
    %254 = llvm.getelementptr %241[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %231, %254 : i64, !llvm.ptr
    %255 = llvm.getelementptr %241[%31] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %234, %255 : i64, !llvm.ptr
    %256 = llvm.getelementptr %241[%34] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %237, %256 : i64, !llvm.ptr
    %257 = llvm.getelementptr %241[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %240, %257 : i64, !llvm.ptr
    %258 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %259 = llvm.getelementptr %258[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    %260 = llvm.extractvalue %247[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %261 = llvm.extractvalue %247[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %262 = llvm.extractvalue %247[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %263 = llvm.extractvalue %247[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %264 = llvm.extractvalue %247[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaLaunchKernel(%8, %6, %14, %8, %10, %14, %260, %261, %262, %263, %264, %259, %33, %22, %27, %8) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    %265 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %266 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %267 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %268 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %269 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%161, %265, %266, %267, %268, %269, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %270 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %271 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %272 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %273 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %274 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%121, %270, %271, %272, %273, %274, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %275 = llvm.extractvalue %79[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %276 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %277 = llvm.extractvalue %79[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %278 = llvm.extractvalue %79[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %279 = llvm.extractvalue %79[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%81, %275, %276, %277, %278, %279, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %280 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %135, %280 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %281 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %282 = llvm.insertvalue %0, %281[0] : !llvm.struct<(i64, ptr)> 
    %283 = llvm.insertvalue %280, %282[1] : !llvm.struct<(i64, ptr)> 
    %284 = builtin.unrealized_conversion_cast %283 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %285 = llvm.extractvalue %283[0] : !llvm.struct<(i64, ptr)> 
    %286 = llvm.extractvalue %283[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%285, %286) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%81) : (i64) -> ()
    llvm.call @cudaFree(%121) : (i64) -> ()
    llvm.call @cudaFree(%161) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = builtin.unrealized_conversion_cast %6 : i64 to index
    %8 = llvm.mlir.constant(32 : index) : i64
    %9 = builtin.unrealized_conversion_cast %8 : i64 to index
    %10 = llvm.mlir.constant(8 : index) : i64
    %11 = builtin.unrealized_conversion_cast %10 : i64 to index
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = builtin.unrealized_conversion_cast %12 : i64 to index
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = builtin.unrealized_conversion_cast %14 : i64 to index
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = builtin.unrealized_conversion_cast %16 : i64 to index
    %18 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %19 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %20 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %21 = llvm.mlir.constant(1048576 : index) : i64
    %22 = llvm.mlir.constant(0 : i32) : i32
    %23 = llvm.mlir.constant(2 : index) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(3 : index) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(4 : index) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(5 : index) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(6 : index) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(12 : index) : i64
    %34 = llvm.mlir.constant(7 : index) : i64
    %35 = builtin.unrealized_conversion_cast %34 : i64 to index
    %36 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%36) : (!llvm.ptr) -> ()
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %3  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %3, %4  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %3  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %2, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %5, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %5, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %5, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %4, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %57 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %60 = llvm.insertvalue %57, %59[0] : !llvm.struct<(ptr, ptr, i64)> 
    %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr, ptr, i64)> 
    %62 = llvm.mlir.constant(0 : index) : i64
    %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %70 = llvm.extractvalue %63[0] : !llvm.struct<(ptr, ptr, i64)> 
    %71 = llvm.extractvalue %63[1] : !llvm.struct<(ptr, ptr, i64)> 
    %72 = llvm.insertvalue %70, %69[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %73 = llvm.insertvalue %71, %72[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %74 = llvm.mlir.constant(0 : index) : i64
    %75 = llvm.insertvalue %74, %73[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %76 = llvm.mlir.constant(1048576 : index) : i64
    %77 = llvm.insertvalue %76, %75[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %78 = llvm.mlir.constant(1 : index) : i64
    %79 = llvm.insertvalue %78, %77[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %80 = builtin.unrealized_conversion_cast %79 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %81 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %82 = builtin.unrealized_conversion_cast %81 : i64 to index
    %83 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.add %84, %43  : i64
    %86 = llvm.urem %85, %3  : i64
    %87 = llvm.sub %85, %86  : i64
    %88 = llvm.inttoptr %87 : i64 to !llvm.ptr
    %89 = llvm.insertvalue %83, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %88, %89[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %2, %90[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %5, %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.insertvalue %5, %92[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.insertvalue %5, %93[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = llvm.insertvalue %4, %94[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = builtin.unrealized_conversion_cast %95 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %97 = llvm.extractvalue %95[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.extractvalue %95[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %99 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %100 = llvm.insertvalue %97, %99[0] : !llvm.struct<(ptr, ptr, i64)> 
    %101 = llvm.insertvalue %98, %100[1] : !llvm.struct<(ptr, ptr, i64)> 
    %102 = llvm.mlir.constant(0 : index) : i64
    %103 = llvm.insertvalue %102, %101[2] : !llvm.struct<(ptr, ptr, i64)> 
    %104 = llvm.extractvalue %95[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %105 = llvm.extractvalue %95[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %106 = llvm.extractvalue %95[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %107 = llvm.extractvalue %95[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.extractvalue %95[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %109 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %110 = llvm.extractvalue %103[0] : !llvm.struct<(ptr, ptr, i64)> 
    %111 = llvm.extractvalue %103[1] : !llvm.struct<(ptr, ptr, i64)> 
    %112 = llvm.insertvalue %110, %109[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %113 = llvm.insertvalue %111, %112[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %114 = llvm.mlir.constant(0 : index) : i64
    %115 = llvm.insertvalue %114, %113[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %116 = llvm.mlir.constant(1048576 : index) : i64
    %117 = llvm.insertvalue %116, %115[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %118 = llvm.mlir.constant(1 : index) : i64
    %119 = llvm.insertvalue %118, %117[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %120 = builtin.unrealized_conversion_cast %119 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %121 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %122 = builtin.unrealized_conversion_cast %121 : i64 to index
    %123 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %124 = llvm.ptrtoint %123 : !llvm.ptr to i64
    %125 = llvm.add %124, %43  : i64
    %126 = llvm.urem %125, %3  : i64
    %127 = llvm.sub %125, %126  : i64
    %128 = llvm.inttoptr %127 : i64 to !llvm.ptr
    %129 = llvm.insertvalue %123, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %128, %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %2, %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %5, %131[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = llvm.insertvalue %5, %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %134 = llvm.insertvalue %5, %133[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %135 = llvm.insertvalue %4, %134[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %136 = builtin.unrealized_conversion_cast %135 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<1024x1024xf64>
    %137 = llvm.extractvalue %135[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %138 = llvm.extractvalue %135[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %139 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %140 = llvm.insertvalue %137, %139[0] : !llvm.struct<(ptr, ptr, i64)> 
    %141 = llvm.insertvalue %138, %140[1] : !llvm.struct<(ptr, ptr, i64)> 
    %142 = llvm.mlir.constant(0 : index) : i64
    %143 = llvm.insertvalue %142, %141[2] : !llvm.struct<(ptr, ptr, i64)> 
    %144 = llvm.extractvalue %135[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %145 = llvm.extractvalue %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %146 = llvm.extractvalue %135[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %147 = llvm.extractvalue %135[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %148 = llvm.extractvalue %135[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %150 = llvm.extractvalue %143[0] : !llvm.struct<(ptr, ptr, i64)> 
    %151 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64)> 
    %152 = llvm.insertvalue %150, %149[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %153 = llvm.insertvalue %151, %152[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %154 = llvm.mlir.constant(0 : index) : i64
    %155 = llvm.insertvalue %154, %153[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %156 = llvm.mlir.constant(1048576 : index) : i64
    %157 = llvm.insertvalue %156, %155[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %158 = llvm.mlir.constant(1 : index) : i64
    %159 = llvm.insertvalue %158, %157[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %160 = builtin.unrealized_conversion_cast %159 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<1048576xf64>
    %161 = llvm.call @cudaMallocF64(%21) : (i64) -> i64
    %162 = builtin.unrealized_conversion_cast %161 : i64 to index
    llvm.br ^bb1(%16 : i64)
  ^bb1(%163: i64):  // 2 preds: ^bb0, ^bb4
    %164 = llvm.icmp "slt" %163, %12 : i64
    llvm.cond_br %164, ^bb2(%16 : i64), ^bb5(%16 : i64)
  ^bb2(%165: i64):  // 2 preds: ^bb1, ^bb3
    %166 = llvm.icmp "slt" %165, %12 : i64
    llvm.cond_br %166, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %167 = llvm.mul %163, %12  : i64
    %168 = llvm.add %167, %165  : i64
    %169 = builtin.unrealized_conversion_cast %168 : i64 to index
    %170 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %171 = llvm.getelementptr %170[%168] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %20, %171 : f64, !llvm.ptr
    %172 = llvm.add %165, %14  : i64
    llvm.br ^bb2(%172 : i64)
  ^bb4:  // pred: ^bb2
    %173 = llvm.add %163, %14  : i64
    llvm.br ^bb1(%173 : i64)
  ^bb5(%174: i64):  // 2 preds: ^bb1, ^bb8
    %175 = llvm.icmp "slt" %174, %12 : i64
    llvm.cond_br %175, ^bb6(%16 : i64), ^bb9(%16 : i64)
  ^bb6(%176: i64):  // 2 preds: ^bb5, ^bb7
    %177 = llvm.icmp "slt" %176, %12 : i64
    llvm.cond_br %177, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %178 = llvm.mul %174, %12  : i64
    %179 = llvm.add %178, %176  : i64
    %180 = builtin.unrealized_conversion_cast %179 : i64 to index
    %181 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %182 = llvm.getelementptr %181[%179] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %19, %182 : f64, !llvm.ptr
    %183 = llvm.add %176, %14  : i64
    llvm.br ^bb6(%183 : i64)
  ^bb8:  // pred: ^bb6
    %184 = llvm.add %174, %14  : i64
    llvm.br ^bb5(%184 : i64)
  ^bb9(%185: i64):  // 2 preds: ^bb5, ^bb12
    %186 = llvm.icmp "slt" %185, %12 : i64
    llvm.cond_br %186, ^bb10(%16 : i64), ^bb13
  ^bb10(%187: i64):  // 2 preds: ^bb9, ^bb11
    %188 = llvm.icmp "slt" %187, %12 : i64
    llvm.cond_br %188, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %189 = llvm.mul %185, %12  : i64
    %190 = llvm.add %189, %187  : i64
    %191 = builtin.unrealized_conversion_cast %190 : i64 to index
    %192 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %193 = llvm.getelementptr %192[%190] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %18, %193 : f64, !llvm.ptr
    %194 = llvm.add %187, %14  : i64
    llvm.br ^bb10(%194 : i64)
  ^bb12:  // pred: ^bb10
    %195 = llvm.add %185, %14  : i64
    llvm.br ^bb9(%195 : i64)
  ^bb13:  // pred: ^bb9
    %196 = builtin.unrealized_conversion_cast %79 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %197 = llvm.extractvalue %79[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %199 = llvm.extractvalue %79[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %200 = llvm.extractvalue %79[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %201 = llvm.extractvalue %79[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%81, %197, %198, %199, %200, %201, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %202 = builtin.unrealized_conversion_cast %119 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %203 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %204 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %205 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %206 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %207 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%121, %203, %204, %205, %206, %207, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %208 = builtin.unrealized_conversion_cast %159 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xf64>
    %209 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %210 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %211 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %212 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %213 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%161, %209, %210, %211, %212, %213, %16) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %214 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %215 = llvm.getelementptr %214[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %6, %215 : i64, !llvm.ptr
    %216 = llvm.ptrtoint %214 : !llvm.ptr to i64
    %217 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %218 = llvm.getelementptr %217[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %8, %218 : i64, !llvm.ptr
    %219 = llvm.ptrtoint %217 : !llvm.ptr to i64
    %220 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %221 = llvm.getelementptr %220[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %221 : i64, !llvm.ptr
    %222 = llvm.ptrtoint %220 : !llvm.ptr to i64
    %223 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %224 = llvm.getelementptr %223[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %224 : i64, !llvm.ptr
    %225 = llvm.ptrtoint %223 : !llvm.ptr to i64
    %226 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %227 = llvm.getelementptr %226[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %81, %227 : i64, !llvm.ptr
    %228 = llvm.ptrtoint %226 : !llvm.ptr to i64
    %229 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %230 = llvm.getelementptr %229[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %230 : i64, !llvm.ptr
    %231 = llvm.ptrtoint %229 : !llvm.ptr to i64
    %232 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %233 = llvm.getelementptr %232[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %121, %233 : i64, !llvm.ptr
    %234 = llvm.ptrtoint %232 : !llvm.ptr to i64
    %235 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %236 = llvm.getelementptr %235[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %12, %236 : i64, !llvm.ptr
    %237 = llvm.ptrtoint %235 : !llvm.ptr to i64
    %238 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    %239 = llvm.getelementptr %238[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %161, %239 : i64, !llvm.ptr
    %240 = llvm.ptrtoint %238 : !llvm.ptr to i64
    %241 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %242 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %243 = llvm.insertvalue %241, %242[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %244 = llvm.insertvalue %241, %243[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %245 = llvm.insertvalue %2, %244[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %246 = llvm.insertvalue %1, %245[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %247 = llvm.insertvalue %4, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %248 = builtin.unrealized_conversion_cast %247 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<?xindex>
    %249 = llvm.getelementptr %241[%16] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %216, %249 : i64, !llvm.ptr
    %250 = llvm.getelementptr %241[%14] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %219, %250 : i64, !llvm.ptr
    %251 = llvm.getelementptr %241[%23] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %222, %251 : i64, !llvm.ptr
    %252 = llvm.getelementptr %241[%25] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %225, %252 : i64, !llvm.ptr
    %253 = llvm.getelementptr %241[%27] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %228, %253 : i64, !llvm.ptr
    %254 = llvm.getelementptr %241[%29] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %231, %254 : i64, !llvm.ptr
    %255 = llvm.getelementptr %241[%31] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %234, %255 : i64, !llvm.ptr
    %256 = llvm.getelementptr %241[%34] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %237, %256 : i64, !llvm.ptr
    %257 = llvm.getelementptr %241[%10] : (!llvm.ptr, i64) -> !llvm.ptr, i64
    llvm.store %240, %257 : i64, !llvm.ptr
    %258 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %259 = llvm.getelementptr %258[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    %260 = llvm.extractvalue %247[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %261 = llvm.extractvalue %247[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %262 = llvm.extractvalue %247[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %263 = llvm.extractvalue %247[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %264 = llvm.extractvalue %247[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaLaunchKernel(%8, %6, %14, %8, %10, %14, %260, %261, %262, %263, %264, %259, %33, %22, %27, %8) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    %265 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %266 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %267 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %268 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %269 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%161, %265, %266, %267, %268, %269, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %270 = llvm.extractvalue %119[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %271 = llvm.extractvalue %119[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %272 = llvm.extractvalue %119[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %273 = llvm.extractvalue %119[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %274 = llvm.extractvalue %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%121, %270, %271, %272, %273, %274, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %275 = llvm.extractvalue %79[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %276 = llvm.extractvalue %79[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %277 = llvm.extractvalue %79[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %278 = llvm.extractvalue %79[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %279 = llvm.extractvalue %79[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.call @cudaMemcpyF64(%81, %275, %276, %277, %278, %279, %14) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %280 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %135, %280 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %281 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %282 = llvm.insertvalue %0, %281[0] : !llvm.struct<(i64, ptr)> 
    %283 = llvm.insertvalue %280, %282[1] : !llvm.struct<(i64, ptr)> 
    %284 = builtin.unrealized_conversion_cast %283 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %285 = llvm.extractvalue %283[0] : !llvm.struct<(i64, ptr)> 
    %286 = llvm.extractvalue %283[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%285, %286) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%81) : (i64) -> ()
    llvm.call @cudaFree(%121) : (i64) -> ()
    llvm.call @cudaFree(%161) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = llvm.mlir.constant(32 : index) : i64
    %8 = llvm.mlir.constant(8 : index) : i64
    %9 = llvm.mlir.constant(1024 : index) : i64
    %10 = llvm.mlir.constant(1 : index) : i64
    %11 = llvm.mlir.constant(0 : index) : i64
    %12 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %13 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %14 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %15 = llvm.mlir.constant(1048576 : index) : i64
    %16 = llvm.mlir.constant(0 : i32) : i32
    %17 = llvm.mlir.constant(2 : index) : i64
    %18 = llvm.mlir.constant(3 : index) : i64
    %19 = llvm.mlir.constant(4 : index) : i64
    %20 = llvm.mlir.constant(5 : index) : i64
    %21 = llvm.mlir.constant(6 : index) : i64
    %22 = llvm.mlir.constant(12 : index) : i64
    %23 = llvm.mlir.constant(7 : index) : i64
    %24 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%24) : (!llvm.ptr) -> ()
    %25 = llvm.mlir.zero : !llvm.ptr
    %26 = llvm.getelementptr %25[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i64
    %28 = llvm.add %27, %3  : i64
    %29 = llvm.call @malloc(%28) : (i64) -> !llvm.ptr
    %30 = llvm.ptrtoint %29 : !llvm.ptr to i64
    %31 = llvm.sub %3, %4  : i64
    %32 = llvm.add %30, %31  : i64
    %33 = llvm.urem %32, %3  : i64
    %34 = llvm.sub %32, %33  : i64
    %35 = llvm.inttoptr %34 : i64 to !llvm.ptr
    %36 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %37 = llvm.insertvalue %29, %36[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %35, %37[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %2, %38[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %5, %39[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %5, %40[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.insertvalue %5, %41[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %4, %42[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %45 = llvm.insertvalue %29, %44[0] : !llvm.struct<(ptr, ptr, i64)> 
    %46 = llvm.insertvalue %35, %45[1] : !llvm.struct<(ptr, ptr, i64)> 
    %47 = llvm.mlir.constant(0 : index) : i64
    %48 = llvm.insertvalue %47, %46[2] : !llvm.struct<(ptr, ptr, i64)> 
    %49 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %50 = llvm.insertvalue %29, %49[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %51 = llvm.insertvalue %35, %50[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %52 = llvm.mlir.constant(0 : index) : i64
    %53 = llvm.insertvalue %52, %51[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %54 = llvm.mlir.constant(1048576 : index) : i64
    %55 = llvm.insertvalue %54, %53[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %56 = llvm.mlir.constant(1 : index) : i64
    %57 = llvm.insertvalue %56, %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %58 = llvm.call @cudaMallocF64(%15) : (i64) -> i64
    %59 = llvm.call @malloc(%28) : (i64) -> !llvm.ptr
    %60 = llvm.ptrtoint %59 : !llvm.ptr to i64
    %61 = llvm.add %60, %31  : i64
    %62 = llvm.urem %61, %3  : i64
    %63 = llvm.sub %61, %62  : i64
    %64 = llvm.inttoptr %63 : i64 to !llvm.ptr
    %65 = llvm.insertvalue %59, %36[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %67 = llvm.insertvalue %2, %66[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.insertvalue %5, %67[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %69 = llvm.insertvalue %5, %68[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %5, %69[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %4, %70[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %73 = llvm.insertvalue %59, %72[0] : !llvm.struct<(ptr, ptr, i64)> 
    %74 = llvm.insertvalue %64, %73[1] : !llvm.struct<(ptr, ptr, i64)> 
    %75 = llvm.mlir.constant(0 : index) : i64
    %76 = llvm.insertvalue %75, %74[2] : !llvm.struct<(ptr, ptr, i64)> 
    %77 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %78 = llvm.insertvalue %59, %77[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %79 = llvm.insertvalue %64, %78[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %80 = llvm.mlir.constant(0 : index) : i64
    %81 = llvm.insertvalue %80, %79[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %82 = llvm.mlir.constant(1048576 : index) : i64
    %83 = llvm.insertvalue %82, %81[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %84 = llvm.mlir.constant(1 : index) : i64
    %85 = llvm.insertvalue %84, %83[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %86 = llvm.call @cudaMallocF64(%15) : (i64) -> i64
    %87 = llvm.call @malloc(%28) : (i64) -> !llvm.ptr
    %88 = llvm.ptrtoint %87 : !llvm.ptr to i64
    %89 = llvm.add %88, %31  : i64
    %90 = llvm.urem %89, %3  : i64
    %91 = llvm.sub %89, %90  : i64
    %92 = llvm.inttoptr %91 : i64 to !llvm.ptr
    %93 = llvm.insertvalue %87, %36[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.insertvalue %92, %93[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = llvm.insertvalue %2, %94[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = llvm.insertvalue %5, %95[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %97 = llvm.insertvalue %5, %96[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.insertvalue %5, %97[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %99 = llvm.insertvalue %4, %98[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %100 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %101 = llvm.insertvalue %87, %100[0] : !llvm.struct<(ptr, ptr, i64)> 
    %102 = llvm.insertvalue %92, %101[1] : !llvm.struct<(ptr, ptr, i64)> 
    %103 = llvm.mlir.constant(0 : index) : i64
    %104 = llvm.insertvalue %103, %102[2] : !llvm.struct<(ptr, ptr, i64)> 
    %105 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %106 = llvm.insertvalue %87, %105[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %107 = llvm.insertvalue %92, %106[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %108 = llvm.mlir.constant(0 : index) : i64
    %109 = llvm.insertvalue %108, %107[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %110 = llvm.mlir.constant(1048576 : index) : i64
    %111 = llvm.insertvalue %110, %109[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %112 = llvm.mlir.constant(1 : index) : i64
    %113 = llvm.insertvalue %112, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %114 = llvm.call @cudaMallocF64(%15) : (i64) -> i64
    llvm.br ^bb1(%11 : i64)
  ^bb1(%115: i64):  // 2 preds: ^bb0, ^bb4
    %116 = llvm.icmp "slt" %115, %9 : i64
    llvm.cond_br %116, ^bb2(%11 : i64), ^bb5(%11 : i64)
  ^bb2(%117: i64):  // 2 preds: ^bb1, ^bb3
    %118 = llvm.icmp "slt" %117, %9 : i64
    llvm.cond_br %118, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %119 = llvm.mul %115, %9  : i64
    %120 = llvm.add %119, %117  : i64
    %121 = llvm.getelementptr %35[%120] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %14, %121 : f64, !llvm.ptr
    %122 = llvm.add %117, %10  : i64
    llvm.br ^bb2(%122 : i64)
  ^bb4:  // pred: ^bb2
    %123 = llvm.add %115, %10  : i64
    llvm.br ^bb1(%123 : i64)
  ^bb5(%124: i64):  // 2 preds: ^bb1, ^bb8
    %125 = llvm.icmp "slt" %124, %9 : i64
    llvm.cond_br %125, ^bb6(%11 : i64), ^bb9(%11 : i64)
  ^bb6(%126: i64):  // 2 preds: ^bb5, ^bb7
    %127 = llvm.icmp "slt" %126, %9 : i64
    llvm.cond_br %127, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %128 = llvm.mul %124, %9  : i64
    %129 = llvm.add %128, %126  : i64
    %130 = llvm.getelementptr %64[%129] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %13, %130 : f64, !llvm.ptr
    %131 = llvm.add %126, %10  : i64
    llvm.br ^bb6(%131 : i64)
  ^bb8:  // pred: ^bb6
    %132 = llvm.add %124, %10  : i64
    llvm.br ^bb5(%132 : i64)
  ^bb9(%133: i64):  // 2 preds: ^bb5, ^bb12
    %134 = llvm.icmp "slt" %133, %9 : i64
    llvm.cond_br %134, ^bb10(%11 : i64), ^bb13
  ^bb10(%135: i64):  // 2 preds: ^bb9, ^bb11
    %136 = llvm.icmp "slt" %135, %9 : i64
    llvm.cond_br %136, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %137 = llvm.mul %133, %9  : i64
    %138 = llvm.add %137, %135  : i64
    %139 = llvm.getelementptr %92[%138] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %12, %139 : f64, !llvm.ptr
    %140 = llvm.add %135, %10  : i64
    llvm.br ^bb10(%140 : i64)
  ^bb12:  // pred: ^bb10
    %141 = llvm.add %133, %10  : i64
    llvm.br ^bb9(%141 : i64)
  ^bb13:  // pred: ^bb9
    llvm.call @cudaMemcpyF64(%58, %29, %35, %52, %54, %56, %11) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%86, %59, %64, %80, %82, %84, %11) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%114, %87, %92, %108, %110, %112, %11) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %142 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %6, %142 : i64, !llvm.ptr
    %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %144 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %7, %144 : i64, !llvm.ptr
    %145 = llvm.ptrtoint %144 : !llvm.ptr to i64
    %146 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %146 : i64, !llvm.ptr
    %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
    %148 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %148 : i64, !llvm.ptr
    %149 = llvm.ptrtoint %148 : !llvm.ptr to i64
    %150 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %58, %150 : i64, !llvm.ptr
    %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
    %152 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %152 : i64, !llvm.ptr
    %153 = llvm.ptrtoint %152 : !llvm.ptr to i64
    %154 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %86, %154 : i64, !llvm.ptr
    %155 = llvm.ptrtoint %154 : !llvm.ptr to i64
    %156 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %156 : i64, !llvm.ptr
    %157 = llvm.ptrtoint %156 : !llvm.ptr to i64
    %158 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %114, %158 : i64, !llvm.ptr
    %159 = llvm.ptrtoint %158 : !llvm.ptr to i64
    %160 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %161 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %162 = llvm.insertvalue %160, %161[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %163 = llvm.insertvalue %160, %162[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %164 = llvm.insertvalue %2, %163[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %165 = llvm.insertvalue %1, %164[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %166 = llvm.insertvalue %4, %165[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    llvm.store %143, %160 : i64, !llvm.ptr
    %167 = llvm.getelementptr %160[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %145, %167 : i64, !llvm.ptr
    %168 = llvm.getelementptr %160[2] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %147, %168 : i64, !llvm.ptr
    %169 = llvm.getelementptr %160[3] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %149, %169 : i64, !llvm.ptr
    %170 = llvm.getelementptr %160[4] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %151, %170 : i64, !llvm.ptr
    %171 = llvm.getelementptr %160[5] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %153, %171 : i64, !llvm.ptr
    %172 = llvm.getelementptr %160[6] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %155, %172 : i64, !llvm.ptr
    %173 = llvm.getelementptr %160[7] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %157, %173 : i64, !llvm.ptr
    %174 = llvm.getelementptr %160[8] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %159, %174 : i64, !llvm.ptr
    %175 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %176 = llvm.getelementptr %175[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    llvm.call @cudaLaunchKernel(%7, %6, %10, %7, %8, %10, %160, %160, %2, %1, %4, %176, %22, %16, %19, %7) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%114, %87, %92, %108, %110, %112, %10) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%86, %59, %64, %80, %82, %84, %10) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%58, %29, %35, %52, %54, %56, %10) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %177 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %99, %177 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %178 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %179 = llvm.insertvalue %0, %178[0] : !llvm.struct<(i64, ptr)> 
    %180 = llvm.insertvalue %177, %179[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%0, %177) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%58) : (i64) -> ()
    llvm.call @cudaFree(%86) : (i64) -> ()
    llvm.call @cudaFree(%114) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = llvm.mlir.constant(8 : index) : i64
    %8 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %9 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %10 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %11 = llvm.mlir.constant(1048576 : index) : i64
    %12 = llvm.mlir.constant(0 : i32) : i32
    %13 = llvm.mlir.constant(4 : index) : i64
    %14 = llvm.mlir.constant(12 : index) : i64
    %15 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%15) : (!llvm.ptr) -> ()
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.add %18, %3  : i64
    %20 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.sub %3, %4  : i64
    %23 = llvm.add %21, %22  : i64
    %24 = llvm.urem %23, %3  : i64
    %25 = llvm.sub %23, %24  : i64
    %26 = llvm.inttoptr %25 : i64 to !llvm.ptr
    %27 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %28 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %29 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %30 = llvm.ptrtoint %29 : !llvm.ptr to i64
    %31 = llvm.add %30, %22  : i64
    %32 = llvm.urem %31, %3  : i64
    %33 = llvm.sub %31, %32  : i64
    %34 = llvm.inttoptr %33 : i64 to !llvm.ptr
    %35 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %36 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %37 = llvm.ptrtoint %36 : !llvm.ptr to i64
    %38 = llvm.add %37, %22  : i64
    %39 = llvm.urem %38, %3  : i64
    %40 = llvm.sub %38, %39  : i64
    %41 = llvm.inttoptr %40 : i64 to !llvm.ptr
    %42 = llvm.insertvalue %36, %27[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %41, %42[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %2, %43[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %5, %44[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.insertvalue %5, %45[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %47 = llvm.insertvalue %5, %46[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.insertvalue %4, %47[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    llvm.br ^bb1(%2 : i64)
  ^bb1(%50: i64):  // 2 preds: ^bb0, ^bb4
    %51 = llvm.icmp "slt" %50, %5 : i64
    llvm.cond_br %51, ^bb2(%2 : i64), ^bb5(%2 : i64)
  ^bb2(%52: i64):  // 2 preds: ^bb1, ^bb3
    %53 = llvm.icmp "slt" %52, %5 : i64
    llvm.cond_br %53, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %54 = llvm.mul %50, %5  : i64
    %55 = llvm.add %54, %52  : i64
    %56 = llvm.getelementptr %26[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %56 : f64, !llvm.ptr
    %57 = llvm.add %52, %4  : i64
    llvm.br ^bb2(%57 : i64)
  ^bb4:  // pred: ^bb2
    %58 = llvm.add %50, %4  : i64
    llvm.br ^bb1(%58 : i64)
  ^bb5(%59: i64):  // 2 preds: ^bb1, ^bb8
    %60 = llvm.icmp "slt" %59, %5 : i64
    llvm.cond_br %60, ^bb6(%2 : i64), ^bb9(%2 : i64)
  ^bb6(%61: i64):  // 2 preds: ^bb5, ^bb7
    %62 = llvm.icmp "slt" %61, %5 : i64
    llvm.cond_br %62, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %63 = llvm.mul %59, %5  : i64
    %64 = llvm.add %63, %61  : i64
    %65 = llvm.getelementptr %34[%64] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %65 : f64, !llvm.ptr
    %66 = llvm.add %61, %4  : i64
    llvm.br ^bb6(%66 : i64)
  ^bb8:  // pred: ^bb6
    %67 = llvm.add %59, %4  : i64
    llvm.br ^bb5(%67 : i64)
  ^bb9(%68: i64):  // 2 preds: ^bb5, ^bb12
    %69 = llvm.icmp "slt" %68, %5 : i64
    llvm.cond_br %69, ^bb10(%2 : i64), ^bb13
  ^bb10(%70: i64):  // 2 preds: ^bb9, ^bb11
    %71 = llvm.icmp "slt" %70, %5 : i64
    llvm.cond_br %71, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %72 = llvm.mul %68, %5  : i64
    %73 = llvm.add %72, %70  : i64
    %74 = llvm.getelementptr %41[%73] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %8, %74 : f64, !llvm.ptr
    %75 = llvm.add %70, %4  : i64
    llvm.br ^bb10(%75 : i64)
  ^bb12:  // pred: ^bb10
    %76 = llvm.add %68, %4  : i64
    llvm.br ^bb9(%76 : i64)
  ^bb13:  // pred: ^bb9
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %77 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %6, %77 : i64, !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %3, %79 : i64, !llvm.ptr
    %80 = llvm.ptrtoint %79 : !llvm.ptr to i64
    %81 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %81 : i64, !llvm.ptr
    %82 = llvm.ptrtoint %81 : !llvm.ptr to i64
    %83 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %83 : i64, !llvm.ptr
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %85 : i64, !llvm.ptr
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %87 : i64, !llvm.ptr
    %88 = llvm.ptrtoint %87 : !llvm.ptr to i64
    %89 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %35, %89 : i64, !llvm.ptr
    %90 = llvm.ptrtoint %89 : !llvm.ptr to i64
    %91 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %91 : i64, !llvm.ptr
    %92 = llvm.ptrtoint %91 : !llvm.ptr to i64
    %93 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %49, %93 : i64, !llvm.ptr
    %94 = llvm.ptrtoint %93 : !llvm.ptr to i64
    %95 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %78, %95 : i64, !llvm.ptr
    %96 = llvm.getelementptr %95[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %80, %96 : i64, !llvm.ptr
    %97 = llvm.getelementptr %95[2] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %82, %97 : i64, !llvm.ptr
    %98 = llvm.getelementptr %95[3] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %84, %98 : i64, !llvm.ptr
    %99 = llvm.getelementptr %95[4] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %86, %99 : i64, !llvm.ptr
    %100 = llvm.getelementptr %95[5] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %88, %100 : i64, !llvm.ptr
    %101 = llvm.getelementptr %95[6] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %90, %101 : i64, !llvm.ptr
    %102 = llvm.getelementptr %95[7] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %92, %102 : i64, !llvm.ptr
    %103 = llvm.getelementptr %95[8] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %94, %103 : i64, !llvm.ptr
    %104 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %105 = llvm.getelementptr %104[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    llvm.call @cudaLaunchKernel(%3, %6, %4, %3, %7, %4, %95, %95, %2, %1, %4, %105, %14, %12, %13, %3) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %106 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %48, %106 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %106) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%28) : (i64) -> ()
    llvm.call @cudaFree(%35) : (i64) -> ()
    llvm.call @cudaFree(%49) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


// -----// IR Dump After CSE (cse) //----- //
module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = llvm.mlir.constant(8 : index) : i64
    %8 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %9 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %10 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %11 = llvm.mlir.constant(1048576 : index) : i64
    %12 = llvm.mlir.constant(0 : i32) : i32
    %13 = llvm.mlir.constant(4 : index) : i64
    %14 = llvm.mlir.constant(12 : index) : i64
    %15 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%15) : (!llvm.ptr) -> ()
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.add %18, %3  : i64
    %20 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.sub %3, %4  : i64
    %23 = llvm.add %21, %22  : i64
    %24 = llvm.urem %23, %3  : i64
    %25 = llvm.sub %23, %24  : i64
    %26 = llvm.inttoptr %25 : i64 to !llvm.ptr
    %27 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %28 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %29 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %30 = llvm.ptrtoint %29 : !llvm.ptr to i64
    %31 = llvm.add %30, %22  : i64
    %32 = llvm.urem %31, %3  : i64
    %33 = llvm.sub %31, %32  : i64
    %34 = llvm.inttoptr %33 : i64 to !llvm.ptr
    %35 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %36 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %37 = llvm.ptrtoint %36 : !llvm.ptr to i64
    %38 = llvm.add %37, %22  : i64
    %39 = llvm.urem %38, %3  : i64
    %40 = llvm.sub %38, %39  : i64
    %41 = llvm.inttoptr %40 : i64 to !llvm.ptr
    %42 = llvm.insertvalue %36, %27[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %41, %42[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %2, %43[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %5, %44[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.insertvalue %5, %45[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %47 = llvm.insertvalue %5, %46[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.insertvalue %4, %47[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    llvm.br ^bb1(%2 : i64)
  ^bb1(%50: i64):  // 2 preds: ^bb0, ^bb4
    %51 = llvm.icmp "slt" %50, %5 : i64
    llvm.cond_br %51, ^bb2(%2 : i64), ^bb5(%2 : i64)
  ^bb2(%52: i64):  // 2 preds: ^bb1, ^bb3
    %53 = llvm.icmp "slt" %52, %5 : i64
    llvm.cond_br %53, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %54 = llvm.mul %50, %5  : i64
    %55 = llvm.add %54, %52  : i64
    %56 = llvm.getelementptr %26[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %56 : f64, !llvm.ptr
    %57 = llvm.add %52, %4  : i64
    llvm.br ^bb2(%57 : i64)
  ^bb4:  // pred: ^bb2
    %58 = llvm.add %50, %4  : i64
    llvm.br ^bb1(%58 : i64)
  ^bb5(%59: i64):  // 2 preds: ^bb1, ^bb8
    %60 = llvm.icmp "slt" %59, %5 : i64
    llvm.cond_br %60, ^bb6(%2 : i64), ^bb9(%2 : i64)
  ^bb6(%61: i64):  // 2 preds: ^bb5, ^bb7
    %62 = llvm.icmp "slt" %61, %5 : i64
    llvm.cond_br %62, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %63 = llvm.mul %59, %5  : i64
    %64 = llvm.add %63, %61  : i64
    %65 = llvm.getelementptr %34[%64] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %65 : f64, !llvm.ptr
    %66 = llvm.add %61, %4  : i64
    llvm.br ^bb6(%66 : i64)
  ^bb8:  // pred: ^bb6
    %67 = llvm.add %59, %4  : i64
    llvm.br ^bb5(%67 : i64)
  ^bb9(%68: i64):  // 2 preds: ^bb5, ^bb12
    %69 = llvm.icmp "slt" %68, %5 : i64
    llvm.cond_br %69, ^bb10(%2 : i64), ^bb13
  ^bb10(%70: i64):  // 2 preds: ^bb9, ^bb11
    %71 = llvm.icmp "slt" %70, %5 : i64
    llvm.cond_br %71, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %72 = llvm.mul %68, %5  : i64
    %73 = llvm.add %72, %70  : i64
    %74 = llvm.getelementptr %41[%73] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %8, %74 : f64, !llvm.ptr
    %75 = llvm.add %70, %4  : i64
    llvm.br ^bb10(%75 : i64)
  ^bb12:  // pred: ^bb10
    %76 = llvm.add %68, %4  : i64
    llvm.br ^bb9(%76 : i64)
  ^bb13:  // pred: ^bb9
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %77 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %6, %77 : i64, !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %3, %79 : i64, !llvm.ptr
    %80 = llvm.ptrtoint %79 : !llvm.ptr to i64
    %81 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %81 : i64, !llvm.ptr
    %82 = llvm.ptrtoint %81 : !llvm.ptr to i64
    %83 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %83 : i64, !llvm.ptr
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %85 : i64, !llvm.ptr
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %87 : i64, !llvm.ptr
    %88 = llvm.ptrtoint %87 : !llvm.ptr to i64
    %89 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %35, %89 : i64, !llvm.ptr
    %90 = llvm.ptrtoint %89 : !llvm.ptr to i64
    %91 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %91 : i64, !llvm.ptr
    %92 = llvm.ptrtoint %91 : !llvm.ptr to i64
    %93 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %49, %93 : i64, !llvm.ptr
    %94 = llvm.ptrtoint %93 : !llvm.ptr to i64
    %95 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %78, %95 : i64, !llvm.ptr
    %96 = llvm.getelementptr %95[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %80, %96 : i64, !llvm.ptr
    %97 = llvm.getelementptr %95[2] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %82, %97 : i64, !llvm.ptr
    %98 = llvm.getelementptr %95[3] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %84, %98 : i64, !llvm.ptr
    %99 = llvm.getelementptr %95[4] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %86, %99 : i64, !llvm.ptr
    %100 = llvm.getelementptr %95[5] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %88, %100 : i64, !llvm.ptr
    %101 = llvm.getelementptr %95[6] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %90, %101 : i64, !llvm.ptr
    %102 = llvm.getelementptr %95[7] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %92, %102 : i64, !llvm.ptr
    %103 = llvm.getelementptr %95[8] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %94, %103 : i64, !llvm.ptr
    %104 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %105 = llvm.getelementptr %104[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    llvm.call @cudaLaunchKernel(%3, %6, %4, %3, %7, %4, %95, %95, %2, %1, %4, %105, %14, %12, %13, %3) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %106 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %48, %106 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %106) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%28) : (i64) -> ()
    llvm.call @cudaFree(%35) : (i64) -> ()
    llvm.call @cudaFree(%49) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}


module attributes {gpu.container_module, "triton_gpu.compute-capability" = 70 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 32 : i32} {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.mlir.global private constant @main_kernel0_str("main_kernel0") {addr_space = 0 : i32}
  llvm.mlir.global internal @ptx("//\0A// Generated by LLVM NVPTX Back-End\0A//\0A\0A.version 6.0\0A.target sm_70\0A.address_size 64\0A\0A\09// .globl\09main_kernel0\0A.extern .shared .align 1 .b8 global_smem[];\0A\0A.visible .entry main_kernel0(\0A\09.param .u32 main_kernel0_param_0,\0A\09.param .u32 main_kernel0_param_1,\0A\09.param .u32 main_kernel0_param_2,\0A\09.param .u32 main_kernel0_param_3,\0A\09.param .u64 main_kernel0_param_4,\0A\09.param .u32 main_kernel0_param_5,\0A\09.param .u64 main_kernel0_param_6,\0A\09.param .u32 main_kernel0_param_7,\0A\09.param .u64 main_kernel0_param_8\0A)\0A.maxntid 128, 1, 1\0A{\0A\09.reg .pred \09%p<12>;\0A\09.reg .b32 \09%r<73>;\0A\09.reg .b64 \09%rd<22>;\0A\09.reg .f64 \09%fd<7>;\0A\0A\09ld.param.u64 \09%rd3, [main_kernel0_param_8];\0A\09ld.param.u64 \09%rd2, [main_kernel0_param_6];\0A\09ld.param.u64 \09%rd1, [main_kernel0_param_4];\0A\09ld.param.u32 \09%r37, [main_kernel0_param_2];\0A\09ld.param.u32 \09%r43, [main_kernel0_param_0];\0A\09ld.param.u32 \09%r44, [main_kernel0_param_1];\0A\09mov.u32 \09%r45, %tid.x;\0A\09and.b32  \09%r46, %r45, 31;\0A\09ld.param.u32 \09%r47, [main_kernel0_param_3];\0A\09bfe.u32 \09%r48, %r45, 5, 2;\0A\09// begin inline asm\0A\09mov.u32 %r38, %nctaid.y;\0A\09// end inline asm\0A\09ld.param.u32 \09%r49, [main_kernel0_param_5];\0A\09// begin inline asm\0A\09mov.u32 %r39, %nctaid.x;\0A\09// end inline asm\0A\09ld.param.u32 \09%r50, [main_kernel0_param_7];\0A\09// begin inline asm\0A\09mov.u32 %r40, %ctaid.y;\0A\09// end inline asm\0A\09// begin inline asm\0A\09mov.u32 %r41, %ctaid.x;\0A\09// end inline asm\0A\09sub.s32 \09%r3, %r43, %r40;\0A\09sub.s32 \09%r4, %r44, %r41;\0A\09shl.b32 \09%r51, %r41, 5;\0A\09shl.b32 \09%r52, %r40, 3;\0A\09or.b32  \09%r5, %r48, %r52;\0A\09or.b32  \09%r53, %r5, 4;\0A\09mul.lo.s32 \09%r69, %r50, %r53;\0A\09mul.lo.s32 \09%r54, %r38, %r50;\0A\09shl.b32 \09%r7, %r54, 3;\0A\09or.b32  \09%r8, %r51, %r46;\0A\09shl.b32 \09%r9, %r39, 5;\0A\09mul.lo.s32 \09%r68, %r50, %r5;\0A\09mul.lo.s32 \09%r67, %r49, %r53;\0A\09mul.lo.s32 \09%r55, %r38, %r49;\0A\09shl.b32 \09%r12, %r55, 3;\0A\09mul.lo.s32 \09%r66, %r49, %r5;\0A\09mul.lo.s32 \09%r65, %r47, %r53;\0A\09mul.lo.s32 \09%r56, %r38, %r47;\0A\09shl.b32 \09%r15, %r56, 3;\0A\09mul.lo.s32 \09%r64, %r47, %r5;\0A\09mov.b32 \09%r70, 0;\0A\09bra.uni \09$L__BB0_1;\0A$L__BB0_5:\0A\09add.s32 \09%r70, %r70, %r38;\0A\09add.s32 \09%r69, %r69, %r7;\0A\09add.s32 \09%r68, %r68, %r7;\0A\09add.s32 \09%r67, %r67, %r12;\0A\09add.s32 \09%r66, %r66, %r12;\0A\09add.s32 \09%r65, %r65, %r15;\0A\09add.s32 \09%r64, %r64, %r15;\0A$L__BB0_1:\0A\09setp.ge.s32 \09%p1, %r70, %r3;\0A\09@%p1 bra \09$L__BB0_6;\0A\09mad.lo.s32 \09%r24, %r70, 8, %r5;\0A\09add.s32 \09%r25, %r24, 4;\0A\09mov.b32 \09%r72, 0;\0A\09setp.lt.s32 \09%p9, %r25, %r37;\0A\09setp.lt.s32 \09%p10, %r24, %r37;\0A\09mov.u32 \09%r71, %r8;\0A$L__BB0_3:\0A\09setp.ge.s32 \09%p2, %r72, %r4;\0A\09@%p2 bra \09$L__BB0_5;\0A\09setp.lt.s32 \09%p11, %r71, %r37;\0A\09and.pred  \09%p3, %p11, %p10;\0A\09and.pred  \09%p4, %p11, %p9;\0A\09add.s32 \09%r58, %r64, %r71;\0A\09add.s32 \09%r59, %r65, %r71;\0A\09mul.wide.s32 \09%rd16, %r58, 8;\0A\09add.s64 \09%rd5, %rd1, %rd16;\0A\09mul.wide.s32 \09%rd17, %r59, 8;\0A\09add.s64 \09%rd7, %rd1, %rd17;\0A\09// begin inline asm\0A\09mov.u64 %rd4, 0x0;\0A\09@%p3 ld.global.b64 { %rd4 }, [ %rd5 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd1, %rd4;\0A\09// begin inline asm\0A\09mov.u64 %rd6, 0x0;\0A\09@%p4 ld.global.b64 { %rd6 }, [ %rd7 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd2, %rd6;\0A\09add.s32 \09%r60, %r66, %r71;\0A\09add.s32 \09%r61, %r67, %r71;\0A\09mul.wide.s32 \09%rd18, %r60, 8;\0A\09add.s64 \09%rd9, %rd2, %rd18;\0A\09mul.wide.s32 \09%rd19, %r61, 8;\0A\09add.s64 \09%rd11, %rd2, %rd19;\0A\09// begin inline asm\0A\09mov.u64 %rd8, 0x0;\0A\09@%p3 ld.global.b64 { %rd8 }, [ %rd9 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd3, %rd8;\0A\09// begin inline asm\0A\09mov.u64 %rd10, 0x0;\0A\09@%p4 ld.global.b64 { %rd10 }, [ %rd11 + 0 ];\0A\09// end inline asm\0A\09mov.b64 \09%fd4, %rd10;\0A\09add.rn.f64 \09%fd5, %fd1, %fd3;\0A\09add.rn.f64 \09%fd6, %fd2, %fd4;\0A\09add.s32 \09%r62, %r68, %r71;\0A\09add.s32 \09%r63, %r69, %r71;\0A\09mul.wide.s32 \09%rd20, %r62, 8;\0A\09add.s64 \09%rd13, %rd3, %rd20;\0A\09mul.wide.s32 \09%rd21, %r63, 8;\0A\09add.s64 \09%rd15, %rd3, %rd21;\0A\09mov.b64 \09%rd12, %fd5;\0A\09// begin inline asm\0A\09@%p3 st.global.b64 [ %rd13 + 0 ], { %rd12 };\0A\09// end inline asm\0A\09mov.b64 \09%rd14, %fd6;\0A\09// begin inline asm\0A\09@%p4 st.global.b64 [ %rd15 + 0 ], { %rd14 };\0A\09// end inline asm\0A\09add.s32 \09%r72, %r72, %r39;\0A\09add.s32 \09%r71, %r71, %r9;\0A\09bra.uni \09$L__BB0_3;\0A$L__BB0_6:\0A\09ret;\0A\0A}\0A") {addr_space = 0 : i32, alignment = 32 : i64}
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(9 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(1024 : index) : i64
    %6 = llvm.mlir.constant(128 : index) : i64
    %7 = llvm.mlir.constant(8 : index) : i64
    %8 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %9 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %10 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %11 = llvm.mlir.constant(1048576 : index) : i64
    %12 = llvm.mlir.constant(0 : i32) : i32
    %13 = llvm.mlir.constant(4 : index) : i64
    %14 = llvm.mlir.constant(12 : index) : i64
    %15 = llvm.mlir.addressof @ptx : !llvm.ptr
    llvm.call @cudaSetModuleImage(%15) : (!llvm.ptr) -> ()
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.add %18, %3  : i64
    %20 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.sub %3, %4  : i64
    %23 = llvm.add %21, %22  : i64
    %24 = llvm.urem %23, %3  : i64
    %25 = llvm.sub %23, %24  : i64
    %26 = llvm.inttoptr %25 : i64 to !llvm.ptr
    %27 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %28 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %29 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %30 = llvm.ptrtoint %29 : !llvm.ptr to i64
    %31 = llvm.add %30, %22  : i64
    %32 = llvm.urem %31, %3  : i64
    %33 = llvm.sub %31, %32  : i64
    %34 = llvm.inttoptr %33 : i64 to !llvm.ptr
    %35 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    %36 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %37 = llvm.ptrtoint %36 : !llvm.ptr to i64
    %38 = llvm.add %37, %22  : i64
    %39 = llvm.urem %38, %3  : i64
    %40 = llvm.sub %38, %39  : i64
    %41 = llvm.inttoptr %40 : i64 to !llvm.ptr
    %42 = llvm.insertvalue %36, %27[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %41, %42[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %2, %43[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %5, %44[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.insertvalue %5, %45[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %47 = llvm.insertvalue %5, %46[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.insertvalue %4, %47[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.call @cudaMallocF64(%11) : (i64) -> i64
    llvm.br ^bb1(%2 : i64)
  ^bb1(%50: i64):  // 2 preds: ^bb0, ^bb4
    %51 = llvm.icmp "slt" %50, %5 : i64
    llvm.cond_br %51, ^bb2(%2 : i64), ^bb5(%2 : i64)
  ^bb2(%52: i64):  // 2 preds: ^bb1, ^bb3
    %53 = llvm.icmp "slt" %52, %5 : i64
    llvm.cond_br %53, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %54 = llvm.mul %50, %5  : i64
    %55 = llvm.add %54, %52  : i64
    %56 = llvm.getelementptr %26[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %56 : f64, !llvm.ptr
    %57 = llvm.add %52, %4  : i64
    llvm.br ^bb2(%57 : i64)
  ^bb4:  // pred: ^bb2
    %58 = llvm.add %50, %4  : i64
    llvm.br ^bb1(%58 : i64)
  ^bb5(%59: i64):  // 2 preds: ^bb1, ^bb8
    %60 = llvm.icmp "slt" %59, %5 : i64
    llvm.cond_br %60, ^bb6(%2 : i64), ^bb9(%2 : i64)
  ^bb6(%61: i64):  // 2 preds: ^bb5, ^bb7
    %62 = llvm.icmp "slt" %61, %5 : i64
    llvm.cond_br %62, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %63 = llvm.mul %59, %5  : i64
    %64 = llvm.add %63, %61  : i64
    %65 = llvm.getelementptr %34[%64] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %65 : f64, !llvm.ptr
    %66 = llvm.add %61, %4  : i64
    llvm.br ^bb6(%66 : i64)
  ^bb8:  // pred: ^bb6
    %67 = llvm.add %59, %4  : i64
    llvm.br ^bb5(%67 : i64)
  ^bb9(%68: i64):  // 2 preds: ^bb5, ^bb12
    %69 = llvm.icmp "slt" %68, %5 : i64
    llvm.cond_br %69, ^bb10(%2 : i64), ^bb13
  ^bb10(%70: i64):  // 2 preds: ^bb9, ^bb11
    %71 = llvm.icmp "slt" %70, %5 : i64
    llvm.cond_br %71, ^bb11, ^bb12
  ^bb11:  // pred: ^bb10
    %72 = llvm.mul %68, %5  : i64
    %73 = llvm.add %72, %70  : i64
    %74 = llvm.getelementptr %41[%73] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %8, %74 : f64, !llvm.ptr
    %75 = llvm.add %70, %4  : i64
    llvm.br ^bb10(%75 : i64)
  ^bb12:  // pred: ^bb10
    %76 = llvm.add %68, %4  : i64
    llvm.br ^bb9(%76 : i64)
  ^bb13:  // pred: ^bb9
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %2) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %77 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %6, %77 : i64, !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %3, %79 : i64, !llvm.ptr
    %80 = llvm.ptrtoint %79 : !llvm.ptr to i64
    %81 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %81 : i64, !llvm.ptr
    %82 = llvm.ptrtoint %81 : !llvm.ptr to i64
    %83 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %83 : i64, !llvm.ptr
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %85 : i64, !llvm.ptr
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %87 : i64, !llvm.ptr
    %88 = llvm.ptrtoint %87 : !llvm.ptr to i64
    %89 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %35, %89 : i64, !llvm.ptr
    %90 = llvm.ptrtoint %89 : !llvm.ptr to i64
    %91 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %91 : i64, !llvm.ptr
    %92 = llvm.ptrtoint %91 : !llvm.ptr to i64
    %93 = llvm.alloca %4 x i64 : (i64) -> !llvm.ptr
    llvm.store %49, %93 : i64, !llvm.ptr
    %94 = llvm.ptrtoint %93 : !llvm.ptr to i64
    %95 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %78, %95 : i64, !llvm.ptr
    %96 = llvm.getelementptr %95[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %80, %96 : i64, !llvm.ptr
    %97 = llvm.getelementptr %95[2] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %82, %97 : i64, !llvm.ptr
    %98 = llvm.getelementptr %95[3] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %84, %98 : i64, !llvm.ptr
    %99 = llvm.getelementptr %95[4] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %86, %99 : i64, !llvm.ptr
    %100 = llvm.getelementptr %95[5] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %88, %100 : i64, !llvm.ptr
    %101 = llvm.getelementptr %95[6] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %90, %101 : i64, !llvm.ptr
    %102 = llvm.getelementptr %95[7] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %92, %102 : i64, !llvm.ptr
    %103 = llvm.getelementptr %95[8] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %94, %103 : i64, !llvm.ptr
    %104 = llvm.mlir.addressof @main_kernel0_str : !llvm.ptr
    %105 = llvm.getelementptr %104[0, 0] : (!llvm.ptr) -> !llvm.ptr, !llvm.array<12 x i8>
    llvm.call @cudaLaunchKernel(%3, %6, %4, %3, %7, %4, %95, %95, %2, %1, %4, %105, %14, %12, %13, %3) : (i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%49, %36, %41, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%35, %29, %34, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    llvm.call @cudaMemcpyF64(%28, %20, %26, %2, %11, %4, %4) : (i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) -> ()
    %106 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %48, %106 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %106) : (i64, !llvm.ptr) -> ()
    llvm.call @cudaFree(%28) : (i64) -> ()
    llvm.call @cudaFree(%35) : (i64) -> ()
    llvm.call @cudaFree(%49) : (i64) -> ()
    llvm.return
  }
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocI64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF32(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMallocF64(i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyI64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyIndex(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF32(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaMemcpyF64(i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaLaunchKernel(i64, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, !llvm.ptr, i64, i32, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @cudaSetModuleImage(!llvm.ptr) attributes {sym_visibility = "private"}
  llvm.func @cudaFree(i64) attributes {sym_visibility = "private"}
}
