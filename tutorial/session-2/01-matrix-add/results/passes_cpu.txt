// -----// IR Dump After {anonymous}::FuncOpLoweringPass () //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @main() {
    %0 = "ta.index_label"() : () -> !ta.indexlabel
    %1 = "ta.index_label"() : () -> !ta.indexlabel
    %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
    "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
    %5 = "ta.add"(%2, %3, %0, %1, %0, %1, %0, %1) <{MaskType = "none", formats = ["Dense", "Dense", "Dense"], indexing_maps = [#map, #map, #map], semiring = "noop_plusxy"}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel) -> tensor<1024x1024xf64>
    "ta.set_op"(%5, %4) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> ()
    "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
    return
  }
}


// -----// IR Dump After {anonymous}::TensorAlgebraCheckImplicitTensorDeclPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "ta.add"(%2, %3, %0, %1, %0, %1, %0, %1) <{MaskType = "none", formats = ["Dense", "Dense", "Dense"], indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], semiring = "noop_plusxy"}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel, !ta.indexlabel) -> tensor<1024x1024xf64>
  "ta.set_op"(%5, %4) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> ()
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerTensorAlgebraToIndexTreePass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %2 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %3 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  %4 = "ta.dense_tensor_decl"() <{format = "Dense"}> : () -> tensor<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DenseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  "ta.fill"(%2) <{value = 2.200000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%3) <{value = 3.400000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  "ta.fill"(%4) <{value = 0.000000e+00 : f64}> : (tensor<1024x1024xf64>) -> ()
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::TensorFillLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DenseTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseTempOutputTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::SparseOutputTensorDeclLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::DimOpLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::TensorFillLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::PCToLoopsLoweringPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerTensorAlgebraToSCFPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %5 = "it.ComputeRHS"(%2, %3) <{allBlocks = [["UNK", "UNK"], ["UNK", "UNK"]], allFormats = [["D", "D"], ["D", "D"]], allPerms = [[0, 1], [0, 1]]}> : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<*xf64>
  %6 = "it.ComputeLHS"(%4) <{allBlocks = [["UNK", "UNK"]], allFormats = [["D", "D"]], allPerms = [[0, 1]]}> : (tensor<1024x1024xf64>) -> tensor<*xf64>
  %7 = "it.Compute"(%5, %6) <{MaskType = "none", comp_worksp_opt = false, semiring = "noop_plusxy"}> : (tensor<*xf64>, tensor<*xf64>) -> i64
  %8 = "it.Indices"(%7) <{indices = [1], iterator_type = "default"}> : (i64) -> i64
  %9 = "it.Indices"(%8) <{indices = [0], iterator_type = "parallel"}> : (i64) -> i64
  %10 = "it.itree"(%9) : (i64) -> i64
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LowerIndexTreeToSCFPass () //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %3 = bufferization.to_tensor %alloc_0 : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %4 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = tensor.dim %2, %c0_4 : tensor<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = tensor.dim %2, %c1_7 : tensor<1024x1024xf64>
    scf.for %arg1 = %c0_5 to %dim_8 step %c1_6 {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %5 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %6 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %7 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %8 = arith.addf %5, %6 : f64
      memref.store %8, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  "ta.print"(%4) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main() {
  %0 = "ta.index_label"() : () -> !ta.indexlabel
  %1 = "ta.index_label"() : () -> !ta.indexlabel
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %2 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.for %arg1 = %c0_5 to %dim_8 step %c1_6 {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %3 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %5 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %6 = arith.addf %3, %4 : f64
      memref.store %6, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  "ta.print"(%2) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::STCRemoveDeadOpsPass () //----- //
func.func @main() {
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.for %arg1 = %c0_5 to %dim_8 step %c1_6 {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = arith.addf %1, %2 : f64
      memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  "ta.print"(%0) : (tensor<1024x1024xf64>) -> ()
  return
}

// -----// IR Dump After {anonymous}::LateLoweringPass () //----- //
func.func @main() {
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
  %cst = arith.constant 2.200000e+00 : f64
  linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
  %cst_2 = arith.constant 3.400000e+00 : f64
  linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
  %cst_3 = arith.constant 0.000000e+00 : f64
  linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
  scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
    scf.for %arg1 = %c0_5 to %dim_8 step %c1_6 {
      %cst_9 = arith.constant 0.000000e+00 : f64
      %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
      %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      %4 = arith.addf %1, %2 : f64
      memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  %cast = memref.cast %alloc_1 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main() {
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_0 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_1 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %0 = bufferization.to_tensor %alloc_1 : memref<1024x1024xf64>
    %cst = arith.constant 2.200000e+00 : f64
    linalg.fill ins(%cst : f64) outs(%alloc : memref<1024x1024xf64>)
    %cst_2 = arith.constant 3.400000e+00 : f64
    linalg.fill ins(%cst_2 : f64) outs(%alloc_0 : memref<1024x1024xf64>)
    %cst_3 = arith.constant 0.000000e+00 : f64
    linalg.fill ins(%cst_3 : f64) outs(%alloc_1 : memref<1024x1024xf64>)
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c0_4 = arith.constant 0 : index
    %dim = memref.dim %alloc, %c0_4 : memref<1024x1024xf64>
    scf.parallel (%arg0) = (%c0) to (%dim) step (%c1) {
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %dim_8 = memref.dim %alloc, %c1_7 : memref<1024x1024xf64>
      scf.for %arg1 = %c0_5 to %dim_8 step %c1_6 {
        %cst_9 = arith.constant 0.000000e+00 : f64
        %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
        %2 = memref.load %alloc_0[%arg0, %arg1] : memref<1024x1024xf64>
        %3 = memref.load %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
        %4 = arith.addf %1, %2 : f64
        memref.store %4, %alloc_1[%arg0, %arg1] : memref<1024x1024xf64>
      }
      scf.reduce 
    }
    %cast = memref.cast %alloc_1 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.parallel (%arg0) = (%c0) to (%c1024) step (%c1) {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
        %1 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
        %2 = arith.addf %0, %1 : f64
        memref.store %2, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
      scf.reduce 
    }
    %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @comet_sort_index(memref<*xindex>, index, index)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @comet_sort_index(memref<*xindex>, index, index)

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) //----- //
func.func @main() {
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 2.200000e+00 : f64
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.parallel (%arg0) = (%c0) to (%c1024) step (%c1) {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %0 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %1 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = arith.addf %0, %1 : f64
      memref.store %2, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @main() {
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 2.200000e+00 : f64
  %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.for %arg0 = %c0 to %c1024 step %c1 {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
    }
  }
  scf.parallel (%arg0) = (%c0) to (%c1024) step (%c1) {
    scf.for %arg1 = %c0 to %c1024 step %c1 {
      %0 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      %1 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      %2 = arith.addf %0, %1 : f64
      memref.store %2, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
    }
    scf.reduce 
  }
  %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
  call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.parallel (%arg0) = (%c0) to (%c1024) step (%c1) {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %0 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
        %1 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
        %2 = arith.addf %0, %1 : f64
        memref.store %2, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
      scf.reduce 
    }
    %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertSCFToOpenMPPass (convert-scf-to-openmp) //----- //
module {
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %alloc = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_2 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    %alloc_3 = memref.alloc() {alignment = 32 : i64} : memref<1024x1024xf64>
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_1, %alloc[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst_0, %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        memref.store %cst, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
      }
    }
    %0 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        memref.alloca_scope  {
          scf.for %arg1 = %c0 to %c1024 step %c1 {
            %1 = memref.load %alloc[%arg0, %arg1] : memref<1024x1024xf64>
            %2 = memref.load %alloc_2[%arg0, %arg1] : memref<1024x1024xf64>
            %3 = arith.addf %1, %2 : f64
            memref.store %3, %alloc_3[%arg0, %arg1] : memref<1024x1024xf64>
          }
        }
        omp.yield
      }
      omp.terminator
    }
    %cast = memref.cast %alloc_3 : memref<1024x1024xf64> to memref<*xf64>
    call @comet_print_memref_f64(%cast) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %0 = llvm.mlir.constant(1024 : index) : i64
    %1 = llvm.mlir.constant(1024 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %3 = llvm.mlir.constant(1048576 : index) : i64
    %4 = llvm.mlir.zero : !llvm.ptr
    %5 = llvm.getelementptr %4[%3] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %6 = llvm.ptrtoint %5 : !llvm.ptr to i64
    %7 = llvm.mlir.constant(32 : index) : i64
    %8 = llvm.add %6, %7  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.mlir.constant(1 : index) : i64
    %12 = llvm.sub %7, %11  : i64
    %13 = llvm.add %10, %12  : i64
    %14 = llvm.urem %13, %7  : i64
    %15 = llvm.sub %13, %14  : i64
    %16 = llvm.inttoptr %15 : i64 to !llvm.ptr
    %17 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %18 = llvm.insertvalue %9, %17[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %19 = llvm.insertvalue %16, %18[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %20 = llvm.mlir.constant(0 : index) : i64
    %21 = llvm.insertvalue %20, %19[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %22 = llvm.insertvalue %0, %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %23 = llvm.insertvalue %1, %22[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %24 = llvm.insertvalue %1, %23[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %25 = llvm.insertvalue %2, %24[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %26 = llvm.mlir.constant(1024 : index) : i64
    %27 = llvm.mlir.constant(1024 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(1048576 : index) : i64
    %30 = llvm.mlir.zero : !llvm.ptr
    %31 = llvm.getelementptr %30[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.mlir.constant(32 : index) : i64
    %34 = llvm.add %32, %33  : i64
    %35 = llvm.call @malloc(%34) : (i64) -> !llvm.ptr
    %36 = llvm.ptrtoint %35 : !llvm.ptr to i64
    %37 = llvm.mlir.constant(1 : index) : i64
    %38 = llvm.sub %33, %37  : i64
    %39 = llvm.add %36, %38  : i64
    %40 = llvm.urem %39, %33  : i64
    %41 = llvm.sub %39, %40  : i64
    %42 = llvm.inttoptr %41 : i64 to !llvm.ptr
    %43 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %44 = llvm.insertvalue %35, %43[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %42, %44[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.mlir.constant(0 : index) : i64
    %47 = llvm.insertvalue %46, %45[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.insertvalue %26, %47[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %27, %48[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %27, %49[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %28, %50[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.mlir.constant(1024 : index) : i64
    %53 = llvm.mlir.constant(1024 : index) : i64
    %54 = llvm.mlir.constant(1 : index) : i64
    %55 = llvm.mlir.constant(1048576 : index) : i64
    %56 = llvm.mlir.zero : !llvm.ptr
    %57 = llvm.getelementptr %56[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %58 = llvm.ptrtoint %57 : !llvm.ptr to i64
    %59 = llvm.mlir.constant(32 : index) : i64
    %60 = llvm.add %58, %59  : i64
    %61 = llvm.call @malloc(%60) : (i64) -> !llvm.ptr
    %62 = llvm.ptrtoint %61 : !llvm.ptr to i64
    %63 = llvm.mlir.constant(1 : index) : i64
    %64 = llvm.sub %59, %63  : i64
    %65 = llvm.add %62, %64  : i64
    %66 = llvm.urem %65, %59  : i64
    %67 = llvm.sub %65, %66  : i64
    %68 = llvm.inttoptr %67 : i64 to !llvm.ptr
    %69 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %70 = llvm.insertvalue %61, %69[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %68, %70[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.mlir.constant(0 : index) : i64
    %73 = llvm.insertvalue %72, %71[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %74 = llvm.insertvalue %52, %73[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.insertvalue %53, %74[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %76 = llvm.insertvalue %53, %75[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.insertvalue %54, %76[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      %86 = builtin.unrealized_conversion_cast %arg0 : index to i64
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %87 = builtin.unrealized_conversion_cast %arg1 : index to i64
        %88 = llvm.extractvalue %25[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %89 = llvm.mlir.constant(1024 : index) : i64
        %90 = llvm.mul %86, %89  : i64
        %91 = llvm.add %90, %87  : i64
        %92 = llvm.getelementptr %88[%91] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst_1, %92 : f64, !llvm.ptr
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      %86 = builtin.unrealized_conversion_cast %arg0 : index to i64
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %87 = builtin.unrealized_conversion_cast %arg1 : index to i64
        %88 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %89 = llvm.mlir.constant(1024 : index) : i64
        %90 = llvm.mul %86, %89  : i64
        %91 = llvm.add %90, %87  : i64
        %92 = llvm.getelementptr %88[%91] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst_0, %92 : f64, !llvm.ptr
      }
    }
    scf.for %arg0 = %c0 to %c1024 step %c1 {
      %86 = builtin.unrealized_conversion_cast %arg0 : index to i64
      scf.for %arg1 = %c0 to %c1024 step %c1 {
        %87 = builtin.unrealized_conversion_cast %arg1 : index to i64
        %88 = llvm.extractvalue %77[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %89 = llvm.mlir.constant(1024 : index) : i64
        %90 = llvm.mul %86, %89  : i64
        %91 = llvm.add %90, %87  : i64
        %92 = llvm.getelementptr %88[%91] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %cst, %92 : f64, !llvm.ptr
      }
    }
    %78 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %86 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %87 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        scf.for %arg1 = %c0 to %c1024 step %c1 {
          %88 = builtin.unrealized_conversion_cast %arg1 : index to i64
          %89 = llvm.extractvalue %25[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %90 = llvm.mlir.constant(1024 : index) : i64
          %91 = llvm.mul %86, %90  : i64
          %92 = llvm.add %91, %88  : i64
          %93 = llvm.getelementptr %89[%92] : (!llvm.ptr, i64) -> !llvm.ptr, f64
          %94 = llvm.load %93 : !llvm.ptr -> f64
          %95 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %96 = llvm.mlir.constant(1024 : index) : i64
          %97 = llvm.mul %86, %96  : i64
          %98 = llvm.add %97, %88  : i64
          %99 = llvm.getelementptr %95[%98] : (!llvm.ptr, i64) -> !llvm.ptr, f64
          %100 = llvm.load %99 : !llvm.ptr -> f64
          %101 = arith.addf %94, %100 : f64
          %102 = llvm.extractvalue %77[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %103 = llvm.mlir.constant(1024 : index) : i64
          %104 = llvm.mul %86, %103  : i64
          %105 = llvm.add %104, %88  : i64
          %106 = llvm.getelementptr %102[%105] : (!llvm.ptr, i64) -> !llvm.ptr, f64
          llvm.store %101, %106 : f64, !llvm.ptr
        }
        llvm.intr.stackrestore %87 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %79 = llvm.mlir.constant(1 : index) : i64
    %80 = llvm.alloca %79 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %77, %80 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %81 = llvm.mlir.constant(2 : index) : i64
    %82 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %83 = llvm.insertvalue %81, %82[0] : !llvm.struct<(i64, ptr)> 
    %84 = llvm.insertvalue %80, %83[1] : !llvm.struct<(i64, ptr)> 
    %85 = builtin.unrealized_conversion_cast %84 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%85) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %0 = llvm.mlir.constant(1024 : index) : i64
    %1 = llvm.mlir.constant(1024 : index) : i64
    %2 = llvm.mlir.constant(1 : index) : i64
    %3 = llvm.mlir.constant(1048576 : index) : i64
    %4 = llvm.mlir.zero : !llvm.ptr
    %5 = llvm.getelementptr %4[%3] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %6 = llvm.ptrtoint %5 : !llvm.ptr to i64
    %7 = llvm.mlir.constant(32 : index) : i64
    %8 = llvm.add %6, %7  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.mlir.constant(1 : index) : i64
    %12 = llvm.sub %7, %11  : i64
    %13 = llvm.add %10, %12  : i64
    %14 = llvm.urem %13, %7  : i64
    %15 = llvm.sub %13, %14  : i64
    %16 = llvm.inttoptr %15 : i64 to !llvm.ptr
    %17 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %18 = llvm.insertvalue %9, %17[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %19 = llvm.insertvalue %16, %18[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %20 = llvm.mlir.constant(0 : index) : i64
    %21 = llvm.insertvalue %20, %19[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %22 = llvm.insertvalue %0, %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %23 = llvm.insertvalue %1, %22[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %24 = llvm.insertvalue %1, %23[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %25 = llvm.insertvalue %2, %24[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %26 = llvm.mlir.constant(1024 : index) : i64
    %27 = llvm.mlir.constant(1024 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(1048576 : index) : i64
    %30 = llvm.mlir.zero : !llvm.ptr
    %31 = llvm.getelementptr %30[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.mlir.constant(32 : index) : i64
    %34 = llvm.add %32, %33  : i64
    %35 = llvm.call @malloc(%34) : (i64) -> !llvm.ptr
    %36 = llvm.ptrtoint %35 : !llvm.ptr to i64
    %37 = llvm.mlir.constant(1 : index) : i64
    %38 = llvm.sub %33, %37  : i64
    %39 = llvm.add %36, %38  : i64
    %40 = llvm.urem %39, %33  : i64
    %41 = llvm.sub %39, %40  : i64
    %42 = llvm.inttoptr %41 : i64 to !llvm.ptr
    %43 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %44 = llvm.insertvalue %35, %43[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %42, %44[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.mlir.constant(0 : index) : i64
    %47 = llvm.insertvalue %46, %45[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.insertvalue %26, %47[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %49 = llvm.insertvalue %27, %48[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %27, %49[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %28, %50[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.mlir.constant(1024 : index) : i64
    %53 = llvm.mlir.constant(1024 : index) : i64
    %54 = llvm.mlir.constant(1 : index) : i64
    %55 = llvm.mlir.constant(1048576 : index) : i64
    %56 = llvm.mlir.zero : !llvm.ptr
    %57 = llvm.getelementptr %56[%55] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    %58 = llvm.ptrtoint %57 : !llvm.ptr to i64
    %59 = llvm.mlir.constant(32 : index) : i64
    %60 = llvm.add %58, %59  : i64
    %61 = llvm.call @malloc(%60) : (i64) -> !llvm.ptr
    %62 = llvm.ptrtoint %61 : !llvm.ptr to i64
    %63 = llvm.mlir.constant(1 : index) : i64
    %64 = llvm.sub %59, %63  : i64
    %65 = llvm.add %62, %64  : i64
    %66 = llvm.urem %65, %59  : i64
    %67 = llvm.sub %65, %66  : i64
    %68 = llvm.inttoptr %67 : i64 to !llvm.ptr
    %69 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %70 = llvm.insertvalue %61, %69[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %68, %70[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.mlir.constant(0 : index) : i64
    %73 = llvm.insertvalue %72, %71[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %74 = llvm.insertvalue %52, %73[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.insertvalue %53, %74[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %76 = llvm.insertvalue %53, %75[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.insertvalue %54, %76[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%78: index):  // 2 preds: ^bb0, ^bb5
    %79 = arith.cmpi slt, %78, %c1024 : index
    cf.cond_br %79, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %80 = builtin.unrealized_conversion_cast %78 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%81: index):  // 2 preds: ^bb2, ^bb4
    %82 = arith.cmpi slt, %81, %c1024 : index
    cf.cond_br %82, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %83 = builtin.unrealized_conversion_cast %81 : index to i64
    %84 = llvm.extractvalue %25[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %85 = llvm.mlir.constant(1024 : index) : i64
    %86 = llvm.mul %80, %85  : i64
    %87 = llvm.add %86, %83  : i64
    %88 = llvm.getelementptr %84[%87] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %88 : f64, !llvm.ptr
    %89 = arith.addi %81, %c1 : index
    cf.br ^bb3(%89 : index)
  ^bb5:  // pred: ^bb3
    %90 = arith.addi %78, %c1 : index
    cf.br ^bb1(%90 : index)
  ^bb6:  // pred: ^bb1
    cf.br ^bb7(%c0 : index)
  ^bb7(%91: index):  // 2 preds: ^bb6, ^bb11
    %92 = arith.cmpi slt, %91, %c1024 : index
    cf.cond_br %92, ^bb8, ^bb12
  ^bb8:  // pred: ^bb7
    %93 = builtin.unrealized_conversion_cast %91 : index to i64
    cf.br ^bb9(%c0 : index)
  ^bb9(%94: index):  // 2 preds: ^bb8, ^bb10
    %95 = arith.cmpi slt, %94, %c1024 : index
    cf.cond_br %95, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %96 = builtin.unrealized_conversion_cast %94 : index to i64
    %97 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.mlir.constant(1024 : index) : i64
    %99 = llvm.mul %93, %98  : i64
    %100 = llvm.add %99, %96  : i64
    %101 = llvm.getelementptr %97[%100] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %101 : f64, !llvm.ptr
    %102 = arith.addi %94, %c1 : index
    cf.br ^bb9(%102 : index)
  ^bb11:  // pred: ^bb9
    %103 = arith.addi %91, %c1 : index
    cf.br ^bb7(%103 : index)
  ^bb12:  // pred: ^bb7
    cf.br ^bb13(%c0 : index)
  ^bb13(%104: index):  // 2 preds: ^bb12, ^bb17
    %105 = arith.cmpi slt, %104, %c1024 : index
    cf.cond_br %105, ^bb14, ^bb18
  ^bb14:  // pred: ^bb13
    %106 = builtin.unrealized_conversion_cast %104 : index to i64
    cf.br ^bb15(%c0 : index)
  ^bb15(%107: index):  // 2 preds: ^bb14, ^bb16
    %108 = arith.cmpi slt, %107, %c1024 : index
    cf.cond_br %108, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %109 = builtin.unrealized_conversion_cast %107 : index to i64
    %110 = llvm.extractvalue %77[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %111 = llvm.mlir.constant(1024 : index) : i64
    %112 = llvm.mul %106, %111  : i64
    %113 = llvm.add %112, %109  : i64
    %114 = llvm.getelementptr %110[%113] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %114 : f64, !llvm.ptr
    %115 = arith.addi %107, %c1 : index
    cf.br ^bb15(%115 : index)
  ^bb17:  // pred: ^bb15
    %116 = arith.addi %104, %c1 : index
    cf.br ^bb13(%116 : index)
  ^bb18:  // pred: ^bb13
    %117 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %125 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %126 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%127: index):  // 2 preds: ^bb1, ^bb3
        %128 = arith.cmpi slt, %127, %c1024 : index
        cf.cond_br %128, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %129 = builtin.unrealized_conversion_cast %127 : index to i64
        %130 = llvm.extractvalue %25[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %131 = llvm.mlir.constant(1024 : index) : i64
        %132 = llvm.mul %125, %131  : i64
        %133 = llvm.add %132, %129  : i64
        %134 = llvm.getelementptr %130[%133] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %135 = llvm.load %134 : !llvm.ptr -> f64
        %136 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %137 = llvm.mlir.constant(1024 : index) : i64
        %138 = llvm.mul %125, %137  : i64
        %139 = llvm.add %138, %129  : i64
        %140 = llvm.getelementptr %136[%139] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %141 = llvm.load %140 : !llvm.ptr -> f64
        %142 = arith.addf %135, %141 : f64
        %143 = llvm.extractvalue %77[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %144 = llvm.mlir.constant(1024 : index) : i64
        %145 = llvm.mul %125, %144  : i64
        %146 = llvm.add %145, %129  : i64
        %147 = llvm.getelementptr %143[%146] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %142, %147 : f64, !llvm.ptr
        %148 = arith.addi %127, %c1 : index
        cf.br ^bb2(%148 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %126 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %118 = llvm.mlir.constant(1 : index) : i64
    %119 = llvm.alloca %118 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %77, %119 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %120 = llvm.mlir.constant(2 : index) : i64
    %121 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %122 = llvm.insertvalue %120, %121[0] : !llvm.struct<(i64, ptr)> 
    %123 = llvm.insertvalue %119, %122[1] : !llvm.struct<(i64, ptr)> 
    %124 = builtin.unrealized_conversion_cast %123 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%124) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.add %18, %2  : i64
    %20 = llvm.call @malloc(%19) : (i64) -> !llvm.ptr
    %21 = llvm.ptrtoint %20 : !llvm.ptr to i64
    %22 = llvm.sub %2, %4  : i64
    %23 = llvm.add %21, %22  : i64
    %24 = llvm.urem %23, %2  : i64
    %25 = llvm.sub %23, %24  : i64
    %26 = llvm.inttoptr %25 : i64 to !llvm.ptr
    %27 = llvm.mlir.zero : !llvm.ptr
    %28 = llvm.getelementptr %27[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %29 = llvm.ptrtoint %28 : !llvm.ptr to i64
    %30 = llvm.add %29, %2  : i64
    %31 = llvm.call @malloc(%30) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.sub %2, %4  : i64
    %34 = llvm.add %32, %33  : i64
    %35 = llvm.urem %34, %2  : i64
    %36 = llvm.sub %34, %35  : i64
    %37 = llvm.inttoptr %36 : i64 to !llvm.ptr
    %38 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %39 = llvm.insertvalue %31, %38[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %37, %39[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %1, %40[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.insertvalue %3, %41[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %3, %42[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %3, %43[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %45 = llvm.insertvalue %4, %44[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%46: index):  // 2 preds: ^bb0, ^bb5
    %47 = arith.cmpi slt, %46, %c1024 : index
    cf.cond_br %47, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %48 = builtin.unrealized_conversion_cast %46 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%49: index):  // 2 preds: ^bb2, ^bb4
    %50 = arith.cmpi slt, %49, %c1024 : index
    cf.cond_br %50, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %51 = builtin.unrealized_conversion_cast %49 : index to i64
    %52 = llvm.mul %48, %3  : i64
    %53 = llvm.add %52, %51  : i64
    %54 = llvm.getelementptr %15[%53] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %54 : f64, !llvm.ptr
    %55 = arith.addi %49, %c1 : index
    cf.br ^bb3(%55 : index)
  ^bb5:  // pred: ^bb3
    %56 = arith.addi %46, %c1 : index
    cf.br ^bb1(%56 : index)
  ^bb6(%57: index):  // 2 preds: ^bb1, ^bb10
    %58 = arith.cmpi slt, %57, %c1024 : index
    cf.cond_br %58, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %59 = builtin.unrealized_conversion_cast %57 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%60: index):  // 2 preds: ^bb7, ^bb9
    %61 = arith.cmpi slt, %60, %c1024 : index
    cf.cond_br %61, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %62 = builtin.unrealized_conversion_cast %60 : index to i64
    %63 = llvm.mul %59, %3  : i64
    %64 = llvm.add %63, %62  : i64
    %65 = llvm.getelementptr %26[%64] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %65 : f64, !llvm.ptr
    %66 = arith.addi %60, %c1 : index
    cf.br ^bb8(%66 : index)
  ^bb10:  // pred: ^bb8
    %67 = arith.addi %57, %c1 : index
    cf.br ^bb6(%67 : index)
  ^bb11(%68: index):  // 2 preds: ^bb6, ^bb15
    %69 = arith.cmpi slt, %68, %c1024 : index
    cf.cond_br %69, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %70 = builtin.unrealized_conversion_cast %68 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%71: index):  // 2 preds: ^bb12, ^bb14
    %72 = arith.cmpi slt, %71, %c1024 : index
    cf.cond_br %72, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %73 = builtin.unrealized_conversion_cast %71 : index to i64
    %74 = llvm.mul %70, %3  : i64
    %75 = llvm.add %74, %73  : i64
    %76 = llvm.getelementptr %37[%75] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %76 : f64, !llvm.ptr
    %77 = arith.addi %71, %c1 : index
    cf.br ^bb13(%77 : index)
  ^bb15:  // pred: ^bb13
    %78 = arith.addi %68, %c1 : index
    cf.br ^bb11(%78 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %84 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %85 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%86: index):  // 2 preds: ^bb1, ^bb3
        %87 = arith.cmpi slt, %86, %c1024 : index
        cf.cond_br %87, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %88 = builtin.unrealized_conversion_cast %86 : index to i64
        %89 = llvm.mul %84, %3  : i64
        %90 = llvm.add %89, %88  : i64
        %91 = llvm.getelementptr %15[%90] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %92 = llvm.load %91 : !llvm.ptr -> f64
        %93 = llvm.mul %84, %3  : i64
        %94 = llvm.add %93, %88  : i64
        %95 = llvm.getelementptr %26[%94] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %96 = llvm.load %95 : !llvm.ptr -> f64
        %97 = arith.addf %92, %96 : f64
        %98 = llvm.mul %84, %3  : i64
        %99 = llvm.add %98, %88  : i64
        %100 = llvm.getelementptr %37[%99] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %97, %100 : f64, !llvm.ptr
        %101 = arith.addi %86, %c1 : index
        cf.br ^bb2(%101 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %85 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %79 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %45, %79 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %80 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %81 = llvm.insertvalue %0, %80[0] : !llvm.struct<(i64, ptr)> 
    %82 = llvm.insertvalue %79, %81[1] : !llvm.struct<(i64, ptr)> 
    %83 = builtin.unrealized_conversion_cast %82 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%83) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After CSE (cse) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    cf.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    cf.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    cf.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    cf.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        cf.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertControlFlowToLLVMPass (convert-cf-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    llvm.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    llvm.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    llvm.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        llvm.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    llvm.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    llvm.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    llvm.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        llvm.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @comet_sort_index(memref<*xindex>, index, index)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func private @comet_print_memref_f64(memref<*xf64>)

// -----// IR Dump After ConvertMathToLLVMPass (convert-math-to-llvm) //----- //
func.func @main() {
  %0 = llvm.mlir.constant(2 : index) : i64
  %1 = llvm.mlir.constant(0 : index) : i64
  %2 = llvm.mlir.constant(32 : index) : i64
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f64
  %cst_0 = arith.constant 3.400000e+00 : f64
  %cst_1 = arith.constant 2.200000e+00 : f64
  %3 = llvm.mlir.constant(1024 : index) : i64
  %4 = llvm.mlir.constant(1 : index) : i64
  %5 = llvm.mlir.zero : !llvm.ptr
  %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
  %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
  %8 = llvm.add %7, %2  : i64
  %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
  %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
  %11 = llvm.sub %2, %4  : i64
  %12 = llvm.add %10, %11  : i64
  %13 = llvm.urem %12, %2  : i64
  %14 = llvm.sub %12, %13  : i64
  %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
  %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
  %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
  %18 = llvm.add %17, %11  : i64
  %19 = llvm.urem %18, %2  : i64
  %20 = llvm.sub %18, %19  : i64
  %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
  %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
  %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
  %24 = llvm.add %23, %11  : i64
  %25 = llvm.urem %24, %2  : i64
  %26 = llvm.sub %24, %25  : i64
  %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
  %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
  %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
  cf.br ^bb1(%c0 : index)
^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
  %37 = arith.cmpi slt, %36, %c1024 : index
  cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
^bb2:  // pred: ^bb1
  %38 = builtin.unrealized_conversion_cast %36 : index to i64
  cf.br ^bb3(%c0 : index)
^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
  %40 = arith.cmpi slt, %39, %c1024 : index
  llvm.cond_br %40, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %41 = builtin.unrealized_conversion_cast %39 : index to i64
  %42 = llvm.mul %38, %3  : i64
  %43 = llvm.add %42, %41  : i64
  %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst_1, %44 : f64, !llvm.ptr
  %45 = arith.addi %39, %c1 : index
  cf.br ^bb3(%45 : index)
^bb5:  // pred: ^bb3
  %46 = arith.addi %36, %c1 : index
  cf.br ^bb1(%46 : index)
^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
  %48 = arith.cmpi slt, %47, %c1024 : index
  cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
^bb7:  // pred: ^bb6
  %49 = builtin.unrealized_conversion_cast %47 : index to i64
  cf.br ^bb8(%c0 : index)
^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
  %51 = arith.cmpi slt, %50, %c1024 : index
  llvm.cond_br %51, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %52 = builtin.unrealized_conversion_cast %50 : index to i64
  %53 = llvm.mul %49, %3  : i64
  %54 = llvm.add %53, %52  : i64
  %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst_0, %55 : f64, !llvm.ptr
  %56 = arith.addi %50, %c1 : index
  cf.br ^bb8(%56 : index)
^bb10:  // pred: ^bb8
  %57 = arith.addi %47, %c1 : index
  cf.br ^bb6(%57 : index)
^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
  %59 = arith.cmpi slt, %58, %c1024 : index
  llvm.cond_br %59, ^bb12, ^bb16
^bb12:  // pred: ^bb11
  %60 = builtin.unrealized_conversion_cast %58 : index to i64
  cf.br ^bb13(%c0 : index)
^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
  %62 = arith.cmpi slt, %61, %c1024 : index
  llvm.cond_br %62, ^bb14, ^bb15
^bb14:  // pred: ^bb13
  %63 = builtin.unrealized_conversion_cast %61 : index to i64
  %64 = llvm.mul %60, %3  : i64
  %65 = llvm.add %64, %63  : i64
  %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
  llvm.store %cst, %66 : f64, !llvm.ptr
  %67 = arith.addi %61, %c1 : index
  cf.br ^bb13(%67 : index)
^bb15:  // pred: ^bb13
  %68 = arith.addi %58, %c1 : index
  cf.br ^bb11(%68 : index)
^bb16:  // pred: ^bb11
  omp.parallel {
    omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
      %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
      %75 = llvm.intr.stacksave : !llvm.ptr
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      cf.br ^bb2(%c0 : index)
    ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
      %77 = arith.cmpi slt, %76, %c1024 : index
      llvm.cond_br %77, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %78 = builtin.unrealized_conversion_cast %76 : index to i64
      %79 = llvm.mul %74, %3  : i64
      %80 = llvm.add %79, %78  : i64
      %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
      %82 = llvm.load %81 : !llvm.ptr -> f64
      %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
      %84 = llvm.load %83 : !llvm.ptr -> f64
      %85 = arith.addf %82, %84 : f64
      %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
      llvm.store %85, %86 : f64, !llvm.ptr
      %87 = arith.addi %76, %c1 : index
      cf.br ^bb2(%87 : index)
    ^bb4:  // pred: ^bb2
      llvm.intr.stackrestore %75 : !llvm.ptr
      llvm.br ^bb5
    ^bb5:  // pred: ^bb4
      omp.yield
    }
    omp.terminator
  }
  %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
  llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
  %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
  %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
  %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
  %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
  call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    llvm.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    llvm.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    llvm.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        llvm.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    llvm.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    llvm.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    llvm.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        llvm.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f64
    %cst_0 = arith.constant 3.400000e+00 : f64
    %cst_1 = arith.constant 2.200000e+00 : f64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.zero : !llvm.ptr
    %6 = llvm.getelementptr %5[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %7 = llvm.ptrtoint %6 : !llvm.ptr to i64
    %8 = llvm.add %7, %2  : i64
    %9 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.sub %2, %4  : i64
    %12 = llvm.add %10, %11  : i64
    %13 = llvm.urem %12, %2  : i64
    %14 = llvm.sub %12, %13  : i64
    %15 = llvm.inttoptr %14 : i64 to !llvm.ptr
    %16 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %17 = llvm.ptrtoint %16 : !llvm.ptr to i64
    %18 = llvm.add %17, %11  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%8) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %11  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %29 = llvm.insertvalue %22, %28[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %30 = llvm.insertvalue %27, %29[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %1, %30[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.insertvalue %3, %31[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %3, %32[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %3, %33[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %4, %34[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    cf.br ^bb1(%c0 : index)
  ^bb1(%36: index):  // 2 preds: ^bb0, ^bb5
    %37 = arith.cmpi slt, %36, %c1024 : index
    cf.cond_br %37, ^bb2, ^bb6(%c0 : index)
  ^bb2:  // pred: ^bb1
    %38 = builtin.unrealized_conversion_cast %36 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%39: index):  // 2 preds: ^bb2, ^bb4
    %40 = arith.cmpi slt, %39, %c1024 : index
    llvm.cond_br %40, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %41 = builtin.unrealized_conversion_cast %39 : index to i64
    %42 = llvm.mul %38, %3  : i64
    %43 = llvm.add %42, %41  : i64
    %44 = llvm.getelementptr %15[%43] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_1, %44 : f64, !llvm.ptr
    %45 = arith.addi %39, %c1 : index
    cf.br ^bb3(%45 : index)
  ^bb5:  // pred: ^bb3
    %46 = arith.addi %36, %c1 : index
    cf.br ^bb1(%46 : index)
  ^bb6(%47: index):  // 2 preds: ^bb1, ^bb10
    %48 = arith.cmpi slt, %47, %c1024 : index
    cf.cond_br %48, ^bb7, ^bb11(%c0 : index)
  ^bb7:  // pred: ^bb6
    %49 = builtin.unrealized_conversion_cast %47 : index to i64
    cf.br ^bb8(%c0 : index)
  ^bb8(%50: index):  // 2 preds: ^bb7, ^bb9
    %51 = arith.cmpi slt, %50, %c1024 : index
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %49, %3  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %21[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst_0, %55 : f64, !llvm.ptr
    %56 = arith.addi %50, %c1 : index
    cf.br ^bb8(%56 : index)
  ^bb10:  // pred: ^bb8
    %57 = arith.addi %47, %c1 : index
    cf.br ^bb6(%57 : index)
  ^bb11(%58: index):  // 2 preds: ^bb6, ^bb15
    %59 = arith.cmpi slt, %58, %c1024 : index
    llvm.cond_br %59, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %60 = builtin.unrealized_conversion_cast %58 : index to i64
    cf.br ^bb13(%c0 : index)
  ^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
    %62 = arith.cmpi slt, %61, %c1024 : index
    llvm.cond_br %62, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %63 = builtin.unrealized_conversion_cast %61 : index to i64
    %64 = llvm.mul %60, %3  : i64
    %65 = llvm.add %64, %63  : i64
    %66 = llvm.getelementptr %27[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %cst, %66 : f64, !llvm.ptr
    %67 = arith.addi %61, %c1 : index
    cf.br ^bb13(%67 : index)
  ^bb15:  // pred: ^bb13
    %68 = arith.addi %58, %c1 : index
    cf.br ^bb11(%68 : index)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%c0) to (%c1024) step (%c1) {
        %74 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %75 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%c0 : index)
      ^bb2(%76: index):  // 2 preds: ^bb1, ^bb3
        %77 = arith.cmpi slt, %76, %c1024 : index
        llvm.cond_br %77, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %78 = builtin.unrealized_conversion_cast %76 : index to i64
        %79 = llvm.mul %74, %3  : i64
        %80 = llvm.add %79, %78  : i64
        %81 = llvm.getelementptr %15[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %82 = llvm.load %81 : !llvm.ptr -> f64
        %83 = llvm.getelementptr %21[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %84 = llvm.load %83 : !llvm.ptr -> f64
        %85 = arith.addf %82, %84 : f64
        %86 = llvm.getelementptr %27[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %85, %86 : f64, !llvm.ptr
        %87 = arith.addi %76, %c1 : index
        cf.br ^bb2(%87 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %75 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %35, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    %73 = builtin.unrealized_conversion_cast %72 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    call @comet_print_memref_f64(%73) : (memref<*xf64>) -> ()
    return
  }
  func.func private @comet_sort_index(memref<*xindex>, index, index)
  func.func private @comet_print_memref_f64(memref<*xf64>)
}


// -----// IR Dump After ConvertFuncToLLVMPass (convert-func-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = builtin.unrealized_conversion_cast %3 : i64 to index
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = builtin.unrealized_conversion_cast %5 : i64 to index
    %7 = llvm.mlir.constant(0 : index) : i64
    %8 = builtin.unrealized_conversion_cast %7 : i64 to index
    %9 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %10 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %11 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.mlir.zero : !llvm.ptr
    %15 = llvm.getelementptr %14[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %16 = llvm.ptrtoint %15 : !llvm.ptr to i64
    %17 = llvm.add %16, %2  : i64
    %18 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %19 = llvm.ptrtoint %18 : !llvm.ptr to i64
    %20 = llvm.sub %2, %13  : i64
    %21 = llvm.add %19, %20  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %20  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.add %32, %20  : i64
    %34 = llvm.urem %33, %2  : i64
    %35 = llvm.sub %33, %34  : i64
    %36 = llvm.inttoptr %35 : i64 to !llvm.ptr
    %37 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %38 = llvm.insertvalue %31, %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %1, %39[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %12, %40[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.insertvalue %12, %41[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %12, %42[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %13, %43[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%7 : i64)
  ^bb1(%45: i64):  // 2 preds: ^bb0, ^bb5
    %46 = builtin.unrealized_conversion_cast %45 : i64 to index
    %47 = llvm.icmp "slt" %45, %3 : i64
    llvm.cond_br %47, ^bb2, ^bb6(%7 : i64)
  ^bb2:  // pred: ^bb1
    %48 = builtin.unrealized_conversion_cast %46 : index to i64
    llvm.br ^bb3(%7 : i64)
  ^bb3(%49: i64):  // 2 preds: ^bb2, ^bb4
    %50 = builtin.unrealized_conversion_cast %49 : i64 to index
    %51 = llvm.icmp "slt" %49, %3 : i64
    llvm.cond_br %51, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %52 = builtin.unrealized_conversion_cast %50 : index to i64
    %53 = llvm.mul %48, %12  : i64
    %54 = llvm.add %53, %52  : i64
    %55 = llvm.getelementptr %24[%54] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %11, %55 : f64, !llvm.ptr
    %56 = llvm.add %49, %5  : i64
    llvm.br ^bb3(%56 : i64)
  ^bb5:  // pred: ^bb3
    %57 = llvm.add %45, %5  : i64
    llvm.br ^bb1(%57 : i64)
  ^bb6(%58: i64):  // 2 preds: ^bb1, ^bb10
    %59 = builtin.unrealized_conversion_cast %58 : i64 to index
    %60 = llvm.icmp "slt" %58, %3 : i64
    llvm.cond_br %60, ^bb7, ^bb11(%7 : i64)
  ^bb7:  // pred: ^bb6
    %61 = builtin.unrealized_conversion_cast %59 : index to i64
    llvm.br ^bb8(%7 : i64)
  ^bb8(%62: i64):  // 2 preds: ^bb7, ^bb9
    %63 = builtin.unrealized_conversion_cast %62 : i64 to index
    %64 = llvm.icmp "slt" %62, %3 : i64
    llvm.cond_br %64, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %65 = builtin.unrealized_conversion_cast %63 : index to i64
    %66 = llvm.mul %61, %12  : i64
    %67 = llvm.add %66, %65  : i64
    %68 = llvm.getelementptr %30[%67] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %68 : f64, !llvm.ptr
    %69 = llvm.add %62, %5  : i64
    llvm.br ^bb8(%69 : i64)
  ^bb10:  // pred: ^bb8
    %70 = llvm.add %58, %5  : i64
    llvm.br ^bb6(%70 : i64)
  ^bb11(%71: i64):  // 2 preds: ^bb6, ^bb15
    %72 = builtin.unrealized_conversion_cast %71 : i64 to index
    %73 = llvm.icmp "slt" %71, %3 : i64
    llvm.cond_br %73, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    %74 = builtin.unrealized_conversion_cast %72 : index to i64
    llvm.br ^bb13(%7 : i64)
  ^bb13(%75: i64):  // 2 preds: ^bb12, ^bb14
    %76 = builtin.unrealized_conversion_cast %75 : i64 to index
    %77 = llvm.icmp "slt" %75, %3 : i64
    llvm.cond_br %77, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %78 = builtin.unrealized_conversion_cast %76 : index to i64
    %79 = llvm.mul %74, %12  : i64
    %80 = llvm.add %79, %78  : i64
    %81 = llvm.getelementptr %36[%80] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %81 : f64, !llvm.ptr
    %82 = llvm.add %75, %5  : i64
    llvm.br ^bb13(%82 : i64)
  ^bb15:  // pred: ^bb13
    %83 = llvm.add %71, %5  : i64
    llvm.br ^bb11(%83 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%8) to (%4) step (%6) {
        %91 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %92 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%8 : index)
      ^bb2(%93: index):  // 2 preds: ^bb1, ^bb3
        %94 = builtin.unrealized_conversion_cast %93 : index to i64
        %95 = llvm.icmp "slt" %94, %3 : i64
        llvm.cond_br %95, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %96 = builtin.unrealized_conversion_cast %93 : index to i64
        %97 = llvm.mul %91, %12  : i64
        %98 = llvm.add %97, %96  : i64
        %99 = llvm.getelementptr %24[%98] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %100 = llvm.load %99 : !llvm.ptr -> f64
        %101 = llvm.getelementptr %30[%98] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %102 = llvm.load %101 : !llvm.ptr -> f64
        %103 = llvm.fadd %100, %102  : f64
        %104 = llvm.getelementptr %36[%98] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %103, %104 : f64, !llvm.ptr
        %105 = llvm.add %94, %5  : i64
        %106 = builtin.unrealized_conversion_cast %105 : i64 to index
        cf.br ^bb2(%106 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %92 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %84 = llvm.alloca %13 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %44, %84 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %85 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %86 = llvm.insertvalue %0, %85[0] : !llvm.struct<(i64, ptr)> 
    %87 = llvm.insertvalue %84, %86[1] : !llvm.struct<(i64, ptr)> 
    %88 = builtin.unrealized_conversion_cast %87 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %89 = llvm.extractvalue %87[0] : !llvm.struct<(i64, ptr)> 
    %90 = llvm.extractvalue %87[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%89, %90) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ConvertIndexToLLVMPass (convert-index-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = builtin.unrealized_conversion_cast %3 : i64 to index
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = builtin.unrealized_conversion_cast %5 : i64 to index
    %7 = llvm.mlir.constant(0 : index) : i64
    %8 = builtin.unrealized_conversion_cast %7 : i64 to index
    %9 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %10 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %11 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.mlir.zero : !llvm.ptr
    %15 = llvm.getelementptr %14[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %16 = llvm.ptrtoint %15 : !llvm.ptr to i64
    %17 = llvm.add %16, %2  : i64
    %18 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %19 = llvm.ptrtoint %18 : !llvm.ptr to i64
    %20 = llvm.sub %2, %13  : i64
    %21 = llvm.add %19, %20  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %20  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.add %32, %20  : i64
    %34 = llvm.urem %33, %2  : i64
    %35 = llvm.sub %33, %34  : i64
    %36 = llvm.inttoptr %35 : i64 to !llvm.ptr
    %37 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %38 = llvm.insertvalue %31, %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %1, %39[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %12, %40[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.insertvalue %12, %41[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %12, %42[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %13, %43[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%7 : i64)
  ^bb1(%45: i64):  // 2 preds: ^bb0, ^bb5
    %46 = builtin.unrealized_conversion_cast %45 : i64 to index
    %47 = llvm.icmp "slt" %45, %3 : i64
    llvm.cond_br %47, ^bb2, ^bb6(%7 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%7 : i64)
  ^bb3(%48: i64):  // 2 preds: ^bb2, ^bb4
    %49 = builtin.unrealized_conversion_cast %48 : i64 to index
    %50 = llvm.icmp "slt" %48, %3 : i64
    llvm.cond_br %50, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %51 = llvm.mul %45, %12  : i64
    %52 = llvm.add %51, %48  : i64
    %53 = llvm.getelementptr %24[%52] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %11, %53 : f64, !llvm.ptr
    %54 = llvm.add %48, %5  : i64
    llvm.br ^bb3(%54 : i64)
  ^bb5:  // pred: ^bb3
    %55 = llvm.add %45, %5  : i64
    llvm.br ^bb1(%55 : i64)
  ^bb6(%56: i64):  // 2 preds: ^bb1, ^bb10
    %57 = builtin.unrealized_conversion_cast %56 : i64 to index
    %58 = llvm.icmp "slt" %56, %3 : i64
    llvm.cond_br %58, ^bb7, ^bb11(%7 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%7 : i64)
  ^bb8(%59: i64):  // 2 preds: ^bb7, ^bb9
    %60 = builtin.unrealized_conversion_cast %59 : i64 to index
    %61 = llvm.icmp "slt" %59, %3 : i64
    llvm.cond_br %61, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %62 = llvm.mul %56, %12  : i64
    %63 = llvm.add %62, %59  : i64
    %64 = llvm.getelementptr %30[%63] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %64 : f64, !llvm.ptr
    %65 = llvm.add %59, %5  : i64
    llvm.br ^bb8(%65 : i64)
  ^bb10:  // pred: ^bb8
    %66 = llvm.add %56, %5  : i64
    llvm.br ^bb6(%66 : i64)
  ^bb11(%67: i64):  // 2 preds: ^bb6, ^bb15
    %68 = builtin.unrealized_conversion_cast %67 : i64 to index
    %69 = llvm.icmp "slt" %67, %3 : i64
    llvm.cond_br %69, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%7 : i64)
  ^bb13(%70: i64):  // 2 preds: ^bb12, ^bb14
    %71 = builtin.unrealized_conversion_cast %70 : i64 to index
    %72 = llvm.icmp "slt" %70, %3 : i64
    llvm.cond_br %72, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %73 = llvm.mul %67, %12  : i64
    %74 = llvm.add %73, %70  : i64
    %75 = llvm.getelementptr %36[%74] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %75 : f64, !llvm.ptr
    %76 = llvm.add %70, %5  : i64
    llvm.br ^bb13(%76 : i64)
  ^bb15:  // pred: ^bb13
    %77 = llvm.add %67, %5  : i64
    llvm.br ^bb11(%77 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : index = (%8) to (%4) step (%6) {
        %85 = builtin.unrealized_conversion_cast %arg0 : index to i64
        %86 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        cf.br ^bb2(%8 : index)
      ^bb2(%87: index):  // 2 preds: ^bb1, ^bb3
        %88 = builtin.unrealized_conversion_cast %87 : index to i64
        %89 = llvm.icmp "slt" %88, %3 : i64
        llvm.cond_br %89, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %90 = builtin.unrealized_conversion_cast %87 : index to i64
        %91 = llvm.mul %85, %12  : i64
        %92 = llvm.add %91, %90  : i64
        %93 = llvm.getelementptr %24[%92] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %94 = llvm.load %93 : !llvm.ptr -> f64
        %95 = llvm.getelementptr %30[%92] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %96 = llvm.load %95 : !llvm.ptr -> f64
        %97 = llvm.fadd %94, %96  : f64
        %98 = llvm.getelementptr %36[%92] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %97, %98 : f64, !llvm.ptr
        %99 = llvm.add %88, %5  : i64
        %100 = builtin.unrealized_conversion_cast %99 : i64 to index
        cf.br ^bb2(%100 : index)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %86 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %78 = llvm.alloca %13 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %44, %78 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %80 = llvm.insertvalue %0, %79[0] : !llvm.struct<(i64, ptr)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(i64, ptr)> 
    %82 = builtin.unrealized_conversion_cast %81 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %83 = llvm.extractvalue %81[0] : !llvm.struct<(i64, ptr)> 
    %84 = llvm.extractvalue %81[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%83, %84) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = builtin.unrealized_conversion_cast %3 : i64 to index
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = builtin.unrealized_conversion_cast %5 : i64 to index
    %7 = llvm.mlir.constant(0 : index) : i64
    %8 = builtin.unrealized_conversion_cast %7 : i64 to index
    %9 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %10 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %11 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %12 = llvm.mlir.constant(1024 : index) : i64
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.mlir.zero : !llvm.ptr
    %15 = llvm.getelementptr %14[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %16 = llvm.ptrtoint %15 : !llvm.ptr to i64
    %17 = llvm.add %16, %2  : i64
    %18 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %19 = llvm.ptrtoint %18 : !llvm.ptr to i64
    %20 = llvm.sub %2, %13  : i64
    %21 = llvm.add %19, %20  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %20  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.call @malloc(%17) : (i64) -> !llvm.ptr
    %32 = llvm.ptrtoint %31 : !llvm.ptr to i64
    %33 = llvm.add %32, %20  : i64
    %34 = llvm.urem %33, %2  : i64
    %35 = llvm.sub %33, %34  : i64
    %36 = llvm.inttoptr %35 : i64 to !llvm.ptr
    %37 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %38 = llvm.insertvalue %31, %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %1, %39[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %12, %40[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.insertvalue %12, %41[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %43 = llvm.insertvalue %12, %42[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.insertvalue %13, %43[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%7 : i64)
  ^bb1(%45: i64):  // 2 preds: ^bb0, ^bb5
    %46 = builtin.unrealized_conversion_cast %45 : i64 to index
    %47 = llvm.icmp "slt" %45, %3 : i64
    llvm.cond_br %47, ^bb2, ^bb6(%7 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%7 : i64)
  ^bb3(%48: i64):  // 2 preds: ^bb2, ^bb4
    %49 = builtin.unrealized_conversion_cast %48 : i64 to index
    %50 = llvm.icmp "slt" %48, %3 : i64
    llvm.cond_br %50, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %51 = llvm.mul %45, %12  : i64
    %52 = llvm.add %51, %48  : i64
    %53 = llvm.getelementptr %24[%52] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %11, %53 : f64, !llvm.ptr
    %54 = llvm.add %48, %5  : i64
    llvm.br ^bb3(%54 : i64)
  ^bb5:  // pred: ^bb3
    %55 = llvm.add %45, %5  : i64
    llvm.br ^bb1(%55 : i64)
  ^bb6(%56: i64):  // 2 preds: ^bb1, ^bb10
    %57 = builtin.unrealized_conversion_cast %56 : i64 to index
    %58 = llvm.icmp "slt" %56, %3 : i64
    llvm.cond_br %58, ^bb7, ^bb11(%7 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%7 : i64)
  ^bb8(%59: i64):  // 2 preds: ^bb7, ^bb9
    %60 = builtin.unrealized_conversion_cast %59 : i64 to index
    %61 = llvm.icmp "slt" %59, %3 : i64
    llvm.cond_br %61, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %62 = llvm.mul %56, %12  : i64
    %63 = llvm.add %62, %59  : i64
    %64 = llvm.getelementptr %30[%63] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %10, %64 : f64, !llvm.ptr
    %65 = llvm.add %59, %5  : i64
    llvm.br ^bb8(%65 : i64)
  ^bb10:  // pred: ^bb8
    %66 = llvm.add %56, %5  : i64
    llvm.br ^bb6(%66 : i64)
  ^bb11(%67: i64):  // 2 preds: ^bb6, ^bb15
    %68 = builtin.unrealized_conversion_cast %67 : i64 to index
    %69 = llvm.icmp "slt" %67, %3 : i64
    llvm.cond_br %69, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%7 : i64)
  ^bb13(%70: i64):  // 2 preds: ^bb12, ^bb14
    %71 = builtin.unrealized_conversion_cast %70 : i64 to index
    %72 = llvm.icmp "slt" %70, %3 : i64
    llvm.cond_br %72, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %73 = llvm.mul %67, %12  : i64
    %74 = llvm.add %73, %70  : i64
    %75 = llvm.getelementptr %36[%74] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %9, %75 : f64, !llvm.ptr
    %76 = llvm.add %70, %5  : i64
    llvm.br ^bb13(%76 : i64)
  ^bb15:  // pred: ^bb13
    %77 = llvm.add %67, %5  : i64
    llvm.br ^bb11(%77 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : i64 = (%7) to (%3) step (%5) {
        %85 = builtin.unrealized_conversion_cast %arg0 : i64 to index
        %86 = builtin.unrealized_conversion_cast %85 : index to i64
        %87 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        llvm.br ^bb2(%7 : i64)
      ^bb2(%88: i64):  // 2 preds: ^bb1, ^bb3
        %89 = builtin.unrealized_conversion_cast %88 : i64 to index
        %90 = builtin.unrealized_conversion_cast %89 : index to i64
        %91 = llvm.icmp "slt" %90, %3 : i64
        llvm.cond_br %91, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %92 = builtin.unrealized_conversion_cast %89 : index to i64
        %93 = llvm.mul %86, %12  : i64
        %94 = llvm.add %93, %92  : i64
        %95 = llvm.getelementptr %24[%94] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %96 = llvm.load %95 : !llvm.ptr -> f64
        %97 = llvm.getelementptr %30[%94] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %98 = llvm.load %97 : !llvm.ptr -> f64
        %99 = llvm.fadd %96, %98  : f64
        %100 = llvm.getelementptr %36[%94] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %99, %100 : f64, !llvm.ptr
        %101 = llvm.add %90, %5  : i64
        %102 = builtin.unrealized_conversion_cast %101 : i64 to index
        llvm.br ^bb2(%101 : i64)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %87 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %78 = llvm.alloca %13 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %44, %78 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %80 = llvm.insertvalue %0, %79[0] : !llvm.struct<(i64, ptr)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(i64, ptr)> 
    %82 = builtin.unrealized_conversion_cast %81 : !llvm.struct<(i64, ptr)> to memref<*xf64>
    %83 = llvm.extractvalue %81[0] : !llvm.struct<(i64, ptr)> 
    %84 = llvm.extractvalue %81[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%83, %84) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(0 : index) : i64
    %6 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %7 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %8 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %9 = llvm.mlir.constant(1024 : index) : i64
    %10 = llvm.mlir.constant(1 : index) : i64
    %11 = llvm.mlir.zero : !llvm.ptr
    %12 = llvm.getelementptr %11[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.add %13, %2  : i64
    %15 = llvm.call @malloc(%14) : (i64) -> !llvm.ptr
    %16 = llvm.ptrtoint %15 : !llvm.ptr to i64
    %17 = llvm.sub %2, %10  : i64
    %18 = llvm.add %16, %17  : i64
    %19 = llvm.urem %18, %2  : i64
    %20 = llvm.sub %18, %19  : i64
    %21 = llvm.inttoptr %20 : i64 to !llvm.ptr
    %22 = llvm.call @malloc(%14) : (i64) -> !llvm.ptr
    %23 = llvm.ptrtoint %22 : !llvm.ptr to i64
    %24 = llvm.add %23, %17  : i64
    %25 = llvm.urem %24, %2  : i64
    %26 = llvm.sub %24, %25  : i64
    %27 = llvm.inttoptr %26 : i64 to !llvm.ptr
    %28 = llvm.call @malloc(%14) : (i64) -> !llvm.ptr
    %29 = llvm.ptrtoint %28 : !llvm.ptr to i64
    %30 = llvm.add %29, %17  : i64
    %31 = llvm.urem %30, %2  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %28, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %1, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %9, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %10, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%5 : i64)
  ^bb1(%42: i64):  // 2 preds: ^bb0, ^bb5
    %43 = llvm.icmp "slt" %42, %3 : i64
    llvm.cond_br %43, ^bb2, ^bb6(%5 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%5 : i64)
  ^bb3(%44: i64):  // 2 preds: ^bb2, ^bb4
    %45 = llvm.icmp "slt" %44, %3 : i64
    llvm.cond_br %45, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %46 = llvm.mul %42, %9  : i64
    %47 = llvm.add %46, %44  : i64
    %48 = llvm.getelementptr %21[%47] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %8, %48 : f64, !llvm.ptr
    %49 = llvm.add %44, %4  : i64
    llvm.br ^bb3(%49 : i64)
  ^bb5:  // pred: ^bb3
    %50 = llvm.add %42, %4  : i64
    llvm.br ^bb1(%50 : i64)
  ^bb6(%51: i64):  // 2 preds: ^bb1, ^bb10
    %52 = llvm.icmp "slt" %51, %3 : i64
    llvm.cond_br %52, ^bb7, ^bb11(%5 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%5 : i64)
  ^bb8(%53: i64):  // 2 preds: ^bb7, ^bb9
    %54 = llvm.icmp "slt" %53, %3 : i64
    llvm.cond_br %54, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %55 = llvm.mul %51, %9  : i64
    %56 = llvm.add %55, %53  : i64
    %57 = llvm.getelementptr %27[%56] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %7, %57 : f64, !llvm.ptr
    %58 = llvm.add %53, %4  : i64
    llvm.br ^bb8(%58 : i64)
  ^bb10:  // pred: ^bb8
    %59 = llvm.add %51, %4  : i64
    llvm.br ^bb6(%59 : i64)
  ^bb11(%60: i64):  // 2 preds: ^bb6, ^bb15
    %61 = llvm.icmp "slt" %60, %3 : i64
    llvm.cond_br %61, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%5 : i64)
  ^bb13(%62: i64):  // 2 preds: ^bb12, ^bb14
    %63 = llvm.icmp "slt" %62, %3 : i64
    llvm.cond_br %63, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %64 = llvm.mul %60, %9  : i64
    %65 = llvm.add %64, %62  : i64
    %66 = llvm.getelementptr %33[%65] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %6, %66 : f64, !llvm.ptr
    %67 = llvm.add %62, %4  : i64
    llvm.br ^bb13(%67 : i64)
  ^bb15:  // pred: ^bb13
    %68 = llvm.add %60, %4  : i64
    llvm.br ^bb11(%68 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : i64 = (%5) to (%3) step (%4) {
        %73 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        llvm.br ^bb2(%5 : i64)
      ^bb2(%74: i64):  // 2 preds: ^bb1, ^bb3
        %75 = llvm.icmp "slt" %74, %3 : i64
        llvm.cond_br %75, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %76 = llvm.mul %arg0, %9  : i64
        %77 = llvm.add %76, %74  : i64
        %78 = llvm.getelementptr %21[%77] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %79 = llvm.load %78 : !llvm.ptr -> f64
        %80 = llvm.getelementptr %27[%77] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %81 = llvm.load %80 : !llvm.ptr -> f64
        %82 = llvm.fadd %79, %81  : f64
        %83 = llvm.getelementptr %33[%77] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %82, %83 : f64, !llvm.ptr
        %84 = llvm.add %74, %4  : i64
        llvm.br ^bb2(%84 : i64)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %73 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %69 = llvm.alloca %10 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %41, %69 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %70 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %71 = llvm.insertvalue %0, %70[0] : !llvm.struct<(i64, ptr)> 
    %72 = llvm.insertvalue %69, %71[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @comet_print_memref_f64(%0, %69) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %6 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %7 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %8 = llvm.mlir.zero : !llvm.ptr
    %9 = llvm.getelementptr %8[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %2  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %2, %4  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %2  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i64
    %21 = llvm.add %20, %14  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %14  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %32 = llvm.insertvalue %25, %31[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %30, %32[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %1, %33[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %3, %34[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %3, %35[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %3, %36[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %4, %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%1 : i64)
  ^bb1(%39: i64):  // 2 preds: ^bb0, ^bb5
    %40 = llvm.icmp "slt" %39, %3 : i64
    llvm.cond_br %40, ^bb2, ^bb6(%1 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%1 : i64)
  ^bb3(%41: i64):  // 2 preds: ^bb2, ^bb4
    %42 = llvm.icmp "slt" %41, %3 : i64
    llvm.cond_br %42, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %43 = llvm.mul %39, %3  : i64
    %44 = llvm.add %43, %41  : i64
    %45 = llvm.getelementptr %18[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %7, %45 : f64, !llvm.ptr
    %46 = llvm.add %41, %4  : i64
    llvm.br ^bb3(%46 : i64)
  ^bb5:  // pred: ^bb3
    %47 = llvm.add %39, %4  : i64
    llvm.br ^bb1(%47 : i64)
  ^bb6(%48: i64):  // 2 preds: ^bb1, ^bb10
    %49 = llvm.icmp "slt" %48, %3 : i64
    llvm.cond_br %49, ^bb7, ^bb11(%1 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%1 : i64)
  ^bb8(%50: i64):  // 2 preds: ^bb7, ^bb9
    %51 = llvm.icmp "slt" %50, %3 : i64
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = llvm.mul %48, %3  : i64
    %53 = llvm.add %52, %50  : i64
    %54 = llvm.getelementptr %24[%53] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %6, %54 : f64, !llvm.ptr
    %55 = llvm.add %50, %4  : i64
    llvm.br ^bb8(%55 : i64)
  ^bb10:  // pred: ^bb8
    %56 = llvm.add %48, %4  : i64
    llvm.br ^bb6(%56 : i64)
  ^bb11(%57: i64):  // 2 preds: ^bb6, ^bb15
    %58 = llvm.icmp "slt" %57, %3 : i64
    llvm.cond_br %58, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%1 : i64)
  ^bb13(%59: i64):  // 2 preds: ^bb12, ^bb14
    %60 = llvm.icmp "slt" %59, %3 : i64
    llvm.cond_br %60, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %61 = llvm.mul %57, %3  : i64
    %62 = llvm.add %61, %59  : i64
    %63 = llvm.getelementptr %30[%62] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %5, %63 : f64, !llvm.ptr
    %64 = llvm.add %59, %4  : i64
    llvm.br ^bb13(%64 : i64)
  ^bb15:  // pred: ^bb13
    %65 = llvm.add %57, %4  : i64
    llvm.br ^bb11(%65 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : i64 = (%1) to (%3) step (%4) {
        %67 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        llvm.br ^bb2(%1 : i64)
      ^bb2(%68: i64):  // 2 preds: ^bb1, ^bb3
        %69 = llvm.icmp "slt" %68, %3 : i64
        llvm.cond_br %69, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %70 = llvm.mul %arg0, %3  : i64
        %71 = llvm.add %70, %68  : i64
        %72 = llvm.getelementptr %18[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %73 = llvm.load %72 : !llvm.ptr -> f64
        %74 = llvm.getelementptr %24[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %75 = llvm.load %74 : !llvm.ptr -> f64
        %76 = llvm.fadd %73, %75  : f64
        %77 = llvm.getelementptr %30[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %76, %77 : f64, !llvm.ptr
        %78 = llvm.add %68, %4  : i64
        llvm.br ^bb2(%78 : i64)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %67 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %66 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %38, %66 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %66) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


// -----// IR Dump After CSE (cse) //----- //
module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %6 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %7 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %8 = llvm.mlir.zero : !llvm.ptr
    %9 = llvm.getelementptr %8[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %2  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %2, %4  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %2  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i64
    %21 = llvm.add %20, %14  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %14  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %32 = llvm.insertvalue %25, %31[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %30, %32[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %1, %33[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %3, %34[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %3, %35[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %3, %36[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %4, %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%1 : i64)
  ^bb1(%39: i64):  // 2 preds: ^bb0, ^bb5
    %40 = llvm.icmp "slt" %39, %3 : i64
    llvm.cond_br %40, ^bb2, ^bb6(%1 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%1 : i64)
  ^bb3(%41: i64):  // 2 preds: ^bb2, ^bb4
    %42 = llvm.icmp "slt" %41, %3 : i64
    llvm.cond_br %42, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %43 = llvm.mul %39, %3  : i64
    %44 = llvm.add %43, %41  : i64
    %45 = llvm.getelementptr %18[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %7, %45 : f64, !llvm.ptr
    %46 = llvm.add %41, %4  : i64
    llvm.br ^bb3(%46 : i64)
  ^bb5:  // pred: ^bb3
    %47 = llvm.add %39, %4  : i64
    llvm.br ^bb1(%47 : i64)
  ^bb6(%48: i64):  // 2 preds: ^bb1, ^bb10
    %49 = llvm.icmp "slt" %48, %3 : i64
    llvm.cond_br %49, ^bb7, ^bb11(%1 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%1 : i64)
  ^bb8(%50: i64):  // 2 preds: ^bb7, ^bb9
    %51 = llvm.icmp "slt" %50, %3 : i64
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = llvm.mul %48, %3  : i64
    %53 = llvm.add %52, %50  : i64
    %54 = llvm.getelementptr %24[%53] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %6, %54 : f64, !llvm.ptr
    %55 = llvm.add %50, %4  : i64
    llvm.br ^bb8(%55 : i64)
  ^bb10:  // pred: ^bb8
    %56 = llvm.add %48, %4  : i64
    llvm.br ^bb6(%56 : i64)
  ^bb11(%57: i64):  // 2 preds: ^bb6, ^bb15
    %58 = llvm.icmp "slt" %57, %3 : i64
    llvm.cond_br %58, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%1 : i64)
  ^bb13(%59: i64):  // 2 preds: ^bb12, ^bb14
    %60 = llvm.icmp "slt" %59, %3 : i64
    llvm.cond_br %60, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %61 = llvm.mul %57, %3  : i64
    %62 = llvm.add %61, %59  : i64
    %63 = llvm.getelementptr %30[%62] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %5, %63 : f64, !llvm.ptr
    %64 = llvm.add %59, %4  : i64
    llvm.br ^bb13(%64 : i64)
  ^bb15:  // pred: ^bb13
    %65 = llvm.add %57, %4  : i64
    llvm.br ^bb11(%65 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : i64 = (%1) to (%3) step (%4) {
        %67 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        llvm.br ^bb2(%1 : i64)
      ^bb2(%68: i64):  // 2 preds: ^bb1, ^bb3
        %69 = llvm.icmp "slt" %68, %3 : i64
        llvm.cond_br %69, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %70 = llvm.mul %arg0, %3  : i64
        %71 = llvm.add %70, %68  : i64
        %72 = llvm.getelementptr %18[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %73 = llvm.load %72 : !llvm.ptr -> f64
        %74 = llvm.getelementptr %24[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %75 = llvm.load %74 : !llvm.ptr -> f64
        %76 = llvm.fadd %73, %75  : f64
        %77 = llvm.getelementptr %30[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %76, %77 : f64, !llvm.ptr
        %78 = llvm.add %68, %4  : i64
        llvm.br ^bb2(%78 : i64)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %67 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %66 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %38, %66 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %66) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}


module {
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func @main() {
    %0 = llvm.mlir.constant(2 : index) : i64
    %1 = llvm.mlir.constant(0 : index) : i64
    %2 = llvm.mlir.constant(32 : index) : i64
    %3 = llvm.mlir.constant(1024 : index) : i64
    %4 = llvm.mlir.constant(1 : index) : i64
    %5 = llvm.mlir.constant(0.000000e+00 : f64) : f64
    %6 = llvm.mlir.constant(3.400000e+00 : f64) : f64
    %7 = llvm.mlir.constant(2.200000e+00 : f64) : f64
    %8 = llvm.mlir.zero : !llvm.ptr
    %9 = llvm.getelementptr %8[1048576] : (!llvm.ptr) -> !llvm.ptr, f64
    %10 = llvm.ptrtoint %9 : !llvm.ptr to i64
    %11 = llvm.add %10, %2  : i64
    %12 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %13 = llvm.ptrtoint %12 : !llvm.ptr to i64
    %14 = llvm.sub %2, %4  : i64
    %15 = llvm.add %13, %14  : i64
    %16 = llvm.urem %15, %2  : i64
    %17 = llvm.sub %15, %16  : i64
    %18 = llvm.inttoptr %17 : i64 to !llvm.ptr
    %19 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i64
    %21 = llvm.add %20, %14  : i64
    %22 = llvm.urem %21, %2  : i64
    %23 = llvm.sub %21, %22  : i64
    %24 = llvm.inttoptr %23 : i64 to !llvm.ptr
    %25 = llvm.call @malloc(%11) : (i64) -> !llvm.ptr
    %26 = llvm.ptrtoint %25 : !llvm.ptr to i64
    %27 = llvm.add %26, %14  : i64
    %28 = llvm.urem %27, %2  : i64
    %29 = llvm.sub %27, %28  : i64
    %30 = llvm.inttoptr %29 : i64 to !llvm.ptr
    %31 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %32 = llvm.insertvalue %25, %31[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %33 = llvm.insertvalue %30, %32[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %1, %33[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %3, %34[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %3, %35[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %3, %36[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %4, %37[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%1 : i64)
  ^bb1(%39: i64):  // 2 preds: ^bb0, ^bb5
    %40 = llvm.icmp "slt" %39, %3 : i64
    llvm.cond_br %40, ^bb2, ^bb6(%1 : i64)
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%1 : i64)
  ^bb3(%41: i64):  // 2 preds: ^bb2, ^bb4
    %42 = llvm.icmp "slt" %41, %3 : i64
    llvm.cond_br %42, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %43 = llvm.mul %39, %3  : i64
    %44 = llvm.add %43, %41  : i64
    %45 = llvm.getelementptr %18[%44] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %7, %45 : f64, !llvm.ptr
    %46 = llvm.add %41, %4  : i64
    llvm.br ^bb3(%46 : i64)
  ^bb5:  // pred: ^bb3
    %47 = llvm.add %39, %4  : i64
    llvm.br ^bb1(%47 : i64)
  ^bb6(%48: i64):  // 2 preds: ^bb1, ^bb10
    %49 = llvm.icmp "slt" %48, %3 : i64
    llvm.cond_br %49, ^bb7, ^bb11(%1 : i64)
  ^bb7:  // pred: ^bb6
    llvm.br ^bb8(%1 : i64)
  ^bb8(%50: i64):  // 2 preds: ^bb7, ^bb9
    %51 = llvm.icmp "slt" %50, %3 : i64
    llvm.cond_br %51, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %52 = llvm.mul %48, %3  : i64
    %53 = llvm.add %52, %50  : i64
    %54 = llvm.getelementptr %24[%53] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %6, %54 : f64, !llvm.ptr
    %55 = llvm.add %50, %4  : i64
    llvm.br ^bb8(%55 : i64)
  ^bb10:  // pred: ^bb8
    %56 = llvm.add %48, %4  : i64
    llvm.br ^bb6(%56 : i64)
  ^bb11(%57: i64):  // 2 preds: ^bb6, ^bb15
    %58 = llvm.icmp "slt" %57, %3 : i64
    llvm.cond_br %58, ^bb12, ^bb16
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13(%1 : i64)
  ^bb13(%59: i64):  // 2 preds: ^bb12, ^bb14
    %60 = llvm.icmp "slt" %59, %3 : i64
    llvm.cond_br %60, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %61 = llvm.mul %57, %3  : i64
    %62 = llvm.add %61, %59  : i64
    %63 = llvm.getelementptr %30[%62] : (!llvm.ptr, i64) -> !llvm.ptr, f64
    llvm.store %5, %63 : f64, !llvm.ptr
    %64 = llvm.add %59, %4  : i64
    llvm.br ^bb13(%64 : i64)
  ^bb15:  // pred: ^bb13
    %65 = llvm.add %57, %4  : i64
    llvm.br ^bb11(%65 : i64)
  ^bb16:  // pred: ^bb11
    omp.parallel {
      omp.wsloop for  (%arg0) : i64 = (%1) to (%3) step (%4) {
        %67 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        llvm.br ^bb2(%1 : i64)
      ^bb2(%68: i64):  // 2 preds: ^bb1, ^bb3
        %69 = llvm.icmp "slt" %68, %3 : i64
        llvm.cond_br %69, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %70 = llvm.mul %arg0, %3  : i64
        %71 = llvm.add %70, %68  : i64
        %72 = llvm.getelementptr %18[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %73 = llvm.load %72 : !llvm.ptr -> f64
        %74 = llvm.getelementptr %24[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        %75 = llvm.load %74 : !llvm.ptr -> f64
        %76 = llvm.fadd %73, %75  : f64
        %77 = llvm.getelementptr %30[%71] : (!llvm.ptr, i64) -> !llvm.ptr, f64
        llvm.store %76, %77 : f64, !llvm.ptr
        %78 = llvm.add %68, %4  : i64
        llvm.br ^bb2(%78 : i64)
      ^bb4:  // pred: ^bb2
        llvm.intr.stackrestore %67 : !llvm.ptr
        llvm.br ^bb5
      ^bb5:  // pred: ^bb4
        omp.yield
      }
      omp.terminator
    }
    %66 = llvm.alloca %4 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %38, %66 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    llvm.call @comet_print_memref_f64(%0, %66) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @comet_sort_index(i64, !llvm.ptr, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @comet_print_memref_f64(i64, !llvm.ptr) attributes {sym_visibility = "private"}
}
